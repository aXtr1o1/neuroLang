{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import  DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BlenderbotForCausalLM,AutoTokenizer\n",
    "from datasets import Dataset,DatasetDict\n",
    "from peft import LoraConfig,TaskType,get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../..//dataStuff/bookStuff/osBookDoc.txt','r',encoding='utf-8', errors='ignore') as p:\n",
    "    s=p.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'OPERATING SYSTEM CONCEPTS\\n',\n",
       " 'NINTH EDITION\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'OPERATING SYSTEM CONCEPTS\\n',\n",
       " 'ABRAHAM SILBERSCHATZ\\n',\n",
       " 'Yale University\\n',\n",
       " '\\n',\n",
       " 'PETER BAER GALVIN\\n',\n",
       " '\\n',\n",
       " 'Vice President and Executive Publisher\\tDon Fowley\\n',\n",
       " 'Executive Editor\\tBeth Lang Golub\\n',\n",
       " 'Editorial Assistant\\tKatherine Willis\\n',\n",
       " 'Executive Marketing Manager\\tChristopher Ruel\\n',\n",
       " 'Senior Production Editor\\tKen Santor\\n',\n",
       " 'Cover and title page illustrations\\tSusan Cyr\\n',\n",
       " 'Cover Designer\\tMadelyn Lesure\\n',\n",
       " 'Text Designer\\tJudy Allan\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'This book was set in Palatino by the author using LaTeX and printed and bound by Courier- Kendallville. The cover was printed by Courier.\\n',\n",
       " '\\n',\n",
       " 'Copyright  2013, 2012, 2008 John Wiley & Sons, Inc. All rights reserved.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, scanning or otherwise, except as permitted under Sections 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to the Copyright Clearance Center, Inc. 222 Rosewood Drive, Danvers, MA 01923, (978)750-8400, fax (978)750-4470. Requests to the Publisher for permission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030 (201)748-6011, fax (201)748-6008, E-Mail: PERMREQ@WILEY.COM.\\n',\n",
       " '\\n',\n",
       " 'Evaluation copies are provided to qualified academics and professionals for review purposes only, for use in their courses during the next academic year. These copies are licensed and may not be sold or transferred to a third party. Upon completion of the review period, please return the evaluation copy to Wiley. Return instructions and a free-of-charge return shipping label are available at www.wiley.com/go/evalreturn. Outside of the United States, please contact your local representative.\\n',\n",
       " '\\n',\n",
       " 'Founded in 1807, John Wiley & Sons, Inc. has been a valued source of knowledge and understanding for more than 200 years, helping people around the world meet their needs and fulfill their aspirations. Our company is built on a foundation of principles that include responsibility to the communities we serve and where we live and work. In 2008, we launched a Corporate Citizenship Initiative, a global effort to address the environmental, social, economic, and ethical challenges we face in our business. Among the issues we are addressing are carbon impact, paper specifications and procurement, ethical conduct within our business and among our vendors, and community and charitable support. For more information, please visit our website: www.wiley.com/go/citizenship.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'ISBN: 978-1-118-06333-0\\n',\n",
       " 'ISBN BRV:     978-1-118-12938-8\\n',\n",
       " '\\n',\n",
       " 'Printed in the United States of America 10 9 8 7 6 5 4 3 2 1\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'To my children, Lemor, Sivan, and Aaron and my Nicolette\\n',\n",
       " 'Avi Silberschatz\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'To Brendan and Ellen,\\n',\n",
       " 'and Barbara, Anne and Harold, and Walter and Rebecca\\n',\n",
       " 'Peter Baer Galvin\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'To my Mom and Dad,\\n',\n",
       " 'Greg Gagne\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Preface\\n',\n",
       " '\\n',\n",
       " 'Operating systems are an essential part of any computer system. Similarly, a course on operating systems is an essential part of any computer science education. This field is undergoing rapid change, as computers are now prevalent in virtually every arena of day-to-day life from embedded devices in automobiles through the most sophisticated planning tools for governments and multinational firms. Yet the fundamental concepts remain fairly clear, and it is on these that we base this book.\\n',\n",
       " '   We wrote this book as a text for an introductory course in operating systems at the junior or senior undergraduate level or at the first-year graduate level. We hope that practitioners will also find it useful. It provides a clear description of the concepts that underlie operating systems. As prerequisites, we assume that the reader is familiar with basic data structures, computer organization, and a high-level language, such as C or Java. The hardware topics required for an understanding of operating systems are covered in Chapter 1. In that chapter, we also include an overview of the fundamental data structures that are prevalent in most operating systems. For code examples, we use predominantly C, with some Java, but the reader can still understand the algorithms without a thorough knowledge of these languages.\\n',\n",
       " '   Concepts are presented using intuitive descriptions. Important theoretical results are covered, but formal proofs are largely omitted. The bibliographical notes at the end of each chapter contain pointers to research papers in which results were first presented and proved, as well as references to recent material for further reading. In place of proofs, figures and examples are used to suggest why we should expect the result in question to be true.\\n',\n",
       " '   The fundamental concepts and algorithms covered in the book are often based on those used in both commercial and open-source operating systems. Our aim is to present these concepts and algorithms in a general setting that is not tied to one particular operating system. However, we present a large number of examples that pertain to the most popular and the most innovative operating systems, including Linux, Microsoft Windows, Apple Mac OS X, and Solaris. We also include examples of both Android and iOS, currently the two dominant mobile operating systems.\\n',\n",
       " '   The organization of the text reflects our many years of teaching courses on operating systems, as well as curriculum guidelines published by the IEEE\\n',\n",
       " '\\n',\n",
       " 'vii\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Computing Society and the Association for Computing Machinery (ACM). Consideration was also given to the feedback provided by the reviewers of the text, along with the many comments and suggestions we received from readers of our previous editions and from our current and former students.\\n',\n",
       " '\\n',\n",
       " 'Content of This Book\\n',\n",
       " 'The text is organized in eight major parts:\\n',\n",
       " ' Overview. Chapters 1 and 2 explain what operating systems are, what they do, and how they are designed and constructed. These chapters discuss what the common features of an operating system are and what an operating system does for the user. We include coverage of both traditional PC and server operating systems, as well as operating systems for mobile devices. The presentation is motivational and explanatory in nature. We have avoided a discussion of how things are done internally in these chapters. Therefore, they are suitable for individual readers or for students in lower-level classes who want to learn what an operating system is without getting into the details of the internal algorithms.\\n',\n",
       " ' Process management. Chapters 3 through 7 describe the process concept and concurrency as the heart of modern operating systems. A process is the unit of work in a system. Such a system consists of a collection of concurrently executing processes, some of which are operating-system processes (those that execute system code) and the rest of which are user processes (those that execute user code). These chapters cover methods for process scheduling, interprocess communication, process synchronization, and deadlock handling. Also included is a discussion of threads, as well as an examination of issues related to multicore systems and parallel programming.\\n',\n",
       " ' Memory management. Chapters 8 and 9 deal with the management of main memory during the execution of a process. To improve both the utilization of the CPU and the speed of its response to its users, the computer must keep several processes in memory. There are many different memory-management schemes, reflecting various approaches to memory management, and the effectiveness of a particular algorithm depends on the situation.\\n',\n",
       " ' Storage management. Chapters 10 through 13 describe how mass storage, the file system, and I/O are handled in a modern computer system. The file system provides the mechanism for on-line storage of and access to both data and programs. We describe the classic internal algorithms and structures of storage management and provide a firm practical understanding of the algorithms used their properties, advantages, and disadvantages. Since the I/O devices that attach to a computer vary widely, the operating system needs to provide a wide range of functionality to applications to allow them to control all aspects of these devices. We discuss system I/O in depth, including I/O system design, interfaces, and internal system structures and functions. In many ways, I/O devices are the slowest major components of the computer. Because they represent a\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'performance bottleneck, we also examine performance issues associated with I/O devices.\\n',\n",
       " ' Protection and security. Chapters 14 and 15 discuss the mechanisms necessary for the protection and security of computer systems. The processes in an operating system must be protected from one anothers activities, and to provide such protection, we must ensure that only processes that have gained proper authorization from the operating system can operate on the files, memory, CPU, and other resources of the system. Protection is a mechanism for controlling the access of programs, processes, or users to computer-system resources. This mechanism must provide a means of specifying the controls to be imposed, as well as a means of enforcement. Security protects the integrity of the information stored in the system (both data and code), as well as the physical resources of the system, from unauthorized access, malicious destruction or alteration, and accidental introduction of inconsistency.\\n',\n",
       " ' Advanced topics. Chapters 16 and 17 discuss virtual machines and distributed systems. Chapter 16 is a new chapter that provides an overview of virtual machines and their relationship to contemporary operating systems. Included is an overview of the hardware and software techniques that make virtualization possible. Chapter 17 condenses and updates the three chapters on distributed computing from the previous edition. This change is meant to make it easier for instructors to cover the material in the limited time available during a semester and for students to gain an understanding of the core ideas of distributed computing more quickly.\\n',\n",
       " ' Case studies. Chapters 18 and 19 in the text, along with Appendices A and B (which are available on (http://www.os-book.com), present  detailed case studies of real operating systems, including Linux, Windows 7, FreeBSD, and Mach. Coverage of both Linux and Windows 7 are presented throughout this text; however, the case studies provide much more detail. It is especially interesting to compare and contrast the design of these two very different systems. Chapter 20 briefly describes a few other influential operating systems.\\n',\n",
       " '\\n',\n",
       " 'The Ninth Edition\\n',\n",
       " 'As we wrote this Ninth Edition of Operating System Concepts, we were guided by the recent growth in three fundamental areas that affect operating systems:\\n',\n",
       " '1. Multicore systems\\n',\n",
       " '2. Mobile computing\\n',\n",
       " '3. Virtualization\\n',\n",
       " 'To emphasize these topics, we have integrated relevant coverage throughout this new edition and, in the case of virtualization, have written an entirely new chapter. Additionally, we have rewritten material in almost every chapter by bringing older material up to date and removing material that is no longer interesting or relevant.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '   We have also made substantial organizational changes. For example, we have eliminated the chapter on real-time systems and instead have integrated appropriate coverage of these systems throughout the text. We have reordered the chapters on storage management and have moved up the presentation of process synchronization so that it appears before process scheduling. Most of these organizational changes are based on our experiences while teaching courses on operating systems.\\n',\n",
       " '   Below, we provide a brief outline of the major changes to the various chapters:\\n',\n",
       " ' Chapter 1, Introduction, includes updated coverage of multiprocessor and multicore systems, as well as a new section on kernel data structures. Additionally, the coverage of computing environments now includes mobile systems and cloud computing. We also have incorporated an overview of real-time systems.\\n',\n",
       " ' Chapter 2, Operating-System Structures, provides new coverage of user interfaces for mobile devices, including discussions of iOS and Android, and expanded coverage of Mac OS X as a type of hybrid system.\\n',\n",
       " ' Chapter 3, Processes, now includes coverage of multitasking in mobile operating systems, support for the multiprocess model in Googles Chrome web browser, and zombie and orphan processes in UNIX.\\n',\n",
       " ' Chapter 4, Threads, supplies expanded coverage of parallelism and Amdahls law. It also provides a new section on implicit threading, including OpenMP and Apples Grand Central Dispatch.\\n',\n",
       " ' Chapter 5, Process Synchronization (previously Chapter 6), adds a new section on mutex locks as well as coverage of synchronization using OpenMP, as well as functional languages.\\n',\n",
       " ' Chapter 6, CPU Scheduling (previously Chapter 5), contains new coverage of the Linux CFS scheduler and Windows user-mode scheduling. Coverage of real-time scheduling algorithms has also been integrated into this chapter.\\n',\n",
       " ' Chapter 7, Deadlocks, has no major changes.\\n',\n",
       " ' Chapter 8, Main Memory, includes new coverage of swapping on mobile systems and Intel 32- and 64-bit architectures. A new section discusses ARM architecture.\\n',\n",
       " ' Chapter 9, Virtual Memory, updates kernel memory management to include the Linux SLUB and SLOB memory allocators.\\n',\n",
       " ' Chapter 10, Mass-Storage Structure (previously Chapter 12), adds cover- age of solid-state disks.\\n',\n",
       " ' Chapter 11, File-System Interface (previously Chapter 10), is updated with information about current technologies.\\n',\n",
       " ' Chapter 12, File-System Implementation (previously Chapter 11), is updated with coverage of current technologies.\\n',\n",
       " ' Chapter 13, I/O, updates technologies and performance numbers, expands coverage of synchronous/asynchronous and blocking/nonblocking I/O, and adds a section on vectored I/O.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' Chapter 14, Protection, has no major changes.\\n',\n",
       " ' Chapter 15, Security, has a revised cryptography section with modern notation and an improved explanation of various encryption methods and their uses. The chapter also includes new coverage of Windows 7 security.\\n',\n",
       " ' Chapter 16, Virtual Machines, is a new chapter that provides an overview of virtualization and how it relates to contemporary operating systems.\\n',\n",
       " ' Chapter 17, Distributed Systems, is a new chapter that combines and updates a selection of materials from previous Chapters 16, 17, and 18.\\n',\n",
       " ' Chapter 18, The Linux System (previously Chapter 21), has been updated to cover the Linux 3.2 kernel.\\n',\n",
       " ' Chapter 19, Windows 7, is a new chapter presenting a case study of Windows 7.\\n',\n",
       " ' Chapter 20, Influential Operating Systems (previously Chapter 23), has no major changes.\\n',\n",
       " '\\n',\n",
       " 'Programming Environments\\n',\n",
       " 'This book uses examples of many real-world operating systems to illustrate fundamental operating-system concepts. Particular attention is paid to Linux and Microsoft Windows, but we also refer to various versions of UNIX (including Solaris, BSD, and Mac OS X).\\n',\n",
       " '   The text also provides several example programs written in C and Java. These programs are intended to run in the following programming environments:\\n',\n",
       " ' POSIX. POSIX (which stands for Portable Operating System Interface) repre- sents a set of standards implemented primarily for UNIX-based operating systems. Although Windows systems can also run certain POSIX programs, our coverage of POSIX focuses on UNIX and Linux systems. POSIX-compliant systems must implement the POSIX core standard (POSIX.1); Linux, Solaris, and Mac OS X are examples of POSIX-compliant systems. POSIX also defines several extensions to the standards, including real-time extensions (POSIX1.b) and an extension for a threads library (POSIX1.c, better known as Pthreads). We provide several programming examples written in C illustrating the POSIX base API, as well as Pthreads and the extensions for real-time programming. These example programs were tested on Linux 2.6 and 3.2 systems, Mac OS X 10.7, and Solaris 10 using the gcc 4.0 compiler.\\n',\n",
       " ' Java. Java is a widely used programming language with a rich API and built-in language support for thread creation and management. Java programs run on any operating system supporting a Java virtual machine (or JVM). We illustrate various operating-system and networking concepts with Java programs tested using the Java 1.6 JVM.\\n',\n",
       " ' Windows systems. The primary programming environment for Windows systems is the Windows API, which provides a comprehensive set of func- tions for managing processes, threads, memory, and peripheral devices. We supply several C programs illustrating the use of this API. Programs were tested on systems running Windows XP and Windows 7.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '   We have chosen these three programming environments because we believe that they best represent the two most popular operating-system models\\n',\n",
       " ' Windows and UNIX/Linux along with the widely used Java environment. Most programming examples are written in C, and we expect readers to be comfortable with this language. Readers familiar with both the C and Java languages should easily understand most programs provided in this text.\\n',\n",
       " '    In some instances  such as thread creation  we illustrate a specific concept using all three programming environments, allowing the reader to contrast the three different libraries as they address the same task. In other situations, we may use just one of the APIs to demonstrate a concept. For example, we illustrate shared memory using just the POSIX API; socket programming in TCP/IP is highlighted using the Java API.\\n',\n",
       " '\\n',\n",
       " 'Linux Virtual Machine\\n',\n",
       " 'To help students gain a better understanding of the Linux system, we provide a Linux  virtual  machine,  including  the  Linux  source  code, that is available for download  from  the  the  website  supporting  this text  (http://www.os-book.com).   This   virtual   machine   also   includes   a gcc development environment with compilers and editors. Most of the programming assignments in the book can be completed on this virtual machine, with the exception of assignments that require Java or the Windows API.\\n',\n",
       " '   We also provide three programming assignments that modify the Linux kernel through kernel modules:\\n',\n",
       " '1. Adding a basic kernel module to the Linux kernel.\\n',\n",
       " '2. Adding a kernel module that uses various kernel data structures.\\n',\n",
       " '3. Adding a kernel module that iterates over tasks in a running Linux system.\\n',\n",
       " '\\n',\n",
       " 'Over time it is our intention to add additional kernel module assignments on the supporting website.\\n',\n",
       " '\\n',\n",
       " 'Supporting Website\\n',\n",
       " 'When you visit the website supporting this text at http://www.os-book.com, you can download the following resources:\\n',\n",
       " ' Linux virtual machine\\n',\n",
       " ' C and Java source code\\n',\n",
       " ' Sample syllabi\\n',\n",
       " ' Set of Powerpoint slides\\n',\n",
       " ' Set of figures and illustrations\\n',\n",
       " ' FreeBSD and Mach case studies\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' Solutions to practice exercises\\n',\n",
       " ' Study guide for students\\n',\n",
       " ' Errata\\n',\n",
       " 'Notes to Instructors\\n',\n",
       " 'On the website for this text, we provide several sample syllabi that suggest various approaches for using the text in both introductory and advanced courses. As a general rule, we encourage instructors to progress sequentially through the chapters, as this strategy provides the most thorough study of operating systems. However, by using the sample syllabi, an instructor can select a different ordering of chapters (or subsections of chapters).\\n',\n",
       " '   In this edition, we have added over sixty new written exercises and over twenty new programming problems and projects. Most of the new program- ming assignments involve processes, threads, process synchronization, and memory management. Some involve adding kernel modules to the Linux system which requires using either the Linux virtual machine that accompanies this text or another suitable Linux distribution.\\n',\n",
       " '   Solutions to written exercises and programming assignments are available to instructors who have adopted this text for their operating-system class. To obtain these restricted supplements, contact your local John Wiley & Sons sales representative. You can find your Wiley representative by going to http://www.wiley.com/college and clicking Whos my rep?\\n',\n",
       " '\\n',\n",
       " 'Notes to Students\\n',\n",
       " 'We encourage you to take advantage of the practice exercises that appear at the end of each chapter. Solutions to the practice exercises are available for download from the supporting website http://www.os-book.com. We also encourage you to read through the study guide, which was prepared by one of our students. Finally, for students who are unfamiliar with UNIX and Linux systems, we recommend that you download and install the Linux virtual machine that we include on the supporting website. Not only will this provide you with a new computing experience, but the open-source nature of Linux will allow you to easily examine the inner details of this popular operating system.\\n',\n",
       " 'We wish you the very best of luck in your study of operating systems.\\n',\n",
       " '\\n',\n",
       " 'Contacting Us\\n',\n",
       " 'We have endeavored to eliminate typos, bugs, and the like from the text. But, as in new releases of software, bugs almost surely remain. An up-to-date errata list is accessible from the books website. We would be grateful if you would notify us of any errors or omissions in the book that are not on the current list of errata.\\n',\n",
       " '   We would be glad to receive suggestions on improvements to the book. We also welcome any contributions to the book website that could be of\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'use to other readers, such as programming exercises, project suggestions, on-line labs and tutorials, and teaching tips. E-mail should be addressed to os-book-authors@cs.yale.edu.\\n',\n",
       " '\\n',\n",
       " 'Acknowledgments\\n',\n",
       " 'This book is derived from the previous editions, the first three of which were coauthored by James Peterson. Others who helped us with previous editions include Hamid Arabnia, Rida Bazzi, Randy Bentson, David Black, Joseph Boykin, Jeff Brumfield, Gael Buckley, Roy Campbell, P. C. Capon, John Carpenter, Gil Carrick, Thomas Casavant, Bart Childs, Ajoy Kumar Datta, Joe Deck, Sudarshan K. Dhall, Thomas Doeppner, Caleb Drake, M. Racsit Eskiciog lu, Hans Flack, Robert Fowler, G. Scott Graham, Richard Guy, Max Hailperin, Rebecca Hartman, Wayne Hathaway, Christopher Haynes, Don Heller, Bruce Hillyer, Mark Holliday, Dean Hougen, Michael Huang, Ahmed Kamel, Morty Kewstel, Richard Kieburtz, Carol Kroll, Morty Kwestel, Thomas LeBlanc, John Leggett, Jerrold Leichter, Ted Leung, Gary Lippman, Carolyn Miller, Michael Molloy, Euripides Montagne, Yoichi Muraoka, Jim M. Ng, Banu O zden, Ed Posnak, Boris Putanec, Charles Qualline, John Quarterman, Mike Reiter, Gustavo Rodriguez-Rivera, Carolyn J. C. Schauble, Thomas P. Skinner, Yannis Smaragdakis, Jesse St. Laurent, John Stankovic, Adam Stauffer, Steven Stepanek, John Sterling, Hal Stern, Louis Stevens, Pete Thomas, David Umbaugh, Steve Vinoski, Tommy Wagner, Larry L. Wear, John Werth, James\\n',\n",
       " 'M. Westall, J. S. Weston, and Yang Xiang\\n',\n",
       " '   Robert Love updated both Chapter 18 and the Linux coverage throughout the text, as well as answering many of our Android-related questions. Chapter 19 was written by Dave Probert and was derived from Chapter 22 of the Eighth Edition of Operating System Concepts. Jonathan Katz contributed to Chapter\\n',\n",
       " '15. Richard West provided input into Chapter 16. Salahuddin Khan updated Section 15.9 to provide new coverage of Windows 7 security.\\n',\n",
       " '   Parts of Chapter 17 were derived from a paper by Levy and Silberschatz [1990]. Chapter 18 was derived from an unpublished manuscript by Stephen Tweedie. Cliff Martin helped with updating the UNIX appendix to cover FreeBSD. Some of the exercises and accompanying solutions were supplied by Arvind Krishnamurthy. Andrew DeNicola prepared the student study guide that is available on our website. Some of the the slides were prepeared by Marilyn Turnamian.\\n',\n",
       " '   Mike Shapiro, Bryan Cantrill, and Jim Mauro answered several Solaris- related questions, and Bryan Cantrill from Sun Microsystems helped with the ZFS coverage. Josh Dees and Rob Reynolds contributed coverage of Microsofts NET. The project for POSIX message queues was contributed by John Trono of Saint Michaels College in Colchester, Vermont.\\n',\n",
       " '   Judi Paige helped with generating figures and presentation of slides. Thomas Gagne prepared new artwork for this edition. Owen Galvin helped copy-edit Chapter 16. Mark Wogahn has made sure that the software to produce this book (LATEX and fonts) works properly. Ranjan Kumar Meher rewrote some of the LATEX software used in the production of this new text.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '   Our Executive Editor, Beth Lang Golub, provided expert guidance as we prepared this edition. She was assisted by Katherine Willis, who managed many details of the project smoothly. The Senior Production Editor, Ken Santor, was instrumental in handling all the production details.\\n',\n",
       " '   The cover illustrator was Susan Cyr, and the cover designer was Madelyn Lesure. Beverly Peavler copy-edited the manuscript. The freelance proofreader was Katrina Avery; the freelance indexer was WordCo, Inc.\\n',\n",
       " '\\n',\n",
       " 'Abraham Silberschatz, New Haven, CT, 2012 Peter Baer Galvin, Boston, MA, 2012\\n',\n",
       " 'Greg Gagne, Salt Lake City, UT, 2012\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Contents\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'PART ONE\\tOVERVIEW\\n',\n",
       " '\\n',\n",
       " 'Chapter 1\\tIntroduction\\n',\n",
       " '1.1 What Operating Systems Do 4\\n',\n",
       " '1.2 Computer-System Organization   7\\n',\n",
       " '1.3 Computer-System Architecture   12\\n',\n",
       " '1.4 Operating-System Structure 19\\n',\n",
       " '1.5 Operating-System Operations 21\\n',\n",
       " '1.6 Process Management   24\\n',\n",
       " '1.7 Memory Management   25\\n',\n",
       " '1.8 Storage Management   26\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '1.9 Protection and Security   30\\n',\n",
       " '1.10 Kernel Data Structures   31\\n',\n",
       " '1.11 Computing Environments 35\\n',\n",
       " '1.12 Open-Source Operating Systems 43\\n',\n",
       " '1.13 Summary 47 Exercises 49\\n',\n",
       " 'Bibliographical Notes 52\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Chapter 2\\tOperating-System Structures\\n',\n",
       " '\\n',\n",
       " '2.1 Operating-System Services   55\\n',\n",
       " '2.2 User and Operating-System Interface 58\\n',\n",
       " '2.3 System Calls 62\\n',\n",
       " '2.4 Types of System Calls 66\\n',\n",
       " '2.5 System Programs 74\\n',\n",
       " '2.6 Operating-System Design and Implementation 75\\n',\n",
       " '2.7 \\n',\n",
       " 'Operating-System Structure 78\\n',\n",
       " '2.8 Operating-System Debugging   86\\n',\n",
       " '2.9 Operating-System Generation   91\\n',\n",
       " '2.10 System Boot 92\\n',\n",
       " '2.11 Summary 93 Exercises 94\\n',\n",
       " 'Bibliographical Notes 101\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'PART TWO\\n',\n",
       " 'Chapter 3\\tProcesses\\n',\n",
       " '3.1 Process Concept 105\\n',\n",
       " '\\n',\n",
       " 'PROCESS MANAGEMENT\\n',\n",
       " '\\n',\n",
       " '3.6 Communication in Client \\n',\n",
       " '\\n',\n",
       " '3.2 Process Scheduling 110\\n',\n",
       " '3.3 Operations on Processes 115\\n',\n",
       " '3.4 Interprocess Communication   122\\n',\n",
       " '3.5 Examples of IPC Systems 130\\n',\n",
       " '\\n',\n",
       " 'Server Systems   136\\n',\n",
       " '3.7 Summary 147 Exercises 149\\n',\n",
       " 'Bibliographical Notes 161\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'xvii\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Chapter 4\\tThreads\\n',\n",
       " '\\n',\n",
       " '4.1 Overview 163\\n',\n",
       " '\\n',\n",
       " '4.6\\n',\n",
       " 'Threading Issues 183\\n',\n",
       " '4.2 Multicore Programming\\n',\n",
       " '166\\n',\n",
       " '4.7\\n',\n",
       " 'Operating-System Examples 188\\n',\n",
       " '4.3 Multithreading Models\\n',\n",
       " '169\\n',\n",
       " '4.8\\n',\n",
       " 'Summary 191\\n',\n",
       " '4.4 Thread Libraries 171\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Exercises 191\\n',\n",
       " '4.5 Implicit Threading 177\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Bibliographical Notes 199\\n',\n",
       " '\\n',\n",
       " 'Chapter 5\\tProcess Synchronization\\n',\n",
       " '\\n',\n",
       " '5.1 Background 203\\n',\n",
       " '\\n',\n",
       " '5.8\\n',\n",
       " 'Monitors 223\\n',\n",
       " '5.2 The Critical-Section Problem\\n',\n",
       " '206\\n',\n",
       " '5.9\\n',\n",
       " 'Synchronization Examples 232\\n',\n",
       " '5.3 Petersons Solution 207\\n',\n",
       " '\\n',\n",
       " '5.10\\n',\n",
       " 'Alternative Approaches 238\\n',\n",
       " '5.4 Synchronization Hardware\\n',\n",
       " '209\\n',\n",
       " '5.11\\n',\n",
       " 'Summary 242\\n',\n",
       " '5.5 Mutex Locks 212\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Exercises 242\\n',\n",
       " '5.6 Semaphores 213\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Bibliographical Notes 258\\n',\n",
       " '5.7 Classic Problems of\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Synchronization 219\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Chapter 6\\tCPU Scheduling\\n',\n",
       " '6.1 Basic Concepts 261\\n',\n",
       " '6.2 Scheduling Criteria 265\\n',\n",
       " '6.3 Scheduling Algorithms   266\\n',\n",
       " '6.4 Thread Scheduling   277\\n',\n",
       " '6.5 Multiple-Processor Scheduling   278\\n',\n",
       " '6.6 Real-Time CPU Scheduling 283\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '6.7 Operating-System Examples 290\\n',\n",
       " '6.8 Algorithm Evaluation 300\\n',\n",
       " '6.9 Summary 304 Exercises 305\\n',\n",
       " 'Bibliographical Notes 311\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Chapter 7\\tDeadlocks\\n',\n",
       " '7.1 System Model 315\\n',\n",
       " '7.2 Deadlock Characterization 317\\n',\n",
       " '7.3 Methods for Handling Deadlocks   322\\n',\n",
       " '7.4 Deadlock Prevention   323\\n',\n",
       " '7.5 Deadlock Avoidance   327\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '7.6 Deadlock Detection 333\\n',\n",
       " '7.7 Recovery from Deadlock 337\\n',\n",
       " '7.8 Summary 339 Exercises 339\\n',\n",
       " 'Bibliographical Notes 346\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'PART THREE\\tMEMORY MANAGEMENT\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Chapter 8\\tMain Memory\\n',\n",
       " '8.1 Background 351\\n',\n",
       " '8.2 Swapping 358\\n',\n",
       " '8.3 Contiguous Memory Allocation   360\\n',\n",
       " '8.4 Segmentation 364\\n',\n",
       " '8.5 Paging 366\\n',\n",
       " '8.6 Structure of the Page Table 378\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '8.7 Example: Intel 32 and 64-bit Architectures 383\\n',\n",
       " '8.8 Example: ARM Architecture 388\\n',\n",
       " '8.9 Summary 389 Exercises 390\\n',\n",
       " 'Bibliographical Notes 394\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Chapter 9\\tVirtual Memory\\n',\n",
       " '9.1 Background 397\\n',\n",
       " '9.2 Demand Paging 401\\n',\n",
       " '9.3 Copy-on-Write 408\\n',\n",
       " '9.4 Page Replacement 409\\n',\n",
       " '9.5 Allocation of Frames 421\\n',\n",
       " '9.6 Thrashing 425\\n',\n",
       " '9.7 Memory-Mapped Files 430\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '9.8 Allocating Kernel Memory 436\\n',\n",
       " '9.9 Other Considerations 439\\n',\n",
       " '9.10 Operating-System Examples 445\\n',\n",
       " '9.11 Summary 448 Exercises 449\\n',\n",
       " 'Bibliographical Notes 461\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'PART FOUR\\tSTORAGE MANAGEMENT\\n',\n",
       " '\\n',\n",
       " 'Chapter 10\\tMass-Storage Structure\\n',\n",
       " '\\n',\n",
       " '10.1 Overview of Mass-Storage Structure 467\\n',\n",
       " '10.2 Disk Structure 470\\n',\n",
       " '10.3 Disk Attachment 471\\n',\n",
       " '10.4 Disk Scheduling 472\\n',\n",
       " '10.5 Disk Management 478\\n',\n",
       " '10.6 \\n',\n",
       " 'Swap-Space Management 482\\n',\n",
       " '10.7 RAID Structure 484\\n',\n",
       " '10.8 Stable-Storage Implementation 494\\n',\n",
       " '10.9 \\tSummary 496 Exercises 497\\n',\n",
       " 'Bibliographical Notes 501\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Chapter 11\\tFile-System Interface\\n',\n",
       " '\\n',\n",
       " '11.1 File Concept 503\\n',\n",
       " '11.2 Access Methods 513\\n',\n",
       " '11.3 Directory and Disk Structure   515\\n',\n",
       " '11.4 File-System Mounting 526\\n',\n",
       " '11.5 File Sharing 528\\n',\n",
       " '11.6 \\n',\n",
       " 'Protection 533\\n',\n",
       " '11.7 Summary 538 Exercises 539\\n',\n",
       " 'Bibliographical Notes 541\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Chapter 12\\tFile-System Implementation\\n',\n",
       " '\\n',\n",
       " '12.1 File-System Structure 543\\n',\n",
       " '12.7\\n',\n",
       " 'Recovery 568\\n',\n",
       " '\\n',\n",
       " '12.2 File-System Implementation 546\\n',\n",
       " '12.8\\n',\n",
       " 'NFS 571\\n',\n",
       " '\\n',\n",
       " '12.3 Directory Implementation 552\\n',\n",
       " '12.9\\n',\n",
       " 'Example: The WAFL File System\\n',\n",
       " '577\\n',\n",
       " '12.4 Allocation Methods 553\\n',\n",
       " '12.5 Free-Space Management 561\\n',\n",
       " '12.6 Efficiency and Performance 564\\n',\n",
       " '12.10\\n',\n",
       " 'Summary 580\\n',\n",
       " 'Exercises 581\\n',\n",
       " 'Bibliographical Notes 585\\n',\n",
       " '\\n',\n",
       " 'Chapter 13\\tI/O Systems\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '13.1 Overview 587\\n',\n",
       " '13.6\\n',\n",
       " 'STREAMS 613\\n',\n",
       " '\\n',\n",
       " '13.2 I/O Hardware  588\\n',\n",
       " '13.7\\n',\n",
       " 'Performance 615\\n',\n",
       " '\\n',\n",
       " '13.3 Application I/O Interface   597\\n',\n",
       " '13.4 Kernel I/O Subsystem  604\\n',\n",
       " '13.5 Transforming I/O Requests to\\n',\n",
       " '13.8\\n',\n",
       " 'Summary 618\\n',\n",
       " 'Exercises 619\\n',\n",
       " 'Bibliographical Notes 621\\n',\n",
       " '\\n',\n",
       " 'Hardware Operations 611\\n',\n",
       " '\\n',\n",
       " 'PART FIVE\\tPROTECTION AND SECURITY\\n',\n",
       " '\\n',\n",
       " 'Chapter 14\\tProtection\\n',\n",
       " '\\n',\n",
       " '14.1 Goals of Protection 625\\n',\n",
       " '\\n',\n",
       " '14.7\\n',\n",
       " 'Revocation of Access Rights 640\\n',\n",
       " '14.2 Principles of Protection 626\\n',\n",
       " '\\n',\n",
       " '14.8\\n',\n",
       " 'Capability-Based Systems 641\\n',\n",
       " '14.3 Domain of Protection 627\\n',\n",
       " '\\n',\n",
       " '14.9\\n',\n",
       " 'Language-Based Protection 644\\n',\n",
       " '14.4 Access Matrix 632\\n',\n",
       " '14.5 Implementation of the Access Matrix 636\\n',\n",
       " '14.6 Access Control 639\\n',\n",
       " '\\n',\n",
       " '14.10\\n',\n",
       " 'Summary 649\\n',\n",
       " 'Exercises 650\\n',\n",
       " 'Bibliographical Notes 652\\n',\n",
       " 'Chapter 15\\tSecurity\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '15.1 The Security Problem 657\\n',\n",
       " '15.2 Program Threats 661\\n',\n",
       " '15.3 System and Network Threats   669\\n',\n",
       " '\\n',\n",
       " '15.8\\n',\n",
       " '\\n',\n",
       " '15.9\\n',\n",
       " 'Computer-Security Classifications 698\\n',\n",
       " 'An Example: Windows 7 699\\n',\n",
       " '15.4 Cryptography as a Security Tool\\n',\n",
       " '15.5 User Authentication 685\\n',\n",
       " '15.6 Implementing Security Defenses\\n',\n",
       " '674\\n',\n",
       " '\\n',\n",
       " '689\\n',\n",
       " '15.10\\n',\n",
       " 'Summary 701\\n',\n",
       " 'Exercises 702\\n',\n",
       " 'Bibliographical Notes 704\\n',\n",
       " '15.7 Firewalling to Protect Systems and\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Networks 696\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'PART SIX\\tADVANCED TOPICS\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Chapter 16\\tVirtual Machines\\n',\n",
       " '16.1 Overview   711\\n',\n",
       " '16.2 History 713\\n',\n",
       " '16.3 Benefits and Features 714\\n',\n",
       " '16.4 Building Blocks 717\\n',\n",
       " '16.5 Types of Virtual Machines and Their Implementations 721\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '16.6 Virtualization and Operating-System Components 728\\n',\n",
       " '16.7 Examples 735\\n',\n",
       " '16.8 Summary 737 Exercises 738\\n',\n",
       " 'Bibliographical Notes 739\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Chapter 17\\tDistributed Systems\\n',\n",
       " '17.1 Advantages of Distributed Systems 741\\n',\n",
       " '17.2 Types of Network-\\n',\n",
       " 'based Operating Systems 743\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '17.6 An Example: TCP/IP   760\\n',\n",
       " '17.7 Robustness 762\\n',\n",
       " '17.8 Design Issues 764\\n',\n",
       " '17.9 Distributed File Systems 765\\n',\n",
       " '\\n',\n",
       " '17.3 Network Structure 747\\n',\n",
       " '17.4 Communication Structure   751\\n',\n",
       " '17.5 Communication Protocols   756\\n',\n",
       " '17.10 \\t    773 Exercises    774\\n',\n",
       " 'Bibliographical Notes 777\\n',\n",
       " '\\n',\n",
       " 'PART SEVEN\\tCASE STUDIES\\n',\n",
       " '\\n',\n",
       " 'Chapter 18\\tThe Linux System\\n',\n",
       " '\\n',\n",
       " '18.1 Linux History 781\\n',\n",
       " '18.8\\n',\n",
       " 'Input and Output 815\\n',\n",
       " '\\n',\n",
       " '18.2 Design Principles 786\\n',\n",
       " '18.9\\n',\n",
       " 'Interprocess Communication\\n',\n",
       " '818\\n',\n",
       " '18.3 Kernel Modules 789\\n',\n",
       " '18.10\\n',\n",
       " 'Network Structure 819\\n',\n",
       " '\\n',\n",
       " '18.4 Process Management 792\\n',\n",
       " '18.11\\n',\n",
       " 'Security 821\\n',\n",
       " '\\n',\n",
       " '18.5 Scheduling 795\\n',\n",
       " '18.6 Memory Management 800\\n',\n",
       " '18.7 File Systems 809\\n',\n",
       " '18.12\\n',\n",
       " 'Summary 824\\n',\n",
       " 'Exercises 824\\n',\n",
       " 'Bibliographical Notes 826\\n',\n",
       " '\\n',\n",
       " 'Chapter 19\\tWindows 7\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '19.1 History 829\\n',\n",
       " '19.6\\n',\n",
       " 'Networking 869\\n',\n",
       " '\\n',\n",
       " '19.2 Design Principles 831\\n',\n",
       " '19.7\\n',\n",
       " 'Programmer Interface 874\\n',\n",
       " '\\n',\n",
       " '19.3 System Components 838\\n',\n",
       " '19.4 Terminal Services and Fast User\\n',\n",
       " 'Switching 862\\n',\n",
       " '19.8\\n',\n",
       " 'Summary 883\\n',\n",
       " 'Exercises 883\\n',\n",
       " 'Bibliographical Notes 885\\n',\n",
       " '\\n',\n",
       " '19.5 File System 863\\n',\n",
       " '\\n',\n",
       " 'Chapter 20\\tInfluential Operating Systems\\n',\n",
       " '\\n',\n",
       " '20.1 Feature Migration 887\\n',\n",
       " '20.10\\n',\n",
       " 'TOPS-20 901\\n',\n",
       " '\\n',\n",
       " '20.2 Early Systems 888\\n',\n",
       " '20.11\\n',\n",
       " 'CP/M and MS/DOS\\n',\n",
       " '901\\n',\n",
       " '20.3 Atlas 895\\n',\n",
       " '20.12\\n',\n",
       " 'Macintosh Operating System and\\n',\n",
       " '20.4 XDS-940 896\\n',\n",
       " '\\n',\n",
       " 'Windows 902\\n',\n",
       " '20.5 THE 897\\n',\n",
       " '20.13\\n',\n",
       " 'Mach 902\\n',\n",
       " '20.6 RC 4000 897\\n',\n",
       " '20.14\\n',\n",
       " 'Other Systems 904\\n',\n",
       " '20.7 CTSS 898\\n',\n",
       " '\\n',\n",
       " 'Exercises 904\\n',\n",
       " '20.8 MULTICS 899\\n',\n",
       " '\\n',\n",
       " 'Bibliographical Notes 904\\n',\n",
       " '20.9 IBM OS/360   899\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'PART EIGHT\\tAPPENDICES\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Appendix A\\tBSD UNIX\\n',\n",
       " 'A.1 UNIX History A1\\n',\n",
       " 'A.2 Design Principles A6\\n',\n",
       " 'A.3 Programmer Interface A8\\n',\n",
       " 'A.4 User Interface A15\\n',\n",
       " 'A.5 Process Management A18\\n',\n",
       " 'A.6 Memory Management A22\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'A.7 File System   A24\\n',\n",
       " 'A.8 I/O System   A32\\n',\n",
       " 'A.9 Interprocess Communication A36\\n',\n",
       " 'A.10 Summary A40 Exercises A41\\n',\n",
       " 'Bibliographical Notes A42\\n',\n",
       " '\\n',\n",
       " 'xxii\\tContents\\n',\n",
       " 'Appendix B\\tThe Mach System\\n',\n",
       " 'B.1 History of the Mach System B1\\n',\n",
       " 'B.2 Design Principles B3\\n',\n",
       " 'B.3 System Components   B4\\n',\n",
       " 'B.4 Process Management   B7\\n',\n",
       " 'B.5 Interprocess Communication B13\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'B.6 Memory Management   B18\\n',\n",
       " 'B.7 Programmer Interface   B23\\n',\n",
       " 'B.8 Summary B24 Exercises B25\\n',\n",
       " 'Bibliographical Notes B26\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Part One\\n',\n",
       " 'Overview\\n',\n",
       " 'An operating system acts as an intermediary between the user of a computer and the computer hardware. The purpose of an operating system is to provide an environment in which a user can execute programs in a convenient and efficient manner.\\n',\n",
       " '   An operating system is software that manages the computer hard- ware. The hardware must provide appropriate mechanisms to ensure the correct operation of the computer system and to prevent user programs from interfering with the proper operation of the system.\\n',\n",
       " '   Internally, operating systems vary greatly in their makeup, since they are organized along many different lines. The design of a new operating system is a major task. It is important that the goals of the system be well defined before the design begins. These goals form the basis for choices among various algorithms and strategies.\\n',\n",
       " '   Because an operating system is large and complex, it must be created piece by piece. Each of these pieces should be a well-delineated portion of the system, with carefully defined inputs, outputs, and functions.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Introduction\\n',\n",
       " '\\n',\n",
       " 'An operating system is a program that manages a computers hardware. It also provides a basis for application programs and acts as an intermediary between the computer user and the computer hardware. An amazing aspect of operating systems is how they vary in accomplishing these tasks. Mainframe operating systems are designed primarily to optimize utilization of hardware. Personal computer (PC) operating systems support complex games, business applications, and everything in between. Operating systems for mobile com- puters provide an environment in which a user can easily interface with the computer to execute programs. Thus, some operating systems are designed to be convenient, others to be efflcient, and others to be some combination of the two.\\n',\n",
       " '    Before we can explore the details of computer system operation, we need to know something about system structure. We thus discuss the basic functions of system startup, I/O, and storage early in this chapter. We also describe the basic computer architecture that makes it possible to write a functional operating system.\\n',\n",
       " '   Because an operating system is large and complex, it must be created piece by piece. Each of these pieces should be a well-delineated portion of the system, with carefully defined inputs, outputs, and functions. In this chapter, we provide a general overview of the major components of a contemporary computer system as well as the functions provided by the operating system. Additionally, we cover several other topics to help set the stage for the remainder of this text: data structures used in operating systems, computing environments, and open-source operating systems.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '3\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Figure 1.1 Abstract view of the components of a computer system.\\n',\n",
       " '\\n',\n",
       " '1.1 What Operating Systems Do\\n',\n",
       " 'We begin our discussion by looking at the operating systems role in the overall computer system. A computer system can be divided roughly into four components: the hardware, the operating system, the application programs, and the users (Figure 1.1).\\n',\n",
       " '    The hardware the central processing unit (CPU), the memory, and the input/output (I/O) devices provides the basic computing resources for the system. The application programs such as word processors, spreadsheets, compilers, and Web browsers define the ways in which these resources are used to solve users computing problems. The operating system controls the hardware and coordinates its use among the various application programs for the various users.\\n',\n",
       " '   We can also view a computer system as consisting of hardware, software, and data. The operating system provides the means for proper use of these resources in the operation of the computer system. An operating system is similar to a government. Like a government, it performs no useful function by itself. It simply provides an environment within which other programs can do useful work.\\n',\n",
       " '   To understand more fully the operating systems role, we next explore operating systems from two viewpoints: that of the user and that of the system.\\n',\n",
       " '\\n',\n",
       " '1.1.1 User View\\n',\n",
       " 'The users view of the computer varies according to the interface being used. Most computer users sit in front of a PC, consisting of a monitor, keyboard, mouse, and system unit. Such a system is designed for one user\\n',\n",
       " '\\n',\n",
       " '1.1 What Operating Systems Do\\t5\\n',\n",
       " '\\n',\n",
       " 'to monopolize its resources. The goal is to maximize the work (or play) that the user is performing. In this case, the operating system is designed mostly for ease of use, with some attention paid to performance and none paid to resource utilization  how various hardware and software resources are shared. Performance is, of course, important to the user; but such systems are optimized for the single-user experience rather than the requirements of multiple users.\\n',\n",
       " '   In other cases, a user sits at a terminal connected to a mainframe or a minicomputer. Other users are accessing the same computer through other terminals. These users share resources and may exchange information. The operating system in such cases is designed to maximize resource utilization to assure that all available CPU time, memory, and I/O are used efficiently and that no individual user takes more than her fair share.\\n',\n",
       " '   In still other cases, users sit at workstations connected to networks of other workstations and servers. These users have dedicated resources at their disposal, but they also share resources such as networking and servers, including file, compute, and print servers. Therefore, their operating system is designed to compromise between individual usability and resource utilization. Recently, many varieties of mobile computers, such as smartphones and tablets, have come into fashion. Most mobile computers are standalone units for individual users. Quite often, they are connected to networks through cellular or other wireless technologies. Increasingly, these mobile devices are replacing desktop and laptop computers for people who are primarily interested in using computers for e-mail and web browsing. The user interface for mobile computers generally features a touch screen, where the user interacts with the system by pressing and swiping fingers across the screen rather than using a\\n',\n",
       " 'physical keyboard and mouse.\\n',\n",
       " '   Some computers have little or no user view. For example, embedded computers in home devices and automobiles may have numeric keypads and may turn indicator lights on or off to show status, but they and their operating systems are designed primarily to run without user intervention.\\n',\n",
       " '\\n',\n",
       " '1.1.2 System View\\n',\n",
       " 'From the computers point of view, the operating system is the program most intimately involved with the hardware. In this context, we can view an operating system as a resource allocator. A computer system has many resources that may be required to solve a problem: CPU time, memory space, file-storage space, I/O devices, and so on. The operating system acts as the manager of these resources. Facing numerous and possibly conflicting requests for resources, the operating system must decide how to allocate them to specific programs and users so that it can operate the computer system efficiently and fairly. As we have seen, resource allocation is especially important where many users access the same mainframe or minicomputer.\\n',\n",
       " '   A slightly different view of an operating system emphasizes the need to control the various I/O devices and user programs. An operating system is a control program. A control program manages the execution of user programs to prevent errors and improper use of the computer. It is especially concerned with the operation and control of I/O devices.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '1.1.3 Defining Operating Systems\\n',\n",
       " 'By now, you can probably see that the term operating system covers many roles and functions. That is the case, at least in part, because of the myriad designs and uses of computers. Computers are present within toasters, cars, ships, spacecraft, homes, and businesses. They are the basis for game machines, music players, cable TV tuners, and industrial control systems. Although computers have a relatively short history, they have evolved rapidly. Computing started as an experiment to determine what could be done and quickly moved to fixed-purpose systems for military uses, such as code breaking and trajectory plotting, and governmental uses, such as census calculation. Those early computers evolved into general-purpose, multifunction mainframes, and thats when operating systems were born. In the 1960s, Moores Law predicted that the number of transistors on an integrated circuit would double every eighteen months, and that prediction has held true. Computers gained in functionality and shrunk in size, leading to a vast number of uses and a vast number and variety of operating systems. (See Chapter 20 for more details on the history of operating systems.)\\n',\n",
       " '    How, then, can we define what an operating system is? In general, we have no completely adequate definition of an operating system. Operating systems exist because they offer a reasonable way to solve the problem of creating a usable computing system. The fundamental goal of computer systems is to execute user programs and to make solving user problems easier. Computer hardware is constructed toward this goal. Since bare hardware alone is not particularly easy to use, application programs are developed. These programs require certain common operations, such as those controlling the I/O devices. The common functions of controlling and allocating resources are then brought together into one piece of software: the operating system.\\n',\n",
       " '    In addition, we have no universally accepted definition of what is part of the operating system. A simple viewpoint is that it includes everything a vendor ships when you order the operating system. The features included, however, vary greatly across systems. Some systems take up less than a megabyte of space and lack even a full-screen editor, whereas others require gigabytes of space and are based entirely on graphical windowing systems. A more common definition, and the one that we usually follow, is that the operating system is the one program running at all times on the computer usually called the kernel. (Along with the kernel, there are two other types of programs: system programs, which are associated with the operating system but are not necessarily part of the kernel, and application programs, which include all programs not associated with the operation of the system.)\\n',\n",
       " '   The matter of what constitutes an operating system became increasingly important as personal computers became more widespread and operating systems grew increasingly sophisticated. In 1998, the United States Department of Justice filed suit against Microsoft, in essence claiming that Microsoft included too much functionality in its operating systems and thus prevented application vendors from competing. (For example, a Web browser was an integral part of the operating systems.) As a result, Microsoft was found guilty of using its operating-system monopoly to limit competition.\\n',\n",
       " '   Today, however, if we look at operating systems for mobile devices, we see that once again the number of features constituting the operating system\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'is increasing. Mobile operating systems often include not only a core kernel but also middleware  a set of software frameworks that provide additional services to application developers. For example, each of the two most promi- nent mobile operating systems Apples iOS and Googles Android features a core kernel along with middleware that supports databases, multimedia, and graphics (to name a only few).\\n',\n",
       " '\\n',\n",
       " '1.2 Computer-System Organization\\n',\n",
       " 'Before we can explore the details of how computer systems operate, we need general knowledge of the structure of a computer system. In this section, we look at several parts of this structure. The section is mostly concerned with computer-system organization, so you can skim or skip it if you already understand the concepts.\\n',\n",
       " '\\n',\n",
       " '1.2.1 Computer-System Operation\\n',\n",
       " 'A modern general-purpose computer system consists of one or more CPUs and a number of device controllers connected through a common bus that provides access to shared memory (Figure 1.2). Each device controller is in charge of a specific type of device (for example, disk drives, audio devices, or video displays). The CPU and the device controllers can execute in parallel, competing for memory cycles. To ensure orderly access to the shared memory, a memory controller synchronizes access to the memory.\\n',\n",
       " '   For a computer to start running  for instance, when it is powered up or rebooted  it needs to have an initial program to run. This initial program, or bootstrap program, tends to be simple. Typically, it is stored within the computer hardware in read-only memory (ROM) or electrically erasable programmable read-only memory (EEPROM), known by the general term firmware. It initializes all aspects of the system, from CPU registers to device controllers to memory contents. The bootstrap program must know how to load the operating system and how to start executing that system. To accomplish\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Figure 1.2 A modern computer system.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'CPU\\tuser\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'I\\n',\n",
       " 'd\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '  I/O request\\n',\n",
       " '\\n',\n",
       " 'transfer done\\n',\n",
       " '  \\n',\n",
       " 'I/O request\\n',\n",
       " '\\n',\n",
       " 'transfer done\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Figure 1.3 Interrupt timeline for a single process doing output.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'this goal, the bootstrap program must locate the operating-system kernel and load it into memory.\\n',\n",
       " '   Once the kernel is loaded and executing, it can start providing services to the system and its users. Some services are provided outside of the kernel, by system programs that are loaded into memory at boot time to become system processes, or system daemons that run the entire time the kernel is running. On UNIX, the first system process is init, and it starts many other daemons. Once this phase is complete, the system is fully booted, and the system waits for some event to occur.\\n',\n",
       " '   The occurrence of an event is usually signaled by an interrupt from either the hardware or the software. Hardware may trigger an interrupt at any time by sending a signal to the CPU, usually by way of the system bus. Software may trigger an interrupt by executing a special operation called a system call (also called a monitor call).\\n',\n",
       " '   When the CPU is interrupted, it stops what it is doing and immediately transfers execution to a fixed location. The fixed location usually contains the starting address where the service routine for the interrupt is located. The interrupt service routine executes; on completion, the CPU resumes the interrupted computation. A timeline of this operation is shown in Figure 1.3.\\n',\n",
       " '    Interrupts are an important part of a computer architecture. Each computer design has its own interrupt mechanism, but several functions are common. The interrupt must transfer control to the appropriate interrupt service routine. The straightforward method for handling this transfer would be to invoke a generic routine to examine the interrupt information. The routine, in turn, would call the interrupt-specific handler. However, interrupts must be handled quickly. Since only a predefined number of interrupts is possible, a table of pointers to interrupt routines can be used instead to provide the necessary speed. The interrupt routine is called indirectly through the table, with no intermediate routine needed. Generally, the table of pointers is stored in low memory (the first hundred or so locations). These locations hold the addresses of the interrupt service routines for the various devices. This array, or interrupt vector, of addresses is then indexed by a unique device number, given with the interrupt request, to provide the address of the interrupt service routine for\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'the interrupting device. Operating systems as different as Windows and UNIX\\n',\n",
       " 'dispatch interrupts in this manner.\\n',\n",
       " '   The interrupt architecture must also save the address of the interrupted instruction. Many old designs simply stored the interrupt address in a fixed location or in a location indexed by the device number. More recent architectures store the return address on the system stack. If the interrupt routine needs to modify the processor state for instance, by modifying register values it must explicitly save the current state and then restore that state before returning. After the interrupt is serviced, the saved return address is loaded into the program counter, and the interrupted computation resumes as though the interrupt had not occurred.\\n',\n",
       " '\\n',\n",
       " '1.2.2 Storage Structure\\n',\n",
       " 'The CPU can load instructions only from memory, so any programs to run must be stored there. General-purpose computers run most of their programs from rewritable memory, called main memory (also called random-access memory, or RAM). Main memory commonly is implemented in a semiconductor technology called dynamic random-access memory (DRAM).\\n',\n",
       " '    Computers use other forms of memory as well. We have already mentioned read-only memory, ROM) and electrically erasable programmable read-only memory, EEPROM). Because ROM cannot be changed, only static programs, such as the bootstrap program described earlier, are stored there. The immutability of ROM is of use in game cartridges. EEPROM can be changed but cannot be changed frequently and so contains mostly static programs. For example, smartphones have EEPROM to store their factory-installed programs.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '   All forms of memory provide an array of bytes. Each byte has its own address. Interaction is achieved through a sequence of load or store instructions to specific memory addresses. The load instruction moves a byte or word from main memory to an internal register within the CPU, whereas the store instruction moves the content of a register to main memory. Aside from explicit loads and stores, the CPU automatically loads instructions from main memory for execution.\\n',\n",
       " '    A typical instruction execution cycle, as executed on a system with a von Neumann architecture, first fetches an instruction from memory and stores that instruction in the instruction register. The instruction is then decoded and may cause operands to be fetched from memory and stored in some internal register. After the instruction on the operands has been executed, the result may be stored back in memory. Notice that the memory unit sees only a stream of memory addresses. It does not know how they are generated (by the instruction counter, indexing, indirection, literal addresses, or some other means) or what they are for (instructions or data). Accordingly, we can ignore how a memory address is generated by a program. We are interested only in the sequence of memory addresses generated by the running program.\\n',\n",
       " '   Ideally, we want the programs and data to reside in main memory permanently. This arrangement usually is not possible for the following two reasons:\\n',\n",
       " '1. Main memory is usually too small to store all needed programs and data permanently.\\n',\n",
       " '2. Main memory is a volatile storage device that loses its contents when power is turned off or otherwise lost.\\n',\n",
       " 'Thus, most computer systems provide secondary storage as an extension of main memory. The main requirement for secondary storage is that it be able to hold large quantities of data permanently.\\n',\n",
       " '   The most common secondary-storage device is a magnetic disk, which provides storage for both programs and data. Most programs (system and application) are stored on a disk until they are loaded into memory. Many programs then use the disk as both the source and the destination of their processing. Hence, the proper management of disk storage is of central importance to a computer system, as we discuss in Chapter 10.\\n',\n",
       " '   In a larger sense, however, the storage structure that we have described consisting of registers, main memory, and magnetic disks  is only one of many possible storage systems. Others include cache memory, CD-ROM, magnetic tapes, and so on. Each storage system provides the basic functions of storing a datum and holding that datum until it is retrieved at a later time. The main differences among the various storage systems lie in speed, cost, size, and volatility.\\n',\n",
       " '    The wide variety of storage systems can be organized in a hierarchy (Figure 1.4) according to speed and cost. The higher levels are expensive, but they are fast. As we move down the hierarchy, the cost per bit generally decreases, whereas the access time generally increases. This trade-off is reasonable; if a given storage system were both faster and less expensive than another other properties being the same  then there would be no reason to use the slower, more expensive memory. In fact, many early storage devices, including paper\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Figure 1.4 Storage-device hierarchy.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'tape and core memories, are relegated to museums now that magnetic tape and semiconductor memory have become faster and cheaper. The top four levels of memory in Figure 1.4 may be constructed using semiconductor memory.\\n',\n",
       " '   In addition to differing in speed and cost, the various storage systems are either volatile or nonvolatile. As mentioned earlier, volatile storage loses its contents when the power to the device is removed. In the absence of expensive battery and generator backup systems, data must be written to nonvolatile storage for safekeeping. In the hierarchy shown in Figure 1.4, the storage systems above the solid-state disk are volatile, whereas those including the solid-state disk and below are nonvolatile.\\n',\n",
       " '   Solid-state disks have several variants but in general are faster than magnetic disks and are nonvolatile. One type of solid-state disk stores data in a large DRAM array during normal operation but also contains a hidden magnetic hard disk and a battery for backup power. If external power is interrupted, this solid-state disks controller copies the data from RAM to the magnetic disk. When external power is restored, the controller copies the data back into RAM. Another form of solid-state disk is flash memory, which is popular in cameras and personal digital assistants (PDAs), in robots, and increasingly for storage on general-purpose computers. Flash memory is slower than DRAM but needs no power to retain its contents. Another form of nonvolatile storage is NVRAM, which is DRAM with battery backup power. This memory can be as fast as DRAM and (as long as the battery lasts) is nonvolatile.\\n',\n",
       " '   The design of a complete memory system must balance all the factors just discussed: it must use only as much expensive memory as necessary while providing as much inexpensive, nonvolatile memory as possible. Caches can\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'be installed to improve performance where a large disparity in access time or transfer rate exists between two components.\\n',\n",
       " '\\n',\n",
       " '1.2.3 I/O Structure\\n',\n",
       " 'Storage is only one of many types of I/O devices within a computer. A large portion of operating system code is dedicated to managing I/O, both because of its importance to the reliability and performance of a system and because of the varying nature of the devices. Next, we provide an overview of I/O.\\n',\n",
       " '   A general-purpose computer system consists of CPUs and multiple device controllers that are connected through a common bus. Each device controller is in charge of a specific type of device. Depending on the controller, more than one device may be attached. For instance, seven or more devices can be attached to the small computer-systems interface (SCSI) controller. A device controller maintains some local buffer storage and a set of special-purpose registers. The device controller is responsible for moving the data between the peripheral devices that it controls and its local buffer storage. Typically, operating systems have a device driver for each device controller. This device driver understands the device controller and provides the rest of the operating system with a uniform interface to the device.\\n',\n",
       " '   To start an I/O operation, the device driver loads the appropriate registers within the device controller. The device controller, in turn, examines the contents of these registers to determine what action to take (such as read a character from the keyboard). The controller starts the transfer of data from the device to its local buffer. Once the transfer of data is complete, the device controller informs the device driver via an interrupt that it has finished its operation. The device driver then returns control to the operating system, possibly returning the data or a pointer to the data if the operation was a read. For other operations, the device driver returns status information.\\n',\n",
       " '   This form of interrupt-driven I/O is fine for moving small amounts of data but can produce high overhead when used for bulk data movement such as disk I/O. To solve this problem, direct memory access (DMA) is used. After setting up buffers, pointers, and counters for the I/O device, the device controller transfers an entire block of data directly to or from its own buffer storage to memory, with no intervention by the CPU. Only one interrupt is generated per block, to tell the device driver that the operation has completed, rather than the one interrupt per byte generated for low-speed devices. While the device controller is performing these operations, the CPU is available to accomplish other work.\\n',\n",
       " '   Some high-end systems use switch rather than bus architecture. On these systems, multiple components can talk to other components concurrently, rather than competing for cycles on a shared bus. In this case, DMA is even more effective. Figure 1.5 shows the interplay of all components of a computer system.\\n',\n",
       " '\\n',\n",
       " '1.3 Computer-System Architecture\\n',\n",
       " 'In Section 1.2, we introduced the general structure of a typical computer system. A computer system can be organized in a number of different ways, which we\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'instruction execution cycle\\n',\n",
       " '\\n',\n",
       " 'data movement\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'DMA\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " 'Figure 1.5 How a modern computer system works.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'can categorize roughly according to the number of general-purpose processors used.\\n',\n",
       " '\\n',\n",
       " '1.3.1 Single-Processor Systems\\n',\n",
       " 'Until recently, most computer systems used a single processor. On a single- processor system, there is one main CPU capable of executing a general-purpose instruction set, including instructions from user processes. Almost all single- processor systems have other special-purpose processors as well. They may come in the form of device-specific processors, such as disk, keyboard, and graphics controllers; or, on mainframes, they may come in the form of more general-purpose processors, such as I/O processors that move data rapidly among the components of the system.\\n',\n",
       " '   All of these special-purpose processors run a limited instruction set and do not run user processes. Sometimes, they are managed by the operating system, in that the operating system sends them information about their next task and monitors their status. For example, a disk-controller microprocessor receives a sequence of requests from the main CPU and implements its own disk queue and scheduling algorithm. This arrangement relieves the main CPU of the overhead of disk scheduling. PCs contain a microprocessor in the keyboard to convert the keystrokes into codes to be sent to the CPU. In other systems or circumstances, special-purpose processors are low-level components built into the hardware. The operating system cannot communicate with these processors; they do their jobs autonomously. The use of special-purpose microprocessors is common and does not turn a single-processor system into\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'a multiprocessor. If there is only one general-purpose CPU, then the system is a single-processor system.\\n',\n",
       " '\\n',\n",
       " '1.3.2 Multiprocessor Systems\\n',\n",
       " 'Within the past several years, multiprocessor systems (also known as parallel systems or multicore systems) have begun to dominate the landscape of computing. Such systems have two or more processors in close communication, sharing the computer bus and sometimes the clock, memory, and peripheral devices. Multiprocessor systems first appeared prominently appeared in servers and have since migrated to desktop and laptop systems. Recently, multiple processors have appeared on mobile devices such as smartphones and tablet computers.\\n',\n",
       " 'Multiprocessor systems have three main advantages:\\n',\n",
       " '\\n',\n",
       " '1. Increased throughput. By increasing the number of processors, we expect to get more work done in less time. The speed-up ratio with N processors is not N, however; rather, it is less than N. When multiple processors cooperate on a task, a certain amount of overhead is incurred in keeping all the parts working correctly. This overhead, plus contention for shared resources, lowers the expected gain from additional processors. Similarly, N programmers working closely together do not produce N times the amount of work a single programmer would produce.\\n',\n",
       " '2. Economy of scale. Multiprocessor systems can cost less than equivalent multiple single-processor systems, because they can share peripherals, mass storage, and power supplies. If several programs operate on the same set of data, it is cheaper to store those data on one disk and to have all the processors share them than to have many computers with local disks and many copies of the data.\\n',\n",
       " '3. Increased reliability. If functions can be distributed properly among several processors, then the failure of one processor will not halt the system, only slow it down. If we have ten processors and one fails, then each of the remaining nine processors can pick up a share of the work of the failed processor. Thus, the entire system runs only 10 percent slower, rather than failing altogether.\\n',\n",
       " '\\n',\n",
       " '   Increased reliability of a computer system is crucial in many applications. The ability to continue providing service proportional to the level of surviving hardware is called graceful degradation. Some systems go beyond graceful degradation and are called fault tolerant, because they can suffer a failure of any single component and still continue operation. Fault tolerance requires a mechanism to allow the failure to be detected, diagnosed, and, if possible, corrected. The HP NonStop (formerly Tandem) system uses both hardware and software duplication to ensure continued operation despite faults. The system consists of multiple pairs of CPUs, working in lockstep. Both processors in the pair execute each instruction and compare the results. If the results differ, then one CPU of the pair is at fault, and both are halted. The process that was being executed is then moved to another pair of CPUs, and the instruction that failed\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'is restarted. This solution is expensive, since it involves special hardware and considerable hardware duplication.\\n',\n",
       " '   The multiple-processor systems in use today are of two  types. Some systems use asymmetric multiprocessing, in which each processor is assigned a specific task. A boss processor controls the system; the other processors either look to the boss for instruction or have predefined tasks. This scheme defines a boss worker relationship. The boss processor schedules and allocates work to the worker processors.\\n',\n",
       " '   The most common systems use symmetric multiprocessing (SMP), in which each processor performs all tasks within the operating system. SMP means that all processors are peers; no boss worker relationship exists between processors. Figure 1.6 illustrates a typical SMP architecture. Notice that each processor has its own set of registers, as well as a private or local\\n',\n",
       " ' cache. However, all processors share physical memory. An example of an SMP system is AIX, a commercial version of UNIX designed by IBM. An AIX system can be configured to employ dozens of processors. The benefit of this model is that many processes can run simultaneously N processes can run if there are N CPUs without causing performance to deteriorate significantly. However, we must carefully control I/O to ensure that the data reach the appropriate processor. Also, since the CPUs are separate, one may be sitting idle while another is overloaded, resulting in inefficiencies. These inefficiencies can be avoided if the processors share certain data structures. A multiprocessor system of this form will  allow  processes and  resources such as memory to be shared dynamically among the various processors and can lower the variance among the processors. Such a system must be written carefully, as we shall see in Chapter 5. Virtually all modern operating systems including Windows, Mac OS X, and Linux now provide support for SMP.\\n',\n",
       " '   The difference between symmetric and asymmetric multiprocessing may result from either hardware or software. Special hardware can differentiate the multiple processors, or the software can be written to allow only one boss and multiple workers. For instance, Sun Microsystems operating system SunOS Version 4 provided asymmetric multiprocessing, whereas Version 5 (Solaris) is symmetric on the same hardware.\\n',\n",
       " '   Multiprocessing adds CPUs to increase computing power. If the CPU has an integrated memory controller, then adding CPUs can also increase the amount\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Figure 1.6 Symmetric multiprocessing architecture.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'of memory addressable in the system. Either way, multiprocessing can cause a system to change its memory access model from uniform memory access (UMA) to non-uniform memory access (NUMA). UMA is defined as the situation in which access to any RAM from any CPU takes the same amount of time. With NUMA, some parts of memory may take longer to access than other parts, creating a performance penalty. Operating systems can minimize the NUMA penalty through resource management, as discussed in Section 9.5.4.\\n',\n",
       " '   A recent trend in CPU design is to include multiple computing cores on a single chip. Such multiprocessor systems are termed multicore. They can be more efficient than multiple chips with single cores because on-chip communication is faster than between-chip communication. In addition, one chip with multiple cores uses significantly less power than multiple single-core chips.\\n',\n",
       " '   It is important to note that while multicore systems are multiprocessor systems, not all multiprocessor systems are multicore, as we shall see in Section\\n',\n",
       " '1.3.3. In our coverage of multiprocessor systems throughout this text, unless we state otherwise, we generally use the more contemporary term multicore, which excludes some multiprocessor systems.\\n',\n",
       " '   In Figure 1.7, we show a dual-core design with two cores on the same chip. In this design, each core has its own register set as well as its own local cache. Other designs might use a shared cache or a combination of local and shared caches. Aside from architectural considerations, such as cache, memory, and bus contention, these multicore CPUs appear to the operating system as N standard processors. This characteristic puts pressure on operating system designers and application programmers to make use of those processing cores.\\n',\n",
       " '    Finally, blade servers are a relatively recent development in which multiple processor boards, I/O boards, and networking boards are placed in the same chassis. The difference between these and traditional multiprocessor systems is that each blade-processor board boots independently and runs its own operating system. Some blade-server boards are multiprocessor as well, which blurs the lines between types of computers. In essence, these servers consist of multiple independent multiprocessor systems.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Figure 1.7 A dual-core design with two cores placed on the same chip.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '1.3.3 Clustered Systems\\n',\n",
       " 'Another type of multiprocessor system is a clustered system, which gathers together multiple CPUs. Clustered systems differ from the multiprocessor systems described in Section 1.3.2 in that they are composed of two or more individual systems or nodes joined together. Such systems are considered loosely coupled. Each node may be a single processor system or a multicore system. We should note that the definition of clustered is not concrete; many commercial packages wrestle to define a clustered system and why one form is better than another. The generally accepted definition is that clustered computers share storage and are closely linked via a local-area network LAN (as described in Chapter 17) or a faster interconnect, such as InfiniBand.\\n',\n",
       " '    Clustering is usually used to provide high-availability service that is, service will continue even if one or more systems in the cluster fail. Generally, we obtain high availability by adding a level of redundancy in the system. A layer of cluster software runs on the cluster nodes. Each node can monitor one or more of the others (over the LAN). If the monitored machine fails, the monitoring machine can take ownership of its storage and restart the applications that were running on the failed machine. The users and clients of the applications see only a brief interruption of service.\\n',\n",
       " '   Clustering can be structured asymmetrically or symmetrically. In asym- metric clustering, one machine is in hot-standby mode while the other is running the applications. The hot-standby host machine does nothing but monitor the active server. If that server fails, the hot-standby host becomes the active server. In symmetric clustering, two or more hosts are running applications and are monitoring each other. This structure is obviously more efficient, as it uses all of the available hardware. However it does require that more than one application be available to run.\\n',\n",
       " '   Since a cluster consists of several computer systems connected via a network, clusters can also be used to provide high-performance computing environments. Such systems can supply significantly greater computational power than single-processor or even SMP systems because they can run an application concurrently on all computers in the cluster. The application must have been written specifically to take advantage of the cluster, however. This involves a technique known as parallelization, which divides a program into separate components that run in parallel on individual computers in the cluster. Typically, these applications are designed so that once each computing node in the cluster has solved its portion of the problem, the results from all the nodes are combined into a final solution.\\n',\n",
       " '   Other forms of clusters include parallel clusters and clustering over a wide-area network (WAN) (as described in Chapter 17). Parallel clusters allow multiple hosts to access the same data on shared storage. Because most operating systems lack support for simultaneous data access by multiple hosts, parallel clusters usually require the use of special versions of software and special releases of applications. For example, Oracle Real Application Cluster is a version of Oracles database that has been designed to run on a parallel cluster. Each machine runs Oracle, and a layer of software tracks access to the shared disk. Each machine has full access to all data in the database. To provide this shared access, the system must also supply access control and locking to\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'ensure that no conflicting operations occur. This function, commonly known as a distributed lock manager (DLM), is included in some cluster technology. Cluster technology is changing rapidly. Some cluster products support dozens of systems in a cluster, as well as clustered nodes that are separated by miles. Many of these improvements are made possible by storage-area networks (SANs), as described in Section 10.3.3, which allow many systems to attach to a pool of storage. If the applications and their data are stored on the SAN, then the cluster software can assign the application to run on any host that is attached to the SAN. If the host fails, then any other host can take over. In a database cluster, dozens of hosts can share the same database, greatly increasing performance and reliability. Figure 1.8 depicts the general structure\\n',\n",
       " 'of a clustered system.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'interconnect\\tinterconnect\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Figure 1.8 General structure of a clustered system.\\n',\n",
       " '\\n',\n",
       " '1.4 Operating-System Structure\\t19\\n',\n",
       " '0\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Max\\n',\n",
       " '\\n',\n",
       " 'Figure 1.9 Memory layout for a multiprogramming system.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '1.4 Operating-System Structure\\n',\n",
       " 'Now that we have discussed basic computer-system organization and archi- tecture, we are ready to talk about operating systems. An operating system provides the environment within which programs are executed. Internally, operating systems vary greatly in their makeup, since they are organized along many different lines. There are, however, many commonalities, which we consider in this section.\\n',\n",
       " '   One of the most important aspects of operating systems is the ability to multiprogram. A single program cannot, in general, keep either the CPU or the I/O devices busy at all times. Single users frequently have multiple programs running. Multiprogramming increases CPU utilization by organizing jobs (code and data) so that the CPU always has one to execute.\\n',\n",
       " '   The idea is as follows: The operating system keeps several jobs in memory simultaneously (Figure 1.9). Since, in general, main memory is too small to accommodate all jobs, the jobs are kept initially on the disk in the job pool. This pool consists of all processes residing on disk awaiting allocation of main memory.\\n',\n",
       " '   The set of jobs in memory can be a subset of the jobs kept in the job pool. The operating system picks and begins to execute one of the jobs in memory. Eventually, the job may have to wait for some task, such as an I/O operation, to complete. In a non-multiprogrammed system, the CPU would sit idle. In a multiprogrammed system, the operating system simply switches to, and executes, another job. When that job needs to wait, the CPU switches to another job, and so on. Eventually, the first job finishes waiting and gets the CPU back. As long as at least one job needs to execute, the CPU is never idle.\\n',\n",
       " '   This idea is common in other life situations. A lawyer does not work for only one client at a time, for example. While one case is waiting to go to trial or have papers typed, the lawyer can work on another case. If he has enough clients, the lawyer will never be idle for lack of work. (Idle lawyers tend to become politicians, so there is a certain social value in keeping lawyers busy.)\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '   Multiprogrammed systems provide an environment in which the various system resources (for example, CPU, memory, and peripheral devices) are utilized effectively, but they do not provide for user interaction with the computer system. Time sharing (or multitasking) is a logical extension of multiprogramming. In time-sharing systems, the CPU executes multiple jobs by switching among them, but the switches occur so frequently that the users can interact with each program while it is running.\\n',\n",
       " '   Time sharing requires an interactive computer system, which provides direct communication between the user and the system. The user gives instructions to the operating system or to a program directly, using a input device such as a keyboard, mouse, touch pad, or touch screen, and waits for immediate results on an output device. Accordingly, the response time should be short  typically less than one second.\\n',\n",
       " '   A time-shared operating system allows many users to share the computer simultaneously. Since each action or command in a time-shared system tends to be short, only a little CPU time is needed for each user. As the system switches rapidly from one user to the next, each user is given the impression that the entire computer system is dedicated to his use, even though it is being shared among many users.\\n',\n",
       " '   A time-shared operating system uses CPU scheduling and multiprogram- ming to provide each user with a small portion of a time-shared computer. Each user has at least one separate program in memory. A program loaded into memory and executing is called a process. When a process executes, it typically executes for only a short time before it either finishes or needs to perform I/O. I/O may be interactive; that is, output goes to a display for the user, and input comes from a user keyboard, mouse, or other device. Since interactive I/O typically runs at people speeds, it may take a long time to complete. Input, for example, may be bounded by the users typing speed; seven characters per second is fast for people but incredibly slow for computers. Rather than let the CPU sit idle as this interactive input takes place, the operating system will rapidly switch the CPU to the program of some other user.\\n',\n",
       " '   Time sharing and multiprogramming require that several jobs be kept simultaneously in memory. If several jobs are ready to be brought into memory, and if there is not enough room for all of them, then the system must choose among them. Making this decision involves job scheduling, which we discuss in Chapter 6. When the operating system selects a job from the job pool, it loads that job into memory for execution. Having several programs in memory at the same time requires some form of memory management, which we cover in Chapters 8 and 9. In addition, if several jobs are ready to run at the same time, the system must choose which job will run first. Making this decision is CPU scheduling, which is also discussed in Chapter 6. Finally, running multiple jobs concurrently requires that their ability to affect one another be limited in all phases of the operating system, including process scheduling, disk storage, and memory management. We discuss these considerations throughout the text.\\n',\n",
       " '   In a time-sharing system, the operating system must ensure reasonable response time. This goal is sometimes accomplished through swapping, whereby processes are swapped in and out of main memory to the disk. A more common method for ensuring reasonable response time is virtual memory, a technique that allows the execution of a process that is not completely in\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'memory (Chapter 9). The main advantage of the virtual-memory scheme is that it enables users to run programs that are larger than actual physical memory. Further, it abstracts main memory into a large, uniform array of storage, separating logical memory as viewed by the user from physical memory. This arrangement frees programmers from concern over memory-storage limitations.\\n',\n",
       " '   A time-sharing system must also provide a file system (Chapters 11 and 12). The file system resides on a collection of disks; hence, disk management must be provided (Chapter 10). In addition, a time-sharing system provides a mechanism for protecting resources from inappropriate use (Chapter 14). To ensure orderly execution, the system must provide mechanisms for job synchronization and communication (Chapter 5), and it may ensure that jobs do not get stuck in a deadlock, forever waiting for one another (Chapter 7).\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '1.5 Operating-System Operations\\n',\n",
       " 'As mentioned earlier, modern operating systems are interrupt driven. If there are no processes to execute, no I/O devices to service, and no users to whom to respond, an operating system will sit quietly, waiting for something to happen. Events are almost always signaled by the occurrence of an interrupt or a trap. A trap (or an exception) is a software-generated interrupt caused either by an error (for example, division by zero or invalid memory access) or by a specific request from a user program that an operating-system service be performed. The interrupt-driven nature of an operating system defines that systems general structure. For each type of interrupt, separate segments of code in the operating system determine what action should be taken. An interrupt service routine is provided to deal with the interrupt.\\n',\n",
       " '   Since the operating system and the users share the hardware and software resources of the computer system, we need to make sure that an error in a user program could cause problems only for the one program running. With sharing, many processes could be adversely affected by a bug in one program. For example, if a process gets stuck in an infinite loop, this loop could prevent the correct operation of many other processes. More subtle errors can occur in a multiprogramming system, where one erroneous program might modify another program, the data of another program, or even the operating system itself.\\n',\n",
       " '   Without protection against these sorts of errors, either the computer must execute only one process at a time or all output must be suspect. A properly designed operating system must ensure that an incorrect (or malicious) program cannot cause other programs to execute incorrectly.\\n',\n",
       " '\\n',\n",
       " '1.5.1 Dual-Mode and Multimode Operation\\n',\n",
       " 'In order to ensure the proper execution of the operating system, we must be able to distinguish between the execution of operating-system code and user- defined code. The approach taken by most computer systems is to provide hardware support that allows us to differentiate among various modes of execution.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'user process\\n',\n",
       " 'user mode (mode bit = 1)\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=s[706:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20409"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(s)):\n",
    "    try: \n",
    "        s.remove('\\n')\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12045"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['   Internally, operating systems vary greatly in their makeup, since they are organized along many different lines. The design of a new operating system is a major task. It is important that the goals of the system be well defined before the design begins. These goals form the basis for choices among various algorithms and strategies.\\n',\n",
       " '   Because an operating system is large and complex, it must be created piece by piece. Each of these pieces should be a well-delineated portion of the system, with carefully defined inputs, outputs, and functions.\\n',\n",
       " 'Introduction\\n',\n",
       " 'An operating system is a program that manages a computers hardware. It also provides a basis for application programs and acts as an intermediary between the computer user and the computer hardware. An amazing aspect of operating systems is how they vary in accomplishing these tasks. Mainframe operating systems are designed primarily to optimize utilization of hardware. Personal computer (PC) operating systems support complex games, business applications, and everything in between. Operating systems for mobile com- puters provide an environment in which a user can easily interface with the computer to execute programs. Thus, some operating systems are designed to be convenient, others to be efflcient, and others to be some combination of the two.\\n',\n",
       " '    Before we can explore the details of computer system operation, we need to know something about system structure. We thus discuss the basic functions of system startup, I/O, and storage early in this chapter. We also describe the basic computer architecture that makes it possible to write a functional operating system.\\n',\n",
       " '   Because an operating system is large and complex, it must be created piece by piece. Each of these pieces should be a well-delineated portion of the system, with carefully defined inputs, outputs, and functions. In this chapter, we provide a general overview of the major components of a contemporary computer system as well as the functions provided by the operating system. Additionally, we cover several other topics to help set the stage for the remainder of this text: data structures used in operating systems, computing environments, and open-source operating systems.\\n',\n",
       " '3\\n',\n",
       " 'Figure 1.1 Abstract view of the components of a computer system.\\n',\n",
       " '1.1 What Operating Systems Do\\n',\n",
       " 'We begin our discussion by looking at the operating systems role in the overall computer system. A computer system can be divided roughly into four components: the hardware, the operating system, the application programs, and the users (Figure 1.1).\\n',\n",
       " '    The hardware the central processing unit (CPU), the memory, and the input/output (I/O) devices provides the basic computing resources for the system. The application programs such as word processors, spreadsheets, compilers, and Web browsers define the ways in which these resources are used to solve users computing problems. The operating system controls the hardware and coordinates its use among the various application programs for the various users.\\n',\n",
       " '   We can also view a computer system as consisting of hardware, software, and data. The operating system provides the means for proper use of these resources in the operation of the computer system. An operating system is similar to a government. Like a government, it performs no useful function by itself. It simply provides an environment within which other programs can do useful work.\\n',\n",
       " '   To understand more fully the operating systems role, we next explore operating systems from two viewpoints: that of the user and that of the system.\\n',\n",
       " '1.1.1 User View\\n',\n",
       " 'The users view of the computer varies according to the interface being used. Most computer users sit in front of a PC, consisting of a monitor, keyboard, mouse, and system unit. Such a system is designed for one user\\n',\n",
       " '1.1 What Operating Systems Do\\t5\\n',\n",
       " 'to monopolize its resources. The goal is to maximize the work (or play) that the user is performing. In this case, the operating system is designed mostly for ease of use, with some attention paid to performance and none paid to resource utilization  how various hardware and software resources are shared. Performance is, of course, important to the user; but such systems are optimized for the single-user experience rather than the requirements of multiple users.\\n',\n",
       " '   In other cases, a user sits at a terminal connected to a mainframe or a minicomputer. Other users are accessing the same computer through other terminals. These users share resources and may exchange information. The operating system in such cases is designed to maximize resource utilization to assure that all available CPU time, memory, and I/O are used efficiently and that no individual user takes more than her fair share.\\n',\n",
       " '   In still other cases, users sit at workstations connected to networks of other workstations and servers. These users have dedicated resources at their disposal, but they also share resources such as networking and servers, including file, compute, and print servers. Therefore, their operating system is designed to compromise between individual usability and resource utilization. Recently, many varieties of mobile computers, such as smartphones and tablets, have come into fashion. Most mobile computers are standalone units for individual users. Quite often, they are connected to networks through cellular or other wireless technologies. Increasingly, these mobile devices are replacing desktop and laptop computers for people who are primarily interested in using computers for e-mail and web browsing. The user interface for mobile computers generally features a touch screen, where the user interacts with the system by pressing and swiping fingers across the screen rather than using a\\n',\n",
       " 'physical keyboard and mouse.\\n',\n",
       " '   Some computers have little or no user view. For example, embedded computers in home devices and automobiles may have numeric keypads and may turn indicator lights on or off to show status, but they and their operating systems are designed primarily to run without user intervention.\\n',\n",
       " '1.1.2 System View\\n',\n",
       " 'From the computers point of view, the operating system is the program most intimately involved with the hardware. In this context, we can view an operating system as a resource allocator. A computer system has many resources that may be required to solve a problem: CPU time, memory space, file-storage space, I/O devices, and so on. The operating system acts as the manager of these resources. Facing numerous and possibly conflicting requests for resources, the operating system must decide how to allocate them to specific programs and users so that it can operate the computer system efficiently and fairly. As we have seen, resource allocation is especially important where many users access the same mainframe or minicomputer.\\n',\n",
       " '   A slightly different view of an operating system emphasizes the need to control the various I/O devices and user programs. An operating system is a control program. A control program manages the execution of user programs to prevent errors and improper use of the computer. It is especially concerned with the operation and control of I/O devices.\\n',\n",
       " '1.1.3 Defining Operating Systems\\n',\n",
       " 'By now, you can probably see that the term operating system covers many roles and functions. That is the case, at least in part, because of the myriad designs and uses of computers. Computers are present within toasters, cars, ships, spacecraft, homes, and businesses. They are the basis for game machines, music players, cable TV tuners, and industrial control systems. Although computers have a relatively short history, they have evolved rapidly. Computing started as an experiment to determine what could be done and quickly moved to fixed-purpose systems for military uses, such as code breaking and trajectory plotting, and governmental uses, such as census calculation. Those early computers evolved into general-purpose, multifunction mainframes, and thats when operating systems were born. In the 1960s, Moores Law predicted that the number of transistors on an integrated circuit would double every eighteen months, and that prediction has held true. Computers gained in functionality and shrunk in size, leading to a vast number of uses and a vast number and variety of operating systems. (See Chapter 20 for more details on the history of operating systems.)\\n',\n",
       " '    How, then, can we define what an operating system is? In general, we have no completely adequate definition of an operating system. Operating systems exist because they offer a reasonable way to solve the problem of creating a usable computing system. The fundamental goal of computer systems is to execute user programs and to make solving user problems easier. Computer hardware is constructed toward this goal. Since bare hardware alone is not particularly easy to use, application programs are developed. These programs require certain common operations, such as those controlling the I/O devices. The common functions of controlling and allocating resources are then brought together into one piece of software: the operating system.\\n',\n",
       " '    In addition, we have no universally accepted definition of what is part of the operating system. A simple viewpoint is that it includes everything a vendor ships when you order the operating system. The features included, however, vary greatly across systems. Some systems take up less than a megabyte of space and lack even a full-screen editor, whereas others require gigabytes of space and are based entirely on graphical windowing systems. A more common definition, and the one that we usually follow, is that the operating system is the one program running at all times on the computer usually called the kernel. (Along with the kernel, there are two other types of programs: system programs, which are associated with the operating system but are not necessarily part of the kernel, and application programs, which include all programs not associated with the operation of the system.)\\n',\n",
       " '   The matter of what constitutes an operating system became increasingly important as personal computers became more widespread and operating systems grew increasingly sophisticated. In 1998, the United States Department of Justice filed suit against Microsoft, in essence claiming that Microsoft included too much functionality in its operating systems and thus prevented application vendors from competing. (For example, a Web browser was an integral part of the operating systems.) As a result, Microsoft was found guilty of using its operating-system monopoly to limit competition.\\n',\n",
       " '   Today, however, if we look at operating systems for mobile devices, we see that once again the number of features constituting the operating system\\n',\n",
       " 'is increasing. Mobile operating systems often include not only a core kernel but also middleware  a set of software frameworks that provide additional services to application developers. For example, each of the two most promi- nent mobile operating systems Apples iOS and Googles Android features a core kernel along with middleware that supports databases, multimedia, and graphics (to name a only few).\\n',\n",
       " '1.2 Computer-System Organization\\n',\n",
       " 'Before we can explore the details of how computer systems operate, we need general knowledge of the structure of a computer system. In this section, we look at several parts of this structure. The section is mostly concerned with computer-system organization, so you can skim or skip it if you already understand the concepts.\\n',\n",
       " '1.2.1 Computer-System Operation\\n',\n",
       " 'A modern general-purpose computer system consists of one or more CPUs and a number of device controllers connected through a common bus that provides access to shared memory (Figure 1.2). Each device controller is in charge of a specific type of device (for example, disk drives, audio devices, or video displays). The CPU and the device controllers can execute in parallel, competing for memory cycles. To ensure orderly access to the shared memory, a memory controller synchronizes access to the memory.\\n',\n",
       " '   For a computer to start running  for instance, when it is powered up or rebooted  it needs to have an initial program to run. This initial program, or bootstrap program, tends to be simple. Typically, it is stored within the computer hardware in read-only memory (ROM) or electrically erasable programmable read-only memory (EEPROM), known by the general term firmware. It initializes all aspects of the system, from CPU registers to device controllers to memory contents. The bootstrap program must know how to load the operating system and how to start executing that system. To accomplish\\n',\n",
       " 'Figure 1.2 A modern computer system.\\n',\n",
       " 'CPU\\tuser\\n',\n",
       " 'I\\n',\n",
       " 'd\\n',\n",
       " '  I/O request\\n',\n",
       " 'transfer done\\n',\n",
       " '  \\n',\n",
       " 'I/O request\\n',\n",
       " 'transfer done\\n',\n",
       " 'Figure 1.3 Interrupt timeline for a single process doing output.\\n',\n",
       " 'this goal, the bootstrap program must locate the operating-system kernel and load it into memory.\\n',\n",
       " '   Once the kernel is loaded and executing, it can start providing services to the system and its users. Some services are provided outside of the kernel, by system programs that are loaded into memory at boot time to become system processes, or system daemons that run the entire time the kernel is running. On UNIX, the first system process is init, and it starts many other daemons. Once this phase is complete, the system is fully booted, and the system waits for some event to occur.\\n',\n",
       " '   The occurrence of an event is usually signaled by an interrupt from either the hardware or the software. Hardware may trigger an interrupt at any time by sending a signal to the CPU, usually by way of the system bus. Software may trigger an interrupt by executing a special operation called a system call (also called a monitor call).\\n',\n",
       " '   When the CPU is interrupted, it stops what it is doing and immediately transfers execution to a fixed location. The fixed location usually contains the starting address where the service routine for the interrupt is located. The interrupt service routine executes; on completion, the CPU resumes the interrupted computation. A timeline of this operation is shown in Figure 1.3.\\n',\n",
       " '    Interrupts are an important part of a computer architecture. Each computer design has its own interrupt mechanism, but several functions are common. The interrupt must transfer control to the appropriate interrupt service routine. The straightforward method for handling this transfer would be to invoke a generic routine to examine the interrupt information. The routine, in turn, would call the interrupt-specific handler. However, interrupts must be handled quickly. Since only a predefined number of interrupts is possible, a table of pointers to interrupt routines can be used instead to provide the necessary speed. The interrupt routine is called indirectly through the table, with no intermediate routine needed. Generally, the table of pointers is stored in low memory (the first hundred or so locations). These locations hold the addresses of the interrupt service routines for the various devices. This array, or interrupt vector, of addresses is then indexed by a unique device number, given with the interrupt request, to provide the address of the interrupt service routine for\\n',\n",
       " 'the interrupting device. Operating systems as different as Windows and UNIX\\n',\n",
       " 'dispatch interrupts in this manner.\\n',\n",
       " '   The interrupt architecture must also save the address of the interrupted instruction. Many old designs simply stored the interrupt address in a fixed location or in a location indexed by the device number. More recent architectures store the return address on the system stack. If the interrupt routine needs to modify the processor state for instance, by modifying register values it must explicitly save the current state and then restore that state before returning. After the interrupt is serviced, the saved return address is loaded into the program counter, and the interrupted computation resumes as though the interrupt had not occurred.\\n',\n",
       " '1.2.2 Storage Structure\\n',\n",
       " 'The CPU can load instructions only from memory, so any programs to run must be stored there. General-purpose computers run most of their programs from rewritable memory, called main memory (also called random-access memory, or RAM). Main memory commonly is implemented in a semiconductor technology called dynamic random-access memory (DRAM).\\n',\n",
       " '    Computers use other forms of memory as well. We have already mentioned read-only memory, ROM) and electrically erasable programmable read-only memory, EEPROM). Because ROM cannot be changed, only static programs, such as the bootstrap program described earlier, are stored there. The immutability of ROM is of use in game cartridges. EEPROM can be changed but cannot be changed frequently and so contains mostly static programs. For example, smartphones have EEPROM to store their factory-installed programs.\\n',\n",
       " '   All forms of memory provide an array of bytes. Each byte has its own address. Interaction is achieved through a sequence of load or store instructions to specific memory addresses. The load instruction moves a byte or word from main memory to an internal register within the CPU, whereas the store instruction moves the content of a register to main memory. Aside from explicit loads and stores, the CPU automatically loads instructions from main memory for execution.\\n',\n",
       " '    A typical instruction execution cycle, as executed on a system with a von Neumann architecture, first fetches an instruction from memory and stores that instruction in the instruction register. The instruction is then decoded and may cause operands to be fetched from memory and stored in some internal register. After the instruction on the operands has been executed, the result may be stored back in memory. Notice that the memory unit sees only a stream of memory addresses. It does not know how they are generated (by the instruction counter, indexing, indirection, literal addresses, or some other means) or what they are for (instructions or data). Accordingly, we can ignore how a memory address is generated by a program. We are interested only in the sequence of memory addresses generated by the running program.\\n',\n",
       " '   Ideally, we want the programs and data to reside in main memory permanently. This arrangement usually is not possible for the following two reasons:\\n',\n",
       " '1. Main memory is usually too small to store all needed programs and data permanently.\\n',\n",
       " '2. Main memory is a volatile storage device that loses its contents when power is turned off or otherwise lost.\\n',\n",
       " 'Thus, most computer systems provide secondary storage as an extension of main memory. The main requirement for secondary storage is that it be able to hold large quantities of data permanently.\\n',\n",
       " '   The most common secondary-storage device is a magnetic disk, which provides storage for both programs and data. Most programs (system and application) are stored on a disk until they are loaded into memory. Many programs then use the disk as both the source and the destination of their processing. Hence, the proper management of disk storage is of central importance to a computer system, as we discuss in Chapter 10.\\n',\n",
       " '   In a larger sense, however, the storage structure that we have described consisting of registers, main memory, and magnetic disks  is only one of many possible storage systems. Others include cache memory, CD-ROM, magnetic tapes, and so on. Each storage system provides the basic functions of storing a datum and holding that datum until it is retrieved at a later time. The main differences among the various storage systems lie in speed, cost, size, and volatility.\\n',\n",
       " '    The wide variety of storage systems can be organized in a hierarchy (Figure 1.4) according to speed and cost. The higher levels are expensive, but they are fast. As we move down the hierarchy, the cost per bit generally decreases, whereas the access time generally increases. This trade-off is reasonable; if a given storage system were both faster and less expensive than another other properties being the same  then there would be no reason to use the slower, more expensive memory. In fact, many early storage devices, including paper\\n',\n",
       " 'Figure 1.4 Storage-device hierarchy.\\n',\n",
       " 'tape and core memories, are relegated to museums now that magnetic tape and semiconductor memory have become faster and cheaper. The top four levels of memory in Figure 1.4 may be constructed using semiconductor memory.\\n',\n",
       " '   In addition to differing in speed and cost, the various storage systems are either volatile or nonvolatile. As mentioned earlier, volatile storage loses its contents when the power to the device is removed. In the absence of expensive battery and generator backup systems, data must be written to nonvolatile storage for safekeeping. In the hierarchy shown in Figure 1.4, the storage systems above the solid-state disk are volatile, whereas those including the solid-state disk and below are nonvolatile.\\n',\n",
       " '   Solid-state disks have several variants but in general are faster than magnetic disks and are nonvolatile. One type of solid-state disk stores data in a large DRAM array during normal operation but also contains a hidden magnetic hard disk and a battery for backup power. If external power is interrupted, this solid-state disks controller copies the data from RAM to the magnetic disk. When external power is restored, the controller copies the data back into RAM. Another form of solid-state disk is flash memory, which is popular in cameras and personal digital assistants (PDAs), in robots, and increasingly for storage on general-purpose computers. Flash memory is slower than DRAM but needs no power to retain its contents. Another form of nonvolatile storage is NVRAM, which is DRAM with battery backup power. This memory can be as fast as DRAM and (as long as the battery lasts) is nonvolatile.\\n',\n",
       " '   The design of a complete memory system must balance all the factors just discussed: it must use only as much expensive memory as necessary while providing as much inexpensive, nonvolatile memory as possible. Caches can\\n',\n",
       " 'be installed to improve performance where a large disparity in access time or transfer rate exists between two components.\\n',\n",
       " '1.2.3 I/O Structure\\n',\n",
       " 'Storage is only one of many types of I/O devices within a computer. A large portion of operating system code is dedicated to managing I/O, both because of its importance to the reliability and performance of a system and because of the varying nature of the devices. Next, we provide an overview of I/O.\\n',\n",
       " '   A general-purpose computer system consists of CPUs and multiple device controllers that are connected through a common bus. Each device controller is in charge of a specific type of device. Depending on the controller, more than one device may be attached. For instance, seven or more devices can be attached to the small computer-systems interface (SCSI) controller. A device controller maintains some local buffer storage and a set of special-purpose registers. The device controller is responsible for moving the data between the peripheral devices that it controls and its local buffer storage. Typically, operating systems have a device driver for each device controller. This device driver understands the device controller and provides the rest of the operating system with a uniform interface to the device.\\n',\n",
       " '   To start an I/O operation, the device driver loads the appropriate registers within the device controller. The device controller, in turn, examines the contents of these registers to determine what action to take (such as read a character from the keyboard). The controller starts the transfer of data from the device to its local buffer. Once the transfer of data is complete, the device controller informs the device driver via an interrupt that it has finished its operation. The device driver then returns control to the operating system, possibly returning the data or a pointer to the data if the operation was a read. For other operations, the device driver returns status information.\\n',\n",
       " '   This form of interrupt-driven I/O is fine for moving small amounts of data but can produce high overhead when used for bulk data movement such as disk I/O. To solve this problem, direct memory access (DMA) is used. After setting up buffers, pointers, and counters for the I/O device, the device controller transfers an entire block of data directly to or from its own buffer storage to memory, with no intervention by the CPU. Only one interrupt is generated per block, to tell the device driver that the operation has completed, rather than the one interrupt per byte generated for low-speed devices. While the device controller is performing these operations, the CPU is available to accomplish other work.\\n',\n",
       " '   Some high-end systems use switch rather than bus architecture. On these systems, multiple components can talk to other components concurrently, rather than competing for cycles on a shared bus. In this case, DMA is even more effective. Figure 1.5 shows the interplay of all components of a computer system.\\n',\n",
       " '1.3 Computer-System Architecture\\n',\n",
       " 'In Section 1.2, we introduced the general structure of a typical computer system. A computer system can be organized in a number of different ways, which we\\n',\n",
       " 'instruction execution cycle\\n',\n",
       " 'data movement\\n',\n",
       " 'DMA\\n',\n",
       " '\\t\\n',\n",
       " 'Figure 1.5 How a modern computer system works.\\n',\n",
       " 'can categorize roughly according to the number of general-purpose processors used.\\n',\n",
       " '1.3.1 Single-Processor Systems\\n',\n",
       " 'Until recently, most computer systems used a single processor. On a single- processor system, there is one main CPU capable of executing a general-purpose instruction set, including instructions from user processes. Almost all single- processor systems have other special-purpose processors as well. They may come in the form of device-specific processors, such as disk, keyboard, and graphics controllers; or, on mainframes, they may come in the form of more general-purpose processors, such as I/O processors that move data rapidly among the components of the system.\\n',\n",
       " '   All of these special-purpose processors run a limited instruction set and do not run user processes. Sometimes, they are managed by the operating system, in that the operating system sends them information about their next task and monitors their status. For example, a disk-controller microprocessor receives a sequence of requests from the main CPU and implements its own disk queue and scheduling algorithm. This arrangement relieves the main CPU of the overhead of disk scheduling. PCs contain a microprocessor in the keyboard to convert the keystrokes into codes to be sent to the CPU. In other systems or circumstances, special-purpose processors are low-level components built into the hardware. The operating system cannot communicate with these processors; they do their jobs autonomously. The use of special-purpose microprocessors is common and does not turn a single-processor system into\\n',\n",
       " 'a multiprocessor. If there is only one general-purpose CPU, then the system is a single-processor system.\\n',\n",
       " '1.3.2 Multiprocessor Systems\\n',\n",
       " 'Within the past several years, multiprocessor systems (also known as parallel systems or multicore systems) have begun to dominate the landscape of computing. Such systems have two or more processors in close communication, sharing the computer bus and sometimes the clock, memory, and peripheral devices. Multiprocessor systems first appeared prominently appeared in servers and have since migrated to desktop and laptop systems. Recently, multiple processors have appeared on mobile devices such as smartphones and tablet computers.\\n',\n",
       " 'Multiprocessor systems have three main advantages:\\n',\n",
       " '1. Increased throughput. By increasing the number of processors, we expect to get more work done in less time. The speed-up ratio with N processors is not N, however; rather, it is less than N. When multiple processors cooperate on a task, a certain amount of overhead is incurred in keeping all the parts working correctly. This overhead, plus contention for shared resources, lowers the expected gain from additional processors. Similarly, N programmers working closely together do not produce N times the amount of work a single programmer would produce.\\n',\n",
       " '2. Economy of scale. Multiprocessor systems can cost less than equivalent multiple single-processor systems, because they can share peripherals, mass storage, and power supplies. If several programs operate on the same set of data, it is cheaper to store those data on one disk and to have all the processors share them than to have many computers with local disks and many copies of the data.\\n',\n",
       " '3. Increased reliability. If functions can be distributed properly among several processors, then the failure of one processor will not halt the system, only slow it down. If we have ten processors and one fails, then each of the remaining nine processors can pick up a share of the work of the failed processor. Thus, the entire system runs only 10 percent slower, rather than failing altogether.\\n',\n",
       " '   Increased reliability of a computer system is crucial in many applications. The ability to continue providing service proportional to the level of surviving hardware is called graceful degradation. Some systems go beyond graceful degradation and are called fault tolerant, because they can suffer a failure of any single component and still continue operation. Fault tolerance requires a mechanism to allow the failure to be detected, diagnosed, and, if possible, corrected. The HP NonStop (formerly Tandem) system uses both hardware and software duplication to ensure continued operation despite faults. The system consists of multiple pairs of CPUs, working in lockstep. Both processors in the pair execute each instruction and compare the results. If the results differ, then one CPU of the pair is at fault, and both are halted. The process that was being executed is then moved to another pair of CPUs, and the instruction that failed\\n',\n",
       " 'is restarted. This solution is expensive, since it involves special hardware and considerable hardware duplication.\\n',\n",
       " '   The multiple-processor systems in use today are of two  types. Some systems use asymmetric multiprocessing, in which each processor is assigned a specific task. A boss processor controls the system; the other processors either look to the boss for instruction or have predefined tasks. This scheme defines a boss worker relationship. The boss processor schedules and allocates work to the worker processors.\\n',\n",
       " '   The most common systems use symmetric multiprocessing (SMP), in which each processor performs all tasks within the operating system. SMP means that all processors are peers; no boss worker relationship exists between processors. Figure 1.6 illustrates a typical SMP architecture. Notice that each processor has its own set of registers, as well as a private or local\\n',\n",
       " ' cache. However, all processors share physical memory. An example of an SMP system is AIX, a commercial version of UNIX designed by IBM. An AIX system can be configured to employ dozens of processors. The benefit of this model is that many processes can run simultaneously N processes can run if there are N CPUs without causing performance to deteriorate significantly. However, we must carefully control I/O to ensure that the data reach the appropriate processor. Also, since the CPUs are separate, one may be sitting idle while another is overloaded, resulting in inefficiencies. These inefficiencies can be avoided if the processors share certain data structures. A multiprocessor system of this form will  allow  processes and  resources such as memory to be shared dynamically among the various processors and can lower the variance among the processors. Such a system must be written carefully, as we shall see in Chapter 5. Virtually all modern operating systems including Windows, Mac OS X, and Linux now provide support for SMP.\\n',\n",
       " '   The difference between symmetric and asymmetric multiprocessing may result from either hardware or software. Special hardware can differentiate the multiple processors, or the software can be written to allow only one boss and multiple workers. For instance, Sun Microsystems operating system SunOS Version 4 provided asymmetric multiprocessing, whereas Version 5 (Solaris) is symmetric on the same hardware.\\n',\n",
       " '   Multiprocessing adds CPUs to increase computing power. If the CPU has an integrated memory controller, then adding CPUs can also increase the amount\\n',\n",
       " 'Figure 1.6 Symmetric multiprocessing architecture.\\n',\n",
       " 'of memory addressable in the system. Either way, multiprocessing can cause a system to change its memory access model from uniform memory access (UMA) to non-uniform memory access (NUMA). UMA is defined as the situation in which access to any RAM from any CPU takes the same amount of time. With NUMA, some parts of memory may take longer to access than other parts, creating a performance penalty. Operating systems can minimize the NUMA penalty through resource management, as discussed in Section 9.5.4.\\n',\n",
       " '   A recent trend in CPU design is to include multiple computing cores on a single chip. Such multiprocessor systems are termed multicore. They can be more efficient than multiple chips with single cores because on-chip communication is faster than between-chip communication. In addition, one chip with multiple cores uses significantly less power than multiple single-core chips.\\n',\n",
       " '   It is important to note that while multicore systems are multiprocessor systems, not all multiprocessor systems are multicore, as we shall see in Section\\n',\n",
       " '1.3.3. In our coverage of multiprocessor systems throughout this text, unless we state otherwise, we generally use the more contemporary term multicore, which excludes some multiprocessor systems.\\n',\n",
       " '   In Figure 1.7, we show a dual-core design with two cores on the same chip. In this design, each core has its own register set as well as its own local cache. Other designs might use a shared cache or a combination of local and shared caches. Aside from architectural considerations, such as cache, memory, and bus contention, these multicore CPUs appear to the operating system as N standard processors. This characteristic puts pressure on operating system designers and application programmers to make use of those processing cores.\\n',\n",
       " '    Finally, blade servers are a relatively recent development in which multiple processor boards, I/O boards, and networking boards are placed in the same chassis. The difference between these and traditional multiprocessor systems is that each blade-processor board boots independently and runs its own operating system. Some blade-server boards are multiprocessor as well, which blurs the lines between types of computers. In essence, these servers consist of multiple independent multiprocessor systems.\\n',\n",
       " 'Figure 1.7 A dual-core design with two cores placed on the same chip.\\n',\n",
       " '1.3.3 Clustered Systems\\n',\n",
       " 'Another type of multiprocessor system is a clustered system, which gathers together multiple CPUs. Clustered systems differ from the multiprocessor systems described in Section 1.3.2 in that they are composed of two or more individual systems or nodes joined together. Such systems are considered loosely coupled. Each node may be a single processor system or a multicore system. We should note that the definition of clustered is not concrete; many commercial packages wrestle to define a clustered system and why one form is better than another. The generally accepted definition is that clustered computers share storage and are closely linked via a local-area network LAN (as described in Chapter 17) or a faster interconnect, such as InfiniBand.\\n',\n",
       " '    Clustering is usually used to provide high-availability service that is, service will continue even if one or more systems in the cluster fail. Generally, we obtain high availability by adding a level of redundancy in the system. A layer of cluster software runs on the cluster nodes. Each node can monitor one or more of the others (over the LAN). If the monitored machine fails, the monitoring machine can take ownership of its storage and restart the applications that were running on the failed machine. The users and clients of the applications see only a brief interruption of service.\\n',\n",
       " '   Clustering can be structured asymmetrically or symmetrically. In asym- metric clustering, one machine is in hot-standby mode while the other is running the applications. The hot-standby host machine does nothing but monitor the active server. If that server fails, the hot-standby host becomes the active server. In symmetric clustering, two or more hosts are running applications and are monitoring each other. This structure is obviously more efficient, as it uses all of the available hardware. However it does require that more than one application be available to run.\\n',\n",
       " '   Since a cluster consists of several computer systems connected via a network, clusters can also be used to provide high-performance computing environments. Such systems can supply significantly greater computational power than single-processor or even SMP systems because they can run an application concurrently on all computers in the cluster. The application must have been written specifically to take advantage of the cluster, however. This involves a technique known as parallelization, which divides a program into separate components that run in parallel on individual computers in the cluster. Typically, these applications are designed so that once each computing node in the cluster has solved its portion of the problem, the results from all the nodes are combined into a final solution.\\n',\n",
       " '   Other forms of clusters include parallel clusters and clustering over a wide-area network (WAN) (as described in Chapter 17). Parallel clusters allow multiple hosts to access the same data on shared storage. Because most operating systems lack support for simultaneous data access by multiple hosts, parallel clusters usually require the use of special versions of software and special releases of applications. For example, Oracle Real Application Cluster is a version of Oracles database that has been designed to run on a parallel cluster. Each machine runs Oracle, and a layer of software tracks access to the shared disk. Each machine has full access to all data in the database. To provide this shared access, the system must also supply access control and locking to\\n',\n",
       " 'ensure that no conflicting operations occur. This function, commonly known as a distributed lock manager (DLM), is included in some cluster technology. Cluster technology is changing rapidly. Some cluster products support dozens of systems in a cluster, as well as clustered nodes that are separated by miles. Many of these improvements are made possible by storage-area networks (SANs), as described in Section 10.3.3, which allow many systems to attach to a pool of storage. If the applications and their data are stored on the SAN, then the cluster software can assign the application to run on any host that is attached to the SAN. If the host fails, then any other host can take over. In a database cluster, dozens of hosts can share the same database, greatly increasing performance and reliability. Figure 1.8 depicts the general structure\\n',\n",
       " 'of a clustered system.\\n',\n",
       " 'interconnect\\tinterconnect\\n',\n",
       " '\\t\\n',\n",
       " 'Figure 1.8 General structure of a clustered system.\\n',\n",
       " '1.4 Operating-System Structure\\t19\\n',\n",
       " '0\\n',\n",
       " 'Max\\n',\n",
       " 'Figure 1.9 Memory layout for a multiprogramming system.\\n',\n",
       " '1.4 Operating-System Structure\\n',\n",
       " 'Now that we have discussed basic computer-system organization and archi- tecture, we are ready to talk about operating systems. An operating system provides the environment within which programs are executed. Internally, operating systems vary greatly in their makeup, since they are organized along many different lines. There are, however, many commonalities, which we consider in this section.\\n',\n",
       " '   One of the most important aspects of operating systems is the ability to multiprogram. A single program cannot, in general, keep either the CPU or the I/O devices busy at all times. Single users frequently have multiple programs running. Multiprogramming increases CPU utilization by organizing jobs (code and data) so that the CPU always has one to execute.\\n',\n",
       " '   The idea is as follows: The operating system keeps several jobs in memory simultaneously (Figure 1.9). Since, in general, main memory is too small to accommodate all jobs, the jobs are kept initially on the disk in the job pool. This pool consists of all processes residing on disk awaiting allocation of main memory.\\n',\n",
       " '   The set of jobs in memory can be a subset of the jobs kept in the job pool. The operating system picks and begins to execute one of the jobs in memory. Eventually, the job may have to wait for some task, such as an I/O operation, to complete. In a non-multiprogrammed system, the CPU would sit idle. In a multiprogrammed system, the operating system simply switches to, and executes, another job. When that job needs to wait, the CPU switches to another job, and so on. Eventually, the first job finishes waiting and gets the CPU back. As long as at least one job needs to execute, the CPU is never idle.\\n',\n",
       " '   This idea is common in other life situations. A lawyer does not work for only one client at a time, for example. While one case is waiting to go to trial or have papers typed, the lawyer can work on another case. If he has enough clients, the lawyer will never be idle for lack of work. (Idle lawyers tend to become politicians, so there is a certain social value in keeping lawyers busy.)\\n',\n",
       " '   Multiprogrammed systems provide an environment in which the various system resources (for example, CPU, memory, and peripheral devices) are utilized effectively, but they do not provide for user interaction with the computer system. Time sharing (or multitasking) is a logical extension of multiprogramming. In time-sharing systems, the CPU executes multiple jobs by switching among them, but the switches occur so frequently that the users can interact with each program while it is running.\\n',\n",
       " '   Time sharing requires an interactive computer system, which provides direct communication between the user and the system. The user gives instructions to the operating system or to a program directly, using a input device such as a keyboard, mouse, touch pad, or touch screen, and waits for immediate results on an output device. Accordingly, the response time should be short  typically less than one second.\\n',\n",
       " '   A time-shared operating system allows many users to share the computer simultaneously. Since each action or command in a time-shared system tends to be short, only a little CPU time is needed for each user. As the system switches rapidly from one user to the next, each user is given the impression that the entire computer system is dedicated to his use, even though it is being shared among many users.\\n',\n",
       " '   A time-shared operating system uses CPU scheduling and multiprogram- ming to provide each user with a small portion of a time-shared computer. Each user has at least one separate program in memory. A program loaded into memory and executing is called a process. When a process executes, it typically executes for only a short time before it either finishes or needs to perform I/O. I/O may be interactive; that is, output goes to a display for the user, and input comes from a user keyboard, mouse, or other device. Since interactive I/O typically runs at people speeds, it may take a long time to complete. Input, for example, may be bounded by the users typing speed; seven characters per second is fast for people but incredibly slow for computers. Rather than let the CPU sit idle as this interactive input takes place, the operating system will rapidly switch the CPU to the program of some other user.\\n',\n",
       " '   Time sharing and multiprogramming require that several jobs be kept simultaneously in memory. If several jobs are ready to be brought into memory, and if there is not enough room for all of them, then the system must choose among them. Making this decision involves job scheduling, which we discuss in Chapter 6. When the operating system selects a job from the job pool, it loads that job into memory for execution. Having several programs in memory at the same time requires some form of memory management, which we cover in Chapters 8 and 9. In addition, if several jobs are ready to run at the same time, the system must choose which job will run first. Making this decision is CPU scheduling, which is also discussed in Chapter 6. Finally, running multiple jobs concurrently requires that their ability to affect one another be limited in all phases of the operating system, including process scheduling, disk storage, and memory management. We discuss these considerations throughout the text.\\n',\n",
       " '   In a time-sharing system, the operating system must ensure reasonable response time. This goal is sometimes accomplished through swapping, whereby processes are swapped in and out of main memory to the disk. A more common method for ensuring reasonable response time is virtual memory, a technique that allows the execution of a process that is not completely in\\n',\n",
       " 'memory (Chapter 9). The main advantage of the virtual-memory scheme is that it enables users to run programs that are larger than actual physical memory. Further, it abstracts main memory into a large, uniform array of storage, separating logical memory as viewed by the user from physical memory. This arrangement frees programmers from concern over memory-storage limitations.\\n',\n",
       " '   A time-sharing system must also provide a file system (Chapters 11 and 12). The file system resides on a collection of disks; hence, disk management must be provided (Chapter 10). In addition, a time-sharing system provides a mechanism for protecting resources from inappropriate use (Chapter 14). To ensure orderly execution, the system must provide mechanisms for job synchronization and communication (Chapter 5), and it may ensure that jobs do not get stuck in a deadlock, forever waiting for one another (Chapter 7).\\n',\n",
       " '1.5 Operating-System Operations\\n',\n",
       " 'As mentioned earlier, modern operating systems are interrupt driven. If there are no processes to execute, no I/O devices to service, and no users to whom to respond, an operating system will sit quietly, waiting for something to happen. Events are almost always signaled by the occurrence of an interrupt or a trap. A trap (or an exception) is a software-generated interrupt caused either by an error (for example, division by zero or invalid memory access) or by a specific request from a user program that an operating-system service be performed. The interrupt-driven nature of an operating system defines that systems general structure. For each type of interrupt, separate segments of code in the operating system determine what action should be taken. An interrupt service routine is provided to deal with the interrupt.\\n',\n",
       " '   Since the operating system and the users share the hardware and software resources of the computer system, we need to make sure that an error in a user program could cause problems only for the one program running. With sharing, many processes could be adversely affected by a bug in one program. For example, if a process gets stuck in an infinite loop, this loop could prevent the correct operation of many other processes. More subtle errors can occur in a multiprogramming system, where one erroneous program might modify another program, the data of another program, or even the operating system itself.\\n',\n",
       " '   Without protection against these sorts of errors, either the computer must execute only one process at a time or all output must be suspect. A properly designed operating system must ensure that an incorrect (or malicious) program cannot cause other programs to execute incorrectly.\\n',\n",
       " '1.5.1 Dual-Mode and Multimode Operation\\n',\n",
       " 'In order to ensure the proper execution of the operating system, we must be able to distinguish between the execution of operating-system code and user- defined code. The approach taken by most computer systems is to provide hardware support that allows us to differentiate among various modes of execution.\\n',\n",
       " 'user process\\n',\n",
       " 'user mode (mode bit = 1)\\n',\n",
       " 'kernel mode (mode bit = 0)\\n',\n",
       " 'Figure 1.10 Transition from user to kernel mode.\\n',\n",
       " '   At the very least, we need two separate modes of operation: user mode and kernel mode (also called supervisor mode, system mode, or privileged mode). A bit, called the mode bit, is added to the hardware of the computer to indicate the current mode: kernel (0) or user (1). With the mode bit, we can distinguish between a task that is executed on behalf of the operating system and one that is executed on behalf of the user. When the computer system is executing on behalf of a user application, the system is in user mode. However, when a user application requests a service from the operating system (via a system call), the system must transition from user to kernel mode to fulfill the request. This is shown in Figure 1.10. As we shall see, this architectural enhancement is useful for many other aspects of system operation as well.\\n',\n",
       " '   At system boot time, the hardware starts in kernel mode. The operating system is then loaded and starts user applications in user mode. Whenever a trap or interrupt occurs, the hardware switches from user mode to kernel mode (that is, changes the state of the mode bit to 0). Thus, whenever the operating system gains control of the computer, it is in kernel mode. The system always switches to user mode (by setting the mode bit to 1) before passing control to a user program.\\n',\n",
       " '   The dual mode of operation provides us with the means for protecting the operating system from errant users  and errant users from one another. We accomplish this protection by designating some of the machine instructions that may cause harm as privileged instructions. The hardware allows privileged instructions to be executed only in kernel mode. If an attempt is made to execute a privileged instruction in user mode, the hardware does not execute the instruction but rather treats it as illegal and traps it to the operating system. The instruction to switch to kernel mode is an example of a privileged instruction. Some other examples include I/O control, timer management, and interrupt management. As we shall see throughout the text, there are many\\n',\n",
       " 'additional privileged instructions.\\n',\n",
       " '   The concept of modes can be extended beyond two modes (in which case the CPU uses more than one bit to set and test the mode). CPUs that support virtualization (Section 16.1) frequently have a separate mode to indicate when the virtual machine manager (VMM)  and the virtualization management software is in control of the system. In this mode, the VMM has more privileges than user processes but fewer than the kernel. It needs that level of privilege so it can create and manage virtual machines, changing the CPU state to do so. Sometimes, too, different modes are used by various kernel\\n',\n",
       " 'components. We should note that, as an alternative to modes, the CPU designer may use other methods to differentiate operational privileges. The Intel 64 family of CPUs supports four privilege levels, for example, and supports virtualization but does not have a separate mode for virtualization.\\n',\n",
       " '    We can now see the life cycle of instruction execution in a computer system. Initial control resides in the operating system, where instructions are executed in kernel mode. When control is given to a user application, the mode is set to user mode. Eventually, control is switched back to the operating system via an interrupt, a trap, or a system call.\\n',\n",
       " '   System calls provide the means for a user program to ask the operating system to perform tasks reserved for the operating system on the user programs behalf. A system call is invoked in a variety of ways, depending on the functionality provided by the underlying processor. In all forms, it is the method used by a process to request action by the operating system. A system call usually takes the form of a trap to a specific location in the interrupt vector. This trap can be executed by a generic trap instruction, although some systems (such as MIPS) have a specific syscall instruction to invoke a system call.\\n',\n",
       " '   When a system call is executed, it is typically treated by the hardware as a software interrupt. Control passes through the interrupt vector to a service routine in the operating system, and the mode bit is set to kernel mode. The system-call service routine is a part of the operating system. The kernel examines the interrupting instruction to determine what system call has occurred; a parameter indicates what type of service the user program is requesting. Additional information needed for the request may be passed in registers, on the stack, or in memory (with pointers to the memory locations passed in registers). The kernel verifies that the parameters are correct and legal, executes the request, and returns control to the instruction following the system call. We describe system calls more fully in Section 2.3.\\n',\n",
       " '   The lack of a hardware-supported dual mode can cause serious shortcom- ings in an operating system. For instance, MS-DOS was written for the Intel 8088 architecture, which has no mode bit and therefore no dual mode. A user program running awry can wipe out the operating system by writing over it with data; and multiple programs are able to write to a device at the same time, with potentially disastrous results. Modern versions of the Intel CPU do provide dual-mode operation. Accordingly, most contemporary operating systems  such as Microsoft Windows 7, as well as Unix and Linux  take advantage of this dual-mode feature and provide greater protection for the operating system.\\n',\n",
       " '   Once hardware protection is in place, it detects errors that violate modes. These errors are normally handled by the operating system. If a user program fails in some way such as by making an attempt either to execute an illegal instruction or to access memory that is not in the users address space  then the hardware traps to the operating system. The trap transfers control through the interrupt vector to the operating system, just as an interrupt does. When a program error occurs, the operating system must terminate the program abnormally. This situation is handled by the same code as a user-requested abnormal termination. An appropriate error message is given, and the memory of the program may be dumped. The memory dump is usually written to a file so that the user or programmer can examine it and perhaps correct it and restart the program.\\n',\n",
       " '1.5.2 Timer\\n',\n",
       " 'We must ensure that the operating system maintains control over the CPU. We cannot allow a user program to get stuck in an infinite loop or to fail to call system services and never return control to the operating system. To accomplish this goal, we can use a timer. A timer can be set to interrupt the computer after a specified period. The period may be fixed (for example, 1/60 second) or variable (for example, from 1 millisecond to 1 second). A variable timer is generally implemented by a fixed-rate clock and a counter. The operating system sets the counter. Every time the clock ticks, the counter is decremented. When the counter reaches 0, an interrupt occurs. For instance, a 10-bit counter with a 1-millisecond clock allows interrupts at intervals from 1 millisecond to 1,024 milliseconds, in steps of 1 millisecond.\\n',\n",
       " '   Before turning over control to the user, the operating system ensures that the timer is set to interrupt. If the timer interrupts, control transfers automatically to the operating system, which may treat the interrupt as a fatal error or may give the program more time. Clearly, instructions that modify the content of the timer are privileged.\\n',\n",
       " '   We can use the timer to prevent a user program from running too long. A simple technique is to initialize a counter with the amount of time that a program is allowed to run. A program with a 7-minute time limit, for example, would have its counter initialized to 420. Every second, the timer interrupts, and the counter is decremented by 1. As long as the counter is positive, control is returned to the user program. When the counter becomes negative, the operating system terminates the program for exceeding the assigned time limit.\\n',\n",
       " '1.6 Process Management\\n',\n",
       " 'A program does nothing unless its instructions are executed by a CPU. A program in execution, as mentioned, is a process. A time-shared user program such as a compiler is a process. A word-processing program being run by an individual user on a PC is a process. A system task, such as sending output to a printer, can also be a process (or at least part of one). For now, you can consider a process to be a job or a time-shared program, but later you will learn that the concept is more general. As we shall see in Chapter 3, it is possible to provide system calls that allow processes to create subprocesses to execute concurrently.\\n',\n",
       " '    A process needs certain resources including CPU time, memory, files, and I/O devices to accomplish its task. These resources are either given to the process when it is created or allocated to it while it is running. In addition to the various physical and logical resources that a process obtains when it is created, various initialization data (input) may be passed along. For example, consider a process whose function is to display the status of a file on the screen of a terminal. The process will be given the name of the file as an input and will execute the appropriate instructions and system calls to obtain and display the desired information on the terminal. When the process terminates, the operating system will reclaim any reusable resources.\\n',\n",
       " 'We emphasize that a program by itself is not a process. A program is a\\n',\n",
       " 'passive entity, like the contents of a file stored on disk, whereas a process\\n',\n",
       " '1.7 Memory Management\\t25\\n',\n",
       " 'is an active entity. A single-threaded process has one program counter specifying the next instruction to execute. (Threads are covered in Chapter 4.) The execution of such a process must be sequential. The CPU executes one instruction of the process after another, until the process completes. Further, at any time, one instruction at most is executed on behalf of the process. Thus, although two processes may be associated with the same program, they are nevertheless considered two separate execution sequences. A multithreaded process has multiple program counters, each pointing to the next instruction to execute for a given thread.\\n',\n",
       " '   A process is the unit of work in a system. A system consists of a collection of processes, some of which are operating-system processes (those that execute system code) and the rest of which are user processes (those that execute user code). All these processes can potentially execute concurrently by multiplexing on a single CPU, for example.\\n',\n",
       " '   The operating system is responsible for the following activities in connec- tion with process management:\\n',\n",
       " ' Scheduling processes and threads on the CPUs\\n',\n",
       " ' Creating and deleting both user and system processes\\n',\n",
       " ' Suspending and resuming processes\\n',\n",
       " ' Providing mechanisms for process synchronization\\n',\n",
       " ' Providing mechanisms for process communication\\n',\n",
       " 'We discuss process-management techniques in Chapters 3 through 5.\\n',\n",
       " '1.7 Memory Management\\n',\n",
       " 'As we discussed in Section 1.2.2, the main memory is central to the operation of a modern computer system. Main memory is a large array of bytes, ranging in size from hundreds of thousands to billions. Each byte has its own address. Main memory is a repository of quickly accessible data shared by the CPU and I/O devices. The central processor reads instructions from main memory during the instruction-fetch cycle and both reads and writes data from main memory during the data-fetch cycle (on a von Neumann architecture). As noted earlier, the main memory is generally the only large storage device that the CPU is able to address and access directly. For example, for the CPU to process data from disk, those data must first be transferred to main memory by CPU-generated I/O calls. In the same way, instructions must be in memory for the CPU to execute them.\\n',\n",
       " '    For a program to be executed, it must be mapped to absolute addresses and loaded into memory. As the program executes, it accesses program instructions and data from memory by generating these absolute addresses. Eventually, the program terminates, its memory space is declared available, and the next program can be loaded and executed.\\n',\n",
       " '    To improve both the utilization of the CPU and the speed of the computers response to its users, general-purpose computers must keep several programs in memory, creating a need for memory management. Many different memory-\\n',\n",
       " 'management schemes are used. These schemes reflect various approaches, and the effectiveness of any given algorithm depends on the situation. In selecting a memory-management scheme for a specific system, we must take into account many factors  especially the hardware design of the system. Each algorithm requires its own hardware support.\\n',\n",
       " '   The operating system is responsible for the following activities in connec- tion with memory management:\\n',\n",
       " ' Keeping track of which parts of memory are currently being used and who is using them\\n',\n",
       " ' Deciding which processes (or parts of processes) and data to move into and out of memory\\n',\n",
       " ' Allocating and deallocating memory space as needed\\n',\n",
       " 'Memory-management techniques are discussed in Chapters 8 and 9.\\n',\n",
       " '1.8 Storage Management\\n',\n",
       " 'To make the computer system convenient for users, the operating system provides a uniform, logical view of information storage. The operating system abstracts from the physical properties of its storage devices to define a logical storage unit, the file. The operating system maps files onto physical media and accesses these files via the storage devices.\\n',\n",
       " '1.8.1 File-System Management\\n',\n",
       " 'File management is one of the most visible components of an operating system. Computers can store information on several different types of physical media. Magnetic disk, optical disk, and magnetic tape are the most common. Each of these media has its own characteristics and physical organization. Each medium is controlled by a device, such as a disk drive or tape drive, that also has its own unique characteristics. These properties include access speed, capacity, data-transfer rate, and access method (sequential or random).\\n',\n",
       " '    A file is a collection of related information defined by its creator. Commonly, files represent programs (both source and object forms) and data. Data files may be numeric, alphabetic, alphanumeric, or binary. Files may be free-form (for example, text files), or they may be formatted rigidly (for example, fixed fields). Clearly, the concept of a file is an extremely general one.\\n',\n",
       " '    The operating system implements the abstract concept of a file by managing mass-storage media, such as tapes and disks, and the devices that control them. In addition, files are normally organized into directories to make them easier to use. Finally, when multiple users have access to files, it may be desirable to control which user may access a file and how that user may access it (for example, read, write, append).\\n',\n",
       " '   The operating system is responsible for the following activities in connec- tion with file management:\\n',\n",
       " ' Creating and deleting files\\n',\n",
       " ' Creating and deleting directories to organize files\\n',\n",
       " ' Supporting primitives for manipulating files and directories\\n',\n",
       " ' Mapping files onto secondary storage\\n',\n",
       " ' Backing up files on stable (nonvolatile) storage media\\n',\n",
       " 'File-management techniques are discussed in Chapters 11 and 12.\\n',\n",
       " '1.8.2 Mass-Storage Management\\n',\n",
       " 'As we have already seen, because main memory is too small to accommodate all data and programs, and because the data that it holds are lost when power is lost, the computer system must provide secondary storage to back up main memory. Most modern computer systems use disks as the principal on-line storage medium for both programs and data. Most programs including compilers, assemblers, word processors, editors, and formatters are stored on a disk until loaded into memory. They then use the disk as both the source and destination of their processing. Hence, the proper management of disk storage is of central importance to a computer system. The operating system is responsible for the following activities in connection with disk management:\\n',\n",
       " ' Free-space management\\n',\n",
       " ' Storage allocation\\n',\n",
       " ' Disk scheduling\\n',\n",
       " 'Because secondary storage is used frequently, it must be used efficiently. The entire speed of operation of a computer may hinge on the speeds of the disk subsystem and the algorithms that manipulate that subsystem.\\n',\n",
       " '   There are, however, many uses for storage that is slower and lower in cost (and sometimes of higher capacity) than secondary storage. Backups of disk data, storage of seldom-used data, and long-term archival storage are some examples. Magnetic tape drives and their tapes and CD and DVD drives and platters are typical tertiary storage devices. The media (tapes and optical platters) vary between WORM (write-once, read-many-times) and RW (read write) formats.\\n',\n",
       " '   Tertiary storage is not crucial to system performance, but it still must be managed. Some operating systems take on this task, while others leave tertiary-storage management to application programs. Some of the functions that operating systems can provide include mounting and unmounting media in devices, allocating and freeing the devices for exclusive use by processes, and migrating data from secondary to tertiary storage.\\n',\n",
       " '   Techniques for secondary and tertiary storage management are discussed in Chapter 10.\\n',\n",
       " '1.8.3 Caching\\n',\n",
       " 'Caching is an important principle of computer systems. Heres how it works. Information is normally kept in some storage system (such as main memory). As it is used, it is copied into a faster storage system the cache on a\\n',\n",
       " 'temporary basis. When we need a particular piece of information, we first check whether it is in the cache. If it is, we use the information directly from the cache. If it is not, we use the information from the source, putting a copy in the cache under the assumption that we will need it again soon.\\n',\n",
       " '   In addition, internal programmable registers, such as index registers, provide a high-speed cache for main memory. The programmer (or compiler) implements the register-allocation and register-replacement algorithms to decide which information to keep in registers and which to keep in main memory.\\n',\n",
       " '   Other caches are implemented totally in hardware. For instance, most systems have an instruction cache to hold the instructions expected to be executed next. Without this cache, the CPU would have to wait several cycles while an instruction was fetched from main memory. For similar reasons, most systems have one or more high-speed data caches in the memory hierarchy. We are not concerned with these hardware-only caches in this text, since they are outside the control of the operating system.\\n',\n",
       " '   Because caches have limited size, cache management is an important design problem. Careful selection of the cache size and of a replacement policy can result in greatly increased performance. Figure 1.11 compares storage performance in large workstations and small servers. Various replacement algorithms for software-controlled caches are discussed in Chapter 9.\\n',\n",
       " '   Main memory can be viewed as a fast cache for secondary storage, since data in secondary storage must be copied into main memory for use and data must be in main memory before being moved to secondary storage for safekeeping. The file-system data, which resides permanently on secondary storage, may appear on several levels in the storage hierarchy. At the highest level, the operating system may maintain a cache of file-system data in main memory. In addition, solid-state disks may be used for high-speed storage that is accessed through the file-system interface. The bulk of secondary storage is on magnetic disks. The magnetic-disk storage, in turn, is often backed up onto magnetic tapes or removable disks to protect against data loss in case of a hard-disk failure. Some systems automatically archive old file data from secondary storage to tertiary storage, such as tape jukeboxes, to lower the storage cost (see Chapter 10).\\n',\n",
       " 'Level\\n',\n",
       " '1\\n',\n",
       " '2\\n',\n",
       " '3\\n',\n",
       " '4\\n',\n",
       " '5\\n',\n",
       " 'Name\\n',\n",
       " 'registers\\n',\n",
       " 'cache\\n',\n",
       " 'main memory\\n',\n",
       " 'solid state disk\\n',\n",
       " 'magnetic disk\\n',\n",
       " 'Typical size\\n',\n",
       " '< 1 KB\\n',\n",
       " '< 16MB\\n',\n",
       " '< 64GB\\n',\n",
       " '< 1 TB\\n',\n",
       " '< 10 TB\\n',\n",
       " 'Implementation technology\\n',\n",
       " 'custom memory with multiple ports CMOS\\n',\n",
       " 'on-chip or off-chip CMOS SRAM\\n',\n",
       " 'CMOS SRAM\\n',\n",
       " 'flash memory\\n',\n",
       " 'magnetic disk\\n',\n",
       " 'Access time (ns)\\n',\n",
       " '0.25 - 0.5\\n',\n",
       " '0.5 - 25\\n',\n",
       " '80 - 250\\n',\n",
       " '25,000 - 50,000\\n',\n",
       " '5,000,000\\n',\n",
       " 'Bandwidth (MB/sec)\\n',\n",
       " '20,000 - 100,000\\n',\n",
       " '5,000 - 10,000\\n',\n",
       " '1,000 - 5,000\\n',\n",
       " '500\\n',\n",
       " '20 - 150\\n',\n",
       " 'Managed by\\n',\n",
       " 'compiler\\n',\n",
       " 'hardware\\n',\n",
       " 'operating system\\n',\n",
       " 'operating system\\n',\n",
       " 'operating system\\n',\n",
       " 'Backed by\\n',\n",
       " 'cache\\n',\n",
       " 'main memory\\n',\n",
       " 'disk\\n',\n",
       " 'disk\\n',\n",
       " 'disk or tape\\n',\n",
       " 'Figure 1.11 Performance of various levels of storage.\\n',\n",
       " '\\t\\n',\n",
       " 'Figure 1.12 Migration of integer A from disk to register.\\n',\n",
       " '   The movement of information between levels of a storage hierarchy may be either explicit or implicit, depending on the hardware design and the controlling operating-system software. For instance, data transfer from cache to CPU and registers is usually a hardware function, with no operating-system intervention. In contrast, transfer of data from disk to memory is usually controlled by the operating system.\\n',\n",
       " '   In a hierarchical storage structure, the same data may appear in different levels of the storage system. For example, suppose that an integer A that is to be incremented by 1 is located in file B, and file B resides on magnetic disk. The increment operation proceeds by first issuing an I/O operation to copy the disk block on which A resides to main memory. This operation is followed by copying A to the cache and to an internal register. Thus, the copy of A appears in several places: on the magnetic disk, in main memory, in the cache, and in an internal register (see Figure 1.12). Once the increment takes place in the internal register, the value of A differs in the various storage systems. The value of A becomes the same only after the new value of A is written from the internal register back to the magnetic disk.\\n',\n",
       " '   In a computing environment where only one process executes at a time, this arrangement poses no difficulties, since an access to integer A will always be to the copy at the highest level of the hierarchy. However, in a multitasking environment, where the CPU is switched back and forth among various processes, extreme care must be taken to ensure that, if several processes wish to access A, then each of these processes will obtain the most recently updated value of A.\\n',\n",
       " '   The situation becomes more complicated in a multiprocessor environment where, in addition to maintaining internal registers, each of the CPUs also contains a local cache (Figure 1.6). In such an environment, a copy of A may exist simultaneously in several caches. Since the various CPUs can all execute in parallel, we must make sure that an update to the value of A in one cache is immediately reflected in all other caches where A resides. This situation is called cache coherency, and it is usually a hardware issue (handled below the operating-system level).\\n',\n",
       " '   In a distributed environment, the situation becomes even more complex. In this environment, several copies (or replicas) of the same file can be kept on different computers. Since the various replicas may be accessed and updated concurrently, some distributed systems ensure that, when a replica is updated in one place, all other replicas are brought up to date as soon as possible. There are various ways to achieve this guarantee, as we discuss in Chapter 17.\\n',\n",
       " '1.8.4 I/O Systems\\n',\n",
       " 'One of the purposes of an operating system is to hide the peculiarities of specific hardware devices from the user. For example, in UNIX, the peculiarities of I/O\\n',\n",
       " 'devices are hidden from the bulk of the operating system itself by the I/O\\n',\n",
       " 'subsystem. The I/O subsystem consists of several components:\\n',\n",
       " ' A memory-management component that includes buffering, caching, and spooling\\n',\n",
       " ' A general device-driver interface\\n',\n",
       " ' Drivers for specific hardware devices\\n',\n",
       " 'Only the device driver knows the peculiarities of the specific device to which it is assigned.\\n',\n",
       " '    We discussed in Section 1.2.3 how interrupt handlers and device drivers are used in the construction of efficient I/O subsystems. In Chapter 13, we discuss how the I/O subsystem interfaces to the other system components, manages devices, transfers data, and detects I/O completion.\\n',\n",
       " '1.9 Protection and Security\\n',\n",
       " 'If a computer system has multiple users and allows the concurrent execution of multiple processes, then access to data must be regulated. For that purpose, mechanisms ensure that files, memory segments, CPU, and other resources can be operated on by only those processes that have gained proper authoriza- tion from the operating system. For example, memory-addressing hardware ensures that a process can execute only within its own address space. The timer ensures that no process can gain control of the CPU without eventually relinquishing control. Device-control registers are not accessible to users, so the integrity of the various peripheral devices is protected.\\n',\n",
       " '   Protection, then, is any mechanism for controlling the access of processes or users to the resources defined by a computer system. This mechanism must provide means to specify the controls to be imposed and to enforce the controls. Protection can improve reliability by detecting latent errors at the interfaces between component subsystems. Early detection of interface errors can often prevent contamination of a healthy subsystem by another subsystem that is malfunctioning. Furthermore, an unprotected resource cannot defend against use (or misuse) by an unauthorized or incompetent user. A protection-oriented system provides a means to distinguish between authorized and unauthorized\\n',\n",
       " 'usage, as we discuss in Chapter 14.\\n',\n",
       " '   A system can have adequate protection but still be prone to failure and allow inappropriate access. Consider a user whose authentication information (her means of identifying herself to the system) is stolen. Her data could be copied or deleted, even though file and memory protection are working. It is the job of security to defend a system from external and internal attacks. Such attacks spread across a huge range and include viruses and worms, denial-of- service attacks (which use all of a systems resources and so keep legitimate users out of the system), identity theft, and theft of service (unauthorized use of a system). Prevention of some of these attacks is considered an operating-system function on some systems, while other systems leave it to policy or additional software. Due to the alarming rise in security incidents,\\n',\n",
       " 'operating-system security features represent a fast-growing area of research and implementation. We discuss security in Chapter 15.\\n',\n",
       " '   Protection and security require the system to be able to distinguish among all its users. Most operating systems maintain a list of user names and associated user identifiers (user IDs). In Windows parlance, this is a security ID (SID). These numerical IDs are unique, one per user. When a user logs in to the system, the authentication stage determines the appropriate user ID for the user. That user ID is associated with all of the users processes and threads. When an ID needs to be readable by a user, it is translated back to the user name via the user name list.\\n',\n",
       " '   In some circumstances, we wish to distinguish among sets of users rather than individual users. For example, the owner of a file on a UNIX system may be allowed to issue all operations on that file, whereas a selected set of users may be allowed only to read the file. To accomplish this, we need to define a group name and the set of users belonging to that group. Group functionality can be implemented as a system-wide list of group names and group identifiers. A user can be in one or more groups, depending on operating-system design decisions. The users group IDs are also included in every associated process and thread.\\n',\n",
       " '   In the course of normal system use, the user ID and group ID for a user are sufficient. However, a user sometimes needs to escalate privileges to gain extra permissions for an activity. The user may need access to a device that is restricted, for example. Operating systems provide various methods to allow privilege escalation. On UNIX, for instance, the setuid attribute on a program causes that program to run with the user ID of the owner of the file, rather than the current users ID. The process runs with this effective UID until it turns off the extra privileges or terminates.\\n',\n",
       " '1.10 Kernel Data Structures\\n',\n",
       " 'We turn next to a topic central to operating-system implementation: the way data are structured in the system. In this section, we briefly describe several fundamental data structures used extensively in operating systems. Readers who require further details on these structures, as well as others, should consult the bibliography at the end of the chapter.\\n',\n",
       " '1.10.1 Lists, Stacks, and Queues\\n',\n",
       " 'An array is a simple data structure in which each element can be accessed directly. For example, main memory is constructed as an array. If the data item being stored is larger than one byte, then multiple bytes can be allocated to the item, and the item is addressed as item number item size. But what about storing an item whose size may vary? And what about removing an item if the relative positions of the remaining items must be preserved? In such situations, arrays give way to other data structures.\\n',\n",
       " '   After arrays, lists are perhaps the most fundamental data structures in computer science. Whereas each item in an array can be accessed directly, the items in a list must be accessed in a particular order. That is, a list represents a collection of data values as a sequence. The most common method for\\n',\n",
       " 'data\\tdata\\tdata\\tnull\\n',\n",
       " '\\t\\t\\n',\n",
       " '\\t\\t\\t\\n',\n",
       " 'Figure 1.13 Singly linked list.\\n',\n",
       " 'implementing this structure is a linked list, in which items are linked to one another. Linked lists are of several types:\\n',\n",
       " ' In a singly linked list, each item points to its successor, as illustrated in Figure 1.13.\\n',\n",
       " ' In a doubly linked list, a given item can refer either to its predecessor or to its successor, as illustrated in Figure 1.14.\\n',\n",
       " ' In a circularly linked list, the last element in the list refers to the first element, rather than to null, as illustrated in Figure 1.15.\\n',\n",
       " '   Linked lists accommodate items of varying sizes and allow easy insertion and deletion of items. One potential disadvantage of using a list is that performance for retrieving a specified item in a list of size n is linear  O(n), as it requires potentially traversing all n elements in the worst case. Lists are sometimes used directly by kernel algorithms. Frequently, though, they are used for constructing more powerful data structures, such as stacks and queues.\\n',\n",
       " '   A stack is a sequentially ordered data structure that uses the last in, first out (LIFO) principle for adding and removing items, meaning that the last item placed onto a stack is the first item removed. The operations for inserting and removing items from a stack are known as push and pop, respectively. An operating system often uses a stack when invoking function calls. Parameters, local variables, and the return address are pushed onto the stack when a function is called; returning from the function call pops those items off the stack.\\n',\n",
       " '   A queue, in contrast, is a sequentially ordered data structure that uses the first in, first out (FIFO) principle: items are removed from a queue in the order in which they were inserted. There are many everyday examples of queues, including shoppers waiting in a checkout line at a store and cars waiting in line at a traffic signal. Queues are also quite common in operating systems jobs that are sent to a printer are typically printed in the order in which they were submitted, for example. As we shall see in Chapter 6, tasks that are waiting to be run on an available CPU are often organized in queues.\\n',\n",
       " '\\t\\t\\t\\n',\n",
       " 'data null\\n',\n",
       " 'data\\tdata\\tdata\\n',\n",
       " 'null\\n',\n",
       " '  \\n',\n",
       " '\\t\\t\\t\\n',\n",
       " 'Figure 1.14 Doubly linked list.\\n',\n",
       " 'data\\tdata\\tdata\\tdata\\n',\n",
       " '\\t\\t\\n',\n",
       " '\\t\\t\\t\\n',\n",
       " 'Figure 1.15 Circularly linked list.\\n',\n",
       " '1.10.2 Trees\\n',\n",
       " 'A tree is a data structure that can be used to represent data hierarchically. Data values in a tree structure are linked through parent child relationships. In a general tree, a parent may have an unlimited number of children. In a binary tree, a parent may have at most two children, which we term the left child and the right child. A binary search tree additionally requires an ordering between the parents two children in which le f t child <    right child. Figure\\n',\n",
       " '1.16 provides an example of a binary search tree. When we search for an item in a binary search tree, the worst-case performance is O(n) (consider how this can occur). To remedy this situation, we can use an algorithm to create a balanced binary search tree. Here, a tree containing n items has at most lg n levels, thus ensuring worst-case performance of O(lg n). We shall see in Section 6.7.1 that Linux uses a balanced binary search tree as part its CPU-scheduling algorithm.\\n',\n",
       " '1.10.3 Hash Functions and Maps\\n',\n",
       " 'A hash function takes data as its input, performs a numeric operation on this data, and returns a numeric value. This numeric value can then be used as an index into a table (typically an array) to quickly retrieve the data. Whereas searching for a data item through a list of size n can require up to O(n) comparisons in the worst case, using a hash function for retrieving data from table can be as good as O(1) in the worst case, depending on implementation details. Because of this performance, hash functions are used extensively in operating systems.\\n',\n",
       " 'Figure 1.16 Binary search tree.\\n',\n",
       " 'hash_function(key)\\n',\n",
       " 'hash map\\n',\n",
       " 'Figure 1.17 Hash map.\\n',\n",
       " '   One potential difficulty with hash functions is that two inputs can result in the same output value  that is, they can link to the same table location. We can accommodate this hash collision by having a linked list at that table location that contains all of the items with the same hash value. Of course, the more collisions there are, the less efficient the hash function is.\\n',\n",
       " '   One use of a hash function is to implement a hash map, which associates (or maps) [key:value] pairs using a hash function. For example, we can map the key operating to the value system. Once the mapping is established, we can apply the hash function to the key to obtain the value from the hash map (Figure 1.17). For example, suppose that a user name is mapped to a password. Password authentication then proceeds as follows: a user enters his user name and password. The hash function is applied to the user name, which is then used to retrieve the password. The retrieved password is then compared with the password entered by the user for authentication.\\n',\n",
       " '1.10.4 Bitmaps\\n',\n",
       " 'A bitmap is a string of n binary digits that can be used to represent the status of n items. For example, suppose we have several resources, and the availability of each resource is indicated by the value of a binary digit: 0 means that the resource is available, while 1 indicates that it is unavailable (or vice-versa). The value of the ith position in the bitmap is associated with the ith resource. As an example, consider the bitmap shown below:\\n',\n",
       " '001011101 \\n',\n",
       " 'Resources 2, 4, 5, 6, and 8 are unavailable; resources 0, 1, 3, and 7 are available. The power of bitmaps becomes apparent when we consider their space efficiency. If we were to use an eight-bit Boolean value instead of a single bit, the resulting data structure would be eight times larger. Thus, bitmaps are commonly used when there is a need to represent the availability of a large number of resources. Disk drives provide a nice illustration. A medium-sized disk drive might be divided into several thousand individual units, called disk\\n',\n",
       " 'blocks.A bitmap can be used to indicate the availability of each disk block.\\n',\n",
       " '   Data structures are pervasive in operating system implementations. Thus, we will see the structures discussed here, along with others, throughout this text as we explore kernel algorithms and their implementations.\\n',\n",
       " '1.11 Computing Environments\\n',\n",
       " 'So far, we have briefly described several aspects of computer systems and the operating systems that manage them. We turn now to a discussion of how operating systems are used in a variety of computing environments.\\n',\n",
       " '1.11.1 Traditional Computing\\n',\n",
       " 'As computing has matured, the lines separating many of the traditional com- puting environments have blurred. Consider the typical office environment. Just a few years ago, this environment consisted of PCs connected to a network, with servers providing file and print services. Remote access was awkward, and portability was achieved by use of laptop computers. Terminals attached to mainframes were prevalent at many companies as well, with even fewer remote access and portability options.\\n',\n",
       " '   The current trend is toward providing more ways to access these computing environments. Web technologies and increasing WAN bandwidth are stretching the boundaries of traditional computing. Companies establish portals, which provide Web accessibility to their internal servers. Network computers (or thin clients) which are essentially terminals that understand web-based computing  are used in place of traditional workstations where more security or easier maintenance is desired. Mobile computers can synchronize with PCs to allow very portable use of company information. Mobile computers can also connect to wireless networks and cellular data networks to use the companys Web portal (as well as the myriad other Web resources).\\n',\n",
       " '   At home, most users once had a single computer with a slow modem connection to the office, the Internet, or both. Today, network-connection speeds once available only at great cost are relatively inexpensive in many places, giving home users more access to more data. These fast data connections are allowing home computers to serve up Web pages and to run networks that include printers, client PCs, and servers. Many homes use firewalls to protect their networks from security breaches.\\n',\n",
       " '   In the latter half of the 20th century, computing resources were relatively scarce. (Before that, they were nonexistent!) For a period of time, systems were either batch or interactive. Batch systems processed jobs in bulk, with predetermined input from files or other data sources. Interactive systems waited for input from users. To optimize the use of the computing resources, multiple users shared time on these systems. Time-sharing systems used a\\n',\n",
       " 'timer and scheduling algorithms to cycle processes rapidly through the CPU, giving each user a share of the resources.\\n',\n",
       " '    Today, traditional time-sharing systems are uncommon. The same schedul- ing technique is still in use on desktop computers, laptops, servers, and even mobile computers, but frequently all the processes are owned by the same user (or a single user and the operating system). User processes, and system processes that provide services to the user, are managed so that each frequently gets a slice of computer time. Consider the windows created while a user is working on a PC, for example, and the fact that they may be performing different tasks at the same time. Even a web browser can be composed of multiple processes, one for each website currently being visited, with time sharing applied to each web browser process.\\n',\n",
       " '1.11.2 Mobile Computing\\n',\n",
       " 'Mobile computing refers to computing on handheld smartphones and tablet computers. These devices share the distinguishing physical features of being portable and lightweight. Historically, compared with desktop and laptop computers, mobile systems gave up screen size, memory capacity, and overall functionality in return for handheld mobile access to services such as e-mail and web browsing. Over the past few years, however, features on mobile devices have become so rich that the distinction in functionality between, say, a consumer laptop and a tablet computer may be difficult to discern. In fact, we might argue that the features of a contemporary mobile device allow it to provide functionality that is either unavailable or impractical on a desktop or laptop computer.\\n',\n",
       " '   Today, mobile systems are used not only for e-mail and web browsing but also for playing music and video, reading digital books, taking photos, and recording high-definition video. Accordingly, tremendous growth continues in the wide range of applications that run on such devices. Many developers are now designing applications that take advantage of the unique features of mobile devices, such as global positioning system (GPS) chips, accelerometers, and gyroscopes. An embedded GPS chip allows a mobile device to use satellites to determine its precise location on earth. That functionality is especially useful in designing applications that provide navigation for example, telling users which way to walk or drive or perhaps directing them to nearby services, such as restaurants. An accelerometer allows a mobile device to detect its orientation with respect to the ground and to detect certain other forces, such as tilting and shaking. In several computer games that employ accelerometers, players interface with the system not by using a mouse or a keyboard but rather by tilting, rotating, and shaking the mobile device! Perhaps more a practical use of these features is found in augmented-reality applications, which overlay information on a display of the current environment. It is difficult to imagine how equivalent applications could be developed on traditional laptop or desktop computer systems.\\n',\n",
       " '   To provide access to on-line services, mobile devices typically use either IEEE standard 802.11 wireless or cellular data networks. The memory capacity and processing speed of mobile devices, however, are more limited than those of PCs. Whereas a smartphone or tablet may have 64 GB in storage, it is not uncommon to find 1 TB in storage on a desktop computer. Similarly, because\\n',\n",
       " 'power consumption is such a concern, mobile devices often use processors that are smaller, are slower, and offer fewer processing cores than processors found on traditional desktop and laptop computers.\\n',\n",
       " '   Two operating systems currently dominate mobile computing: Apple iOS and Google Android. iOS was designed to run on Apple iPhone and iPad mobile devices. Android powers smartphones and tablet computers available from many manufacturers. We examine these two mobile operating systems in further detail in Chapter 2.\\n',\n",
       " '1.11.3 Distributed Systems\\n',\n",
       " 'A distributed system is a collection of physically separate, possibly heteroge- neous, computer systems that are networked to provide users with access to the various resources that the system maintains. Access to a shared resource increases computation speed, functionality, data availability, and reliability. Some operating systems generalize network access as a form of file access, with the details of networking contained in the network interfaces device driver. Others make users specifically invoke network functions. Generally, systems contain a mix of the two modes  for example FTP and NFS. The protocols that create a distributed system can greatly affect that systems utility and popularity.\\n',\n",
       " '   A network, in  the simplest terms, is a communication  path between two or more systems. Distributed systems depend on networking for their functionality. Networks vary by the protocols used, the distances between nodes, and the transport media. TCP/IP is the most common network protocol, and it provides the fundamental architecture of the Internet. Most operating systems support TCP/IP, including all general-purpose ones. Some systems support proprietary protocols to suit their needs. To an operating system, a network protocol simply needs an interface device a network adapter, for example with a device driver to manage it, as well as software to handle data. These concepts are discussed throughout this book.\\n',\n",
       " '   Networks are characterized based on the distances between their nodes. A local-area network (LAN) connects computers within a room, a building, or a campus. A wide-area network (WAN) usually links buildings, cities, or countries. A global company may have a WAN to connect its offices worldwide, for example. These networks may run one protocol or several protocols. The continuing advent of new technologies brings about new forms of networks. For example, a metropolitan-area network (MAN) could link buildings within a city. BlueTooth and 802.11 devices use wireless technology to communicate over a distance of several feet, in essence creating a personal-area network (PAN) between a phone and a headset or a smartphone and a desktop computer. The media to carry networks are equally varied. They include copper wires, fiber strands, and wireless transmissions between satellites, microwave dishes, and radios. When computing devices are connected to cellular phones, they create a network. Even very short-range infrared communication can be used for networking. At a rudimentary level, whenever computers communicate, they use or create a network. These networks also vary in their performance\\n',\n",
       " 'and reliability.\\n',\n",
       " '   Some operating systems have taken the concept of networks and dis- tributed systems further than the notion of providing network connectivity.\\n',\n",
       " 'A network operating system is an operating system that provides features such as file sharing across the network, along with a communication scheme that allows different processes on different computers to exchange messages. A computer running a network operating system acts autonomously from all other computers on the network, although it is aware of the network and is able to communicate with other networked computers. A distributed operating system provides a less autonomous environment. The different computers communicate closely enough to provide the illusion that only a single operating system controls the network. We cover computer networks and distributed systems in Chapter 17.\\n',\n",
       " '1.11.4 Client Server Computing\\n',\n",
       " 'As PCs have become faster, more powerful, and cheaper, designers have shifted away from centralized system architecture. Terminals connected to centralized systems are now being supplanted by PCs and mobile devices. Correspond- ingly, user-interface functionality once handled directly by centralized systems is increasingly being handled by PCs, quite often through a web interface. As a result, many of todays systems act as server systems to satisfy requests generated by client systems. This form of specialized distributed system, called a client server system, has the general structure depicted in Figure 1.18.\\n',\n",
       " '   Server systems can be broadly categorized as compute servers and file servers:\\n',\n",
       " ' The compute-server system provides an interface to which a client can send a request to perform an action (for example, read data). In response, the server executes the action and sends the results to the client. A server running a database that responds to client requests for data is an example of such a system.\\n',\n",
       " ' The file-server system provides a file-system interface where clients can create, update, read, and delete files. An example of such a system is a web server that delivers files to clients running web browsers.\\n',\n",
       " 'Figure 1.18  General structure of a client  server system.\\n',\n",
       " '1.11.5 Peer-to-Peer Computing\\n',\n",
       " 'Another structure for a distributed system is the peer-to-peer (P2P) system model. In this model, clients and servers are not distinguished from one another. Instead, all nodes within the system are considered peers, and each may act as either a client or a server, depending on whether it is requesting or providing a service. Peer-to-peer systems offer an advantage over traditional client-server systems. In a client-server system, the server is a bottleneck; but in a peer-to-peer system, services can be provided by several nodes distributed throughout the network.\\n',\n",
       " '   To participate in a peer-to-peer system, a node must first join the network of peers. Once a node has joined the network, it can begin providing services to and requesting services from other nodes in the network. Determining what services are available is accomplished in one of two general ways:\\n',\n",
       " ' When a node joins a network, it registers its service with a centralized lookup service on the network. Any node desiring a specific service first contacts this centralized lookup service to determine which node provides the service. The remainder of the communication takes place between the client and the service provider.\\n',\n",
       " ' An alternative scheme uses no centralized lookup service. Instead, a peer acting as a client must discover what node provides a desired service by broadcasting a request for the service to all other nodes in the network. The node (or nodes) providing that service responds to the peer making the request. To support this approach, a discovery protocol must be provided that allows peers to discover services provided by other peers in the network. Figure 1.19 illustrates such a scenario.\\n',\n",
       " '    Peer-to-peer networks gained widespread popularity in the late 1990s with several file-sharing services, such as Napster and Gnutella, that enabled peers to exchange files with one another. The Napster system used an approach similar to the first type described above: a centralized server maintained an index of all files stored on peer nodes in the Napster network, and the actual\\n',\n",
       " 'Figure 1.19 Peer-to-peer system with no centralized service.\\n',\n",
       " 'exchange of files took place between the peer nodes. The Gnutella system used a technique similar to the second type: a client broadcasted file requests to other nodes in the system, and nodes that could service the request responded directly to the client. The future of exchanging files remains uncertain because peer-to-peer networks can be used to exchange copyrighted materials (music, for example) anonymously, and there are laws governing the distribution of copyrighted material. Notably, Napster ran into legal trouble for copyright infringement and its services were shut down in 2001.\\n',\n",
       " '   Skype is another example of peer-to-peer computing. It allows clients to make voice calls and video calls and to send text messages over the Internet using a technology known as voice over IP (VoIP). Skype uses a hybrid peer- to-peer approach. It includes a centralized login server, but it also incorporates decentralized peers and allows two peers to communicate.\\n',\n",
       " '1.11.6 Virtualization\\n',\n",
       " 'Virtualization is a technology that allows operating systems to run as appli- cations within other operating systems. At first blush, there seems to be little reason for such functionality. But the virtualization industry is vast and growing, which is a testament to its utility and importance.\\n',\n",
       " '   Broadly speaking, virtualization is one member of a class of software that also includes emulation. Emulation is used when the source CPU type is different from the target CPU type. For example, when Apple switched from the IBM Power CPU to the Intel x86 CPU for its desktop and laptop computers, it included an emulation facility called Rosetta, which allowed applications compiled for the IBM CPU to run on the Intel CPU. That same concept can be extended to allow an entire operating system written for one platform to run on another. Emulation comes at a heavy price, however. Every machine-level instruction that runs natively on the source system must be translated to the equivalent function on the target system, frequently resulting in several target instructions. If the source and target CPUs have similar performance levels, the emulated code can run much slower than the native code.\\n',\n",
       " '   A common example of emulation occurs when a computer language is not compiled to native code but instead is either executed in its high-level form or translated to an intermediate form. This is known as interpretation. Some languages, such as BASIC, can be either compiled or interpreted. Java, in contrast, is always interpreted. Interpretation is a form of emulation in that the high-level language code is translated to native CPU instructions, emulating not another CPU but a theoretical virtual machine on which that language could run natively. Thus, we can run Java programs on Java virtual machines, but technically those virtual machines are Java emulators.\\n',\n",
       " '   With virtualization, in contrast, an operating system that is natively com- piled for a particular CPU architecture runs within another operating system also native to that CPU. Virtualization first came about on IBM mainframes as a method for multiple users to run tasks concurrently. Running multiple virtual machines allowed (and still allows) many users to run tasks on a system designed for a single user. Later, in response to problems with running multiple Microsoft Windows XP applications on the Intel x86 CPU, VMware created a new virtualization technology in the form of an application that ran on XP. That application ran one or more guest copies of Windows or other native\\n',\n",
       " 'programming interface\\n',\n",
       " '(a)\\t(b)\\n',\n",
       " 'Figure 1.20 VMware.\\n',\n",
       " 'x86 operating systems, each running its own applications. (See Figure 1.20.) Windows was the host operating system, and the VMware application was the virtual machine manager VMM. The VMM runs the guest operating systems, manages their resource use, and protects each guest from the others.\\n',\n",
       " '   Even though modern operating systems are fully capable of running multiple applications reliably, the use of virtualization continues to grow. On laptops and desktops, a VMM allows the user to install multiple operating systems for exploration or to run applications written for operating systems other than the native host. For example, an Apple laptop running Mac OS X on the x86 CPU can run a Windows guest to allow execution of Windows applications. Companies writing software for multiple operating systems can use virtualization to run all of those operating systems on a single physical server for development, testing, and debugging. Within data centers, virtualization has become a common method of executing and managing computing environments. VMMs like VMware, ESX, and Citrix XenServer no longer run on host operating systems but rather are the hosts. Full details of the features and implementation of virtualization are found in Chapter 16.\\n',\n",
       " '1.11.7 Cloud Computing\\n',\n",
       " 'Cloud computing is a type of computing that delivers computing, storage, and even applications as a service across a network. In some ways, its a logical extension of virtualization, because it uses virtualization as a base for its functionality. For example, the Amazon Elastic Compute Cloud (EC2) facility has thousands of servers, millions of virtual machines, and petabytes of storage available for use by anyone on the Internet. Users pay per month based on how much of those resources they use.\\n',\n",
       " 'There are actually many types of cloud computing, including the following:\\n',\n",
       " ' Public cloud  a cloud available via the Internet to anyone willing to pay for the services\\n',\n",
       " ' Private cloud a cloud run by a company for that companys own use\\n',\n",
       " ' Hybrid cloud  a cloud that includes both public and private cloud components\\n',\n",
       " ' Software as a service (SaaS)  one or more applications (such as word processors or spreadsheets) available via the Internet\\n',\n",
       " ' Platform as a service (PaaS) a software stack ready for application use via the Internet (for example, a database server)\\n',\n",
       " ' Infrastructure as a service (IaaS) servers or storage available over the Internet (for example, storage available for making backup copies of production data)\\n',\n",
       " 'These cloud-computing types are not discrete, as a cloud computing environ- ment may provide a combination of several types. For example, an organization may provide both SaaS and IaaS as a publicly available service.\\n',\n",
       " '   Certainly, there are traditional operating systems within many of the types of cloud infrastructure. Beyond those are the VMMs that manage the virtual machines in which the user processes run. At a higher level, the VMMs themselves are managed by cloud management tools, such as Vware vCloud Director and the open-source Eucalyptus toolset. These tools manage the resources within a given cloud and provide interfaces to the cloud components, making a good argument for considering them a new type of operating system.\\n',\n",
       " '   Figure 1.21 illustrates a public cloud providing IaaS. Notice that both the cloud services and the cloud user interface are protected by a firewall.\\n',\n",
       " 'Figure 1.21 Cloud computing.\\n',\n",
       " '1.11.8 Real-Time Embedded Systems\\n',\n",
       " 'Embedded computers are the most prevalent form of computers in existence. These devices are found everywhere, from car engines and manufacturing robots to DVDs and microwave ovens. They tend to have very specific tasks. The systems they run on are usually primitive, and so the operating systems provide limited features. Usually, they have little or no user interface, preferring to spend their time monitoring and managing hardware devices, such as automobile engines and robotic arms.\\n',\n",
       " '   These embedded systems vary considerably. Some are general-purpose computers, running standard operating systems  such as Linux  with special-purpose applications to implement the functionality. Others are hard- ware devices with a special-purpose embedded operating system providing just the functionality desired. Yet others are hardware devices with application- specific integrated circuits (ASICs) that perform their tasks without an operat- ing system.\\n',\n",
       " '   The use of embedded systems continues to expand. The power of these devices, both as standalone units and as elements of networks and the web, is sure to increase as well. Even now, entire houses can be computerized, so that a central computer either a general-purpose computer or an embedded system can control heating and lighting, alarm systems, and even coffee makers. Web access can enable a home owner to tell the house to heat up before she arrives home. Someday, the refrigerator can notify the grocery store when it notices the milk is gone.\\n',\n",
       " '   Embedded systems almost always run real-time operating systems. A real-time system is used when rigid time requirements have been placed on the operation of a processor or the flow of data; thus, it is often used as a control device in a dedicated application. Sensors bring data to the computer. The computer must analyze the data and possibly adjust controls to modify the sensor inputs. Systems that control scientific experiments, medical imaging systems, industrial control systems, and certain display systems are real- time systems. Some automobile-engine fuel-injection systems, home-appliance controllers, and weapon systems are also real-time systems.\\n',\n",
       " '   A real-time system has well-defined, fixed time constraints. Processing must be done within the defined constraints, or the system will fail. For instance, it would not do for a robot arm to be instructed to halt after it had smashed into the car it was building. A real-time system functions correctly only if it returns the correct result within its time constraints. Contrast this system with a time-sharing system, where it is desirable (but not mandatory) to respond quickly, or a batch system, which may have no time constraints at all.\\n',\n",
       " '   In Chapter 6, we consider the scheduling facility needed to implement real-time functionality in an operating system. In Chapter 9, we describe the design of memory management for real-time computing. Finally, in Chapters 18 and 19, we describe the real-time components of the Linux and Windows 7 operating systems.\\n',\n",
       " '1.12 Open-Source Operating Systems\\n',\n",
       " 'We noted at the beginning of this chapter that the study of operating systems has been made easier by the availability of a vast number of open-source\\n',\n",
       " 'releases. Open-source operating systems are those available in source-code format rather than as compiled binary code. Linux is the most famous open- source operating system, while Microsoft Windows is a well-known example of the opposite closed-source approach. Apples Mac OS X and iOS operating systems comprise a hybrid approach. They contain an open-source kernel named Darwin yet include proprietary, closed-source components as well.\\n',\n",
       " '   Starting with the source code allows the programmer to produce binary code that can be executed on a system. Doing the opposite reverse engi- neering the source code from the binaries  is quite a lot of work, and useful items such as comments are never recovered. Learning operating systems by examining the source code has other benefits as well. With the source code in hand, a student can modify the operating system and then compile and run the code to try out those changes, which is an excellent learning tool. This text includes projects that involve modifying operating-system source code, while also describing algorithms at a high level to be sure all important operating-system topics are covered. Throughout the text, we provide pointers to examples of open-source code for deeper study.\\n',\n",
       " '   There are many benefits to open-source operating systems, including a community of interested (and usually unpaid) programmers who contribute to the code by helping to debug it, analyze it, provide support, and suggest changes. Arguably, open-source code is more secure than closed-source code because many more eyes are viewing the code. Certainly, open-source code has bugs, but open-source advocates argue that bugs tend to be found and fixed faster owing to the number of people using and viewing the code. Companies that earn revenue from selling their programs often hesitate to open-source their code, but Red Hat and a myriad of other companies are doing just that and showing that commercial companies benefit, rather than suffer, when they open-source their code. Revenue can be generated through support contracts and the sale of hardware on which the software runs, for example.\\n',\n",
       " '1.12.1 History\\n',\n",
       " 'In the early days of modern computing (that is, the 1950s), a great deal of software was available in open-source format. The original hackers (computer enthusiasts) at MITs Tech Model Railroad Club left their programs in drawers for others to work on. Homebrew user groups exchanged code during their meetings. Later, company-specific user groups, such as Digital Equipment Corporations DEC, accepted contributions of source-code programs, collected them onto tapes, and distributed the tapes to interested members.\\n',\n",
       " '   Computer and software companies eventually sought to limit the use of their software to authorized computers and paying customers. Releasing only the binary files compiled from the source code, rather than the source code itself, helped them to achieve this goal, as well as protecting their code and their ideas from their competitors. Another issue involved copyrighted material. Operating systems and other programs can limit the ability to play back movies and music or display electronic books to authorized computers. Such copy protection or digital rights management (DRM) would not be effective if the source code that implemented these limits were published. Laws in many countries, including the U.S. Digital Millennium Copyright Act (DMCA), make it illegal to reverse-engineer DRM code or otherwise try to circumvent copy protection.\\n',\n",
       " '   To counter the move to limit software use and redistribution, Richard Stallman in 1983 started the GNU project to create a free, open-source, UNIX- compatible operating system. In 1985, he published the GNU Manifesto, which argues that all software should be free and open-sourced. He also formed the Free Software Foundation (FSF) with the goal of encouraging the free exchange of software source code and the free use of that software. Rather than copyright its software, the FSF copylefts the software to encourage sharing and improvement. The GNU General Public License (GPL) codifies copylefting and is a common license under which free software is released. Fundamentally, GPL requires that the source code be distributed with any binaries and that any changes made to the source code be released under the same GPL license.\\n',\n",
       " '1.12.2 Linux\\n',\n",
       " 'As an example of an open-source operating system, consider GNU/Linux. The GNU project produced many UNIX-compatible tools, including compilers, editors, and utilities, but never released a kernel. In 1991, a student in Finland, Linus Torvalds, released a rudimentary UNIX-like kernel using the GNU compilers and tools and invited contributions worldwide. The advent of the Internet meant that anyone interested could download the source code, modify it, and submit changes to Torvalds. Releasing updates once a week allowed this so-called Linux operating system to grow rapidly, enhanced by several thousand programmers.\\n',\n",
       " '   The resulting GNU/Linux operating system has spawned hundreds of unique distributions, or custom builds, of the system. Major distributions include RedHat, SUSE, Fedora, Debian, Slackware, and Ubuntu. Distributions vary in function, utility, installed applications, hardware support, user inter- face, and purpose. For example, RedHat Enterprise Linux is geared to large commercial use. PCLinuxOS is a LiveCD  an operating system that can be booted and run from a CD-ROM without being installed on a systems hard disk. One variant of PCLinuxOS  called PCLinuxOS Supergamer DVDis a LiveDVD that includes graphics drivers and games. A gamer can run it on any compatible system simply by booting from the DVD. When the gamer is finished, a reboot of the system resets it to its installed operating system.\\n',\n",
       " '   You can run Linux on a Windows system using the following simple, free approach:\\n',\n",
       " '1. Download the free VMware Player tool from\\n',\n",
       " 'http://www.vmware.com/download/player/\\n',\n",
       " 'and install it on your system.\\n',\n",
       " '2. Choose a Linux version from among the hundreds of appliances, or virtual machine images, available from VMware at\\n',\n",
       " 'http://www.vmware.com/appliances/\\n',\n",
       " 'These images are preinstalled with operating systems and applications and include many flavors of Linux.\\n',\n",
       " '3. Boot the virtual machine within VMware Player.\\n',\n",
       " 'With this text, we provide a virtual machine image of Linux running the Debian release. This image contains the Linux source code as well as tools for software development. We cover examples involving that Linux image throughout this text, as well as in a detailed case study in Chapter 18.\\n',\n",
       " '1.12.3 BSD UNIX\\n',\n",
       " 'BSD UNIX has a longer and more complicated history than Linux. It started in 1978 as a derivative of AT&Ts UNIX. Releases from the University of California at Berkeley (UCB) came in source and binary form, but they were not open- source because a license from AT&T was required. BSD UNIXs development was slowed by a lawsuit by AT&T, but eventually a fully functional, open-source version, 4.4BSD-lite, was released in 1994.\\n',\n",
       " '   Just as with Linux, there are many distributions of BSD UNIX, including FreeBSD, NetBSD, OpenBSD, and DragonflyBSD. To explore the source code of FreeBSD, simply download the virtual machine image of the version of interest and boot it within VMware, as described above for Linux. The source code comes with the distribution and is stored in /usr/src/. The kernel source code is in /usr/src/sys. For example, to examine the virtual memory implementation code in the FreeBSD kernel, see the files in /usr/src/sys/vm. Darwin, the core kernel component  of  Mac  OS  X,  is  based  on  BSD UNIX and is open-sourced as well. That source code is available from http://www.opensource.apple.com/. Every Mac OS X release has its  open- source components posted at that site. The name of the package that contains the kernel begins with xnu. Apple also provides extensive developer tools, documentation, and support at http://connect.apple.com. For more informa-\\n',\n",
       " 'tion, see Appendix A.\\n',\n",
       " '1.12.4 Solaris\\n',\n",
       " 'Solaris is the commercial UNIX-based operating system of Sun Microsystems. Originally, Suns SunOS operating system was based on BSD UNIX. Sun moved to AT&Ts System V UNIX as its base in 1991. In 2005, Sun open-sourced most of the Solaris code as the OpenSolaris project. The purchase of Sun by Oracle in 2009, however, left the state of this project unclear. The source code as it was in 2005 is still available via a source code browser and for download at http://src.opensolaris.org/source.\\n',\n",
       " '   Several groups interested in using OpenSolaris have started from that base and expanded its features. Their working set is Project Illumos, which has expanded from the OpenSolaris base to include more features and to be the basis for several products. Illumos is available at http://wiki.illumos.org.\\n',\n",
       " '1.12.5 Open-Source Systems as Learning Tools\\n',\n",
       " 'The free software movement is driving legions of programmers to create thousands of open-source projects, including operating systems. Sites like http://freshmeat.net/   and   http://distrowatch.com/   provide  portals  to   many of these projects. As we stated earlier, open-source projects enable students to use source code as a learning tool. They can modify programs and test them,\\n',\n",
       " '1.13 Summary\\t47\\n',\n",
       " 'help find and fix bugs, and otherwise explore mature, full-featured operating systems, compilers, tools, user interfaces, and other types of programs. The availability of source code for historic projects, such as Multics, can help students to understand those projects and to build knowledge that will help in the implementation of new projects.\\n',\n",
       " '    GNU/Linux and BSD UNIX are all open-source operating systems, but each has its own goals, utility, licensing, and purpose. Sometimes, licenses are not mutually exclusive and cross-pollination occurs, allowing rapid improvements in operating-system projects. For example, several major components of OpenSolaris have been ported to BSD UNIX. The advantages of free software and open sourcing are likely to increase the number and quality of open-source projects, leading to an increase in the number of individuals and companies that use these projects.\\n',\n",
       " '1.13\\tSummary\\n',\n",
       " 'An operating system is software that manages the computer hardware, as well as providing an environment for application programs to run. Perhaps the most visible aspect of an operating system is the interface to the computer system it provides to the human user.\\n',\n",
       " '    For a computer to do its job of executing programs, the programs must be in main memory. Main memory is the only large storage area that the processor can access directly. It is an array of bytes, ranging in size from millions to billions. Each byte in memory has its own address. The main memory is usually a volatile storage device that loses its contents when power is turned off or lost. Most computer systems provide secondary storage as an extension of main memory. Secondary storage provides a form of nonvolatile storage that is capable of holding large quantities of data permanently. The most common secondary-storage device is a magnetic disk, which provides storage of both programs and data.\\n',\n",
       " '    The wide variety of storage systems in a computer system can be organized in a hierarchy according to speed and cost. The higher levels are expensive, but they are fast. As we move down the hierarchy, the cost per bit generally decreases, whereas the access time generally increases.\\n',\n",
       " '   There are several different strategies for designing a computer system. Single-processor systems have only one processor, while multiprocessor systems contain two or more processors that share physical memory and peripheral devices. The most common multiprocessor design is symmetric multiprocessing (or SMP), where all processors are considered peers and run independently of one another. Clustered systems are a specialized form of multiprocessor systems and consist of multiple computer systems connected by a local-area network.\\n',\n",
       " '   To best utilize the CPU, modern operating systems employ multiprogram- ming, which allows several jobs to be in memory at the same time, thus ensuring that the CPU always has a job to execute. Time-sharing systems are an exten- sion of multiprogramming wherein CPU scheduling algorithms rapidly switch between jobs, thus providing the illusion that each job is running concurrently. The operating system must ensure correct operation of the computer system. To prevent user programs from interfering with the proper operation of\\n',\n",
       " 'THE STUDY OF OPERATING SYSTEMS\\n',\n",
       " 'There has never been a more interesting time to study operating systems, and it has never been easier. The open-source movement has overtaken operating systems, causing many of them to be made available in both source and binary (executable) format. The list of operating systems available in both formats includes Linux, BSD UNIX, Solaris, and part of Mac OS X. The availability of source code allows us to study operating systems from the inside out. Questions that we could once answer only by looking at documentation or the behavior of an operating system we can now answer by examining the code itself.\\n',\n",
       " '  Operating systems that are no longer commercially viable have been open-sourced as well, enabling us to study how systems operated in a time of fewer CPU, memory, and storage resources. An extensive but incomplete list of open-source operating-system projects is available from http://dmoz.org/Computers/Software/Operating Systems/Open Source/.\\n',\n",
       " '  In addition, the rise of virtualization as a mainstream (and frequently free) computer function makes it possible to run many operating systems on top of one core system. For example, VMware ( http://www.vmware.com) provides a free player for Windows on which hundreds of free virtual appliances can run. Virtualbox ( http://www.virtualbox.com) provides a free, open- source virtual machine manager on many operating systems. Using such tools, students can try out hundreds of operating systems without dedicated hardware.\\n',\n",
       " '  In some cases, simulators of specific hardware are also available, allowing the operating system to run on native hardware, all within the confines of a modern computer and modern operating system. For example, a DECSYSTEM-20 simulator running on Mac OS X can boot TOPS-20, load the source tapes, and modify and compile a new TOPS-20 kernel. An interested student can search the Internet to find the original papers that describe the operating system, as well as the original manuals.\\n',\n",
       " '  The advent of open-source operating systems has also made it easier to make the move from student to operating-system developer. With some knowledge, some effort, and an Internet connection, a student can even create a new operating-system distribution. Just a few years ago, it was difficult or impossible to get access to source code. Now, such access is limited only by how much interest, time, and disk space a student has.\\n',\n",
       " 'the system, the hardware has two modes: user mode and kernel mode. Various instructions (such as I/O instructions and halt instructions) are privileged and can be executed only in kernel mode. The memory in which the operating system resides must also be protected from modification by the user. A timer prevents infinite loops. These facilities (dual mode, privileged instructions, memory protection, and timer interrupt) are basic building blocks used by operating systems to achieve correct operation.\\n',\n",
       " '   A process (or job) is the fundamental unit of work in an operating system. Process management includes creating and deleting processes and providing mechanisms for processes to communicate and synchronize with each other.\\n',\n",
       " 'Practice Exercises\\t49\\n',\n",
       " 'An operating system manages memory by keeping track of what parts of memory are being used and by whom. The operating system is also responsible for dynamically allocating and freeing memory space. Storage space is also managed by the operating system; this includes providing file systems for representing files and directories and managing space on mass-storage devices. Operating systems must also be concerned with protecting and securing the operating system and users. Protection measures control the access of processes or users to the resources made available by the computer system. Security measures are responsible for defending a computer system from external or\\n',\n",
       " 'internal attacks.\\n',\n",
       " '    Several data structures that are fundamental to computer science are widely used in operating systems, including lists, stacks, queues, trees, hash functions, maps, and bitmaps.\\n',\n",
       " '    Computing takes place in a variety of environments. Traditional computing involves desktop and laptop PCs, usually connected to a computer network. Mobile computing refers to computing on handheld smartphones and tablet computers, which offer several unique features. Distributed systems allow users to share resources on geographically dispersed hosts connected via a computer network. Services may be provided through either the client server model or the peer-to-peer model. Virtualization involves abstracting a computers hardware into several different execution environments. Cloud computing uses a distributed system to abstract services into a cloud, where users may access the services from remote locations. Real-time operating systems are designed for embedded environments, such as consumer devices, automobiles, and robotics.\\n',\n",
       " '    The free software movement has created thousands of open-source projects, including operating systems. Because of these projects, students are able to use source code as a learning tool. They can modify programs and test them, help find and fix bugs, and otherwise explore mature, full-featured operating systems, compilers, tools, user interfaces, and other types of programs.\\n',\n",
       " '    GNU/Linux and BSD UNIX are open-source operating systems. The advan- tages of free software and open sourcing are likely to increase the number and quality of open-source projects, leading to an increase in the number of individuals and companies that use these projects.\\n',\n",
       " 'Practice Exercises\\n',\n",
       " '1.1 What are the three main purposes of an operating system?\\n',\n",
       " '1.2 We have stressed the need for an operating system to make efficient use of the computing hardware. When is it appropriate for the operating system to forsake this principle and to waste resources? Why is such a system not really wasteful?\\n',\n",
       " '1.3 What is the main difficulty that a programmer must overcome in writing an operating system for a real-time environment?\\n',\n",
       " '1.4 Keeping in mind the various definitions of operating system, consider whether the operating system should include applications such as web browsers and mail programs. Argue both that it should and that it should not, and support your answers.\\n',\n",
       " '1.5 How does the distinction between kernel mode and user mode function as a rudimentary form of protection (security) system?\\n',\n",
       " '1.6 Which of the following instructions should be privileged?\\n',\n",
       " 'a. Set value of timer.\\n',\n",
       " 'b. Read the clock.\\n',\n",
       " 'c. Clear memory.\\n',\n",
       " 'd. Issue a trap instruction.\\n',\n",
       " 'e. Turn off interrupts.\\n',\n",
       " 'f. Modify entries in device-status table.\\n',\n",
       " 'g. Switch from user to kernel mode.\\n',\n",
       " 'h. Access I/O device.\\n',\n",
       " '1.7 Some early computers protected the operating system by placing it in a memory partition that could not be modified by either the user job or the operating system itself. Describe two difficulties that you think could arise with such a scheme.\\n',\n",
       " '1.8 Some CPUs provide for more than two modes of operation. What are two possible uses of these multiple modes?\\n',\n",
       " '1.9 Timers could be used to compute the current time. Provide a short description of how this could be accomplished.\\n',\n",
       " '1.10 Give two reasons why caches are useful. What problems do they solve? What problems do they cause? If a cache can be made as large as the device for which it is caching (for instance, a cache as large as a disk), why not make it that large and eliminate the device?\\n',\n",
       " '1.11 Distinguish between the client server and peer-to-peer models of distributed systems.\\n',\n",
       " 'Exercises\\n',\n",
       " '1.12 In a multiprogramming and time-sharing environment, several users share the system simultaneously. This situation can result in various security problems.\\n',\n",
       " 'a. What are two such problems?\\n',\n",
       " 'b. Can we ensure the same degree of security in a time-shared machine as in a dedicated machine? Explain your answer.\\n',\n",
       " '1.13 The issue of resource utilization shows up in different forms in different types of operating systems. List what resources must be managed carefully in the following settings:\\n',\n",
       " 'a. Mainframe or minicomputer systems\\n',\n",
       " 'b. Workstations connected to servers\\n',\n",
       " 'c. Mobile computers\\n',\n",
       " 'Exercises\\t51\\n',\n",
       " '1.14 Under what circumstances would a user be better off using a time- sharing system than a PC or a single-user workstation?\\n',\n",
       " '1.15 Describe the differences between symmetric and asymmetric multipro- cessing. What are three advantages and one disadvantage of multipro- cessor systems?\\n',\n",
       " '1.16 How do clustered systems differ from multiprocessor systems? What is required for two machines belonging to a cluster to cooperate to provide a highly available service?\\n',\n",
       " '1.17 Consider a computing cluster consisting of two nodes running a database. Describe two ways in which the cluster software can manage access to the data on the disk. Discuss the benefits and disadvantages of each.\\n',\n",
       " '1.18 How are network computers different from traditional personal com- puters? Describe some usage scenarios in which it is advantageous to use network computers.\\n',\n",
       " '1.19 What is the purpose of interrupts? How does an interrupt differ from a trap? Can traps be generated intentionally by a user program? If so, for what purpose?\\n',\n",
       " '1.20 Direct memory access is used for high-speed I/O devices in order to avoid increasing the CPUs execution load.\\n',\n",
       " 'a. How does the CPU interface with the device to coordinate the transfer?\\n',\n",
       " 'b. How does the CPU know when the memory operations are com- plete?\\n',\n",
       " 'c. The CPU is allowed to execute other programs while the DMA controller is transferring data. Does this process interfere with the execution of the user programs? If so, describe what forms of interference are caused.\\n',\n",
       " '1.21 Some computer systems do not provide a privileged mode of operation in hardware. Is it possible to construct a secure operating system for these computer systems? Give arguments both that it is and that it is not possible.\\n',\n",
       " '1.22 Many SMP systems have different levels of caches; one level is local to each processing core, and another level is shared among all processing cores. Why are caching systems designed this way?\\n',\n",
       " '1.23 Consider an SMP system similar to the one shown in Figure 1.6. Illustrate with an example how data residing in memory could in fact have a different value in each of the local caches.\\n',\n",
       " '1.24 Discuss, with examples, how the problem of maintaining coherence of cached data manifests itself in the following processing environments:\\n',\n",
       " 'a. Single-processor systems\\n',\n",
       " 'b. Multiprocessor systems\\n',\n",
       " 'c. Distributed systems\\n',\n",
       " '1.25 Describe a mechanism for enforcing memory protection in order to prevent a program from modifying the memory associated with other programs.\\n',\n",
       " '1.26 Which network configuration  LAN or WAN  would best suit the following environments?\\n',\n",
       " 'a. A campus student union\\n',\n",
       " 'b. Several campus locations across a statewide university system\\n',\n",
       " 'c. A neighborhood\\n',\n",
       " '1.27 Describe some of the challenges of designing operating systems for mobile devices compared with designing operating systems for tradi- tional PCs.\\n',\n",
       " '1.28 What are some advantages of peer-to-peer systems over client-server systems?\\n',\n",
       " '1.29 Describe some distributed applications that would be appropriate for a peer-to-peer system.\\n',\n",
       " '1.30 Identify several advantages and several disadvantages of open-source operating systems. Include the types of people who would find each aspect to be an advantage or a disadvantage.\\n',\n",
       " 'Bibliographical Notes\\n',\n",
       " '[Brookshear (2012)] provides an overview of computer science in general. Thorough coverage of data structures can be found in [Cormen et al. (2009)].\\n',\n",
       " '    [Russinovich and Solomon (2009)] give an overview of Microsoft Windows and covers considerable technical detail about the system internals and components. [McDougall and Mauro (2007)] cover the internals of the Solaris operating system. Mac OS X internals are discussed in [Singh (2007)]. [Love (2010)] provides an overview of the Linux operating system and great detail about data structures used in the Linux kernel.\\n',\n",
       " '   Many general textbooks  cover operating systems, including [Stallings (2011)], [Deitel et al. (2004)], and [Tanenbaum (2007)]. [Kurose and Ross (2013)] provides a general overview of computer networks, including a discussion of client-server and peer-to-peer systems. [Tarkoma and Lagerspetz (2011)] examines several different mobile operating systems, including Android and iOS.\\n',\n",
       " '   [Hennessy and Patterson (2012)] provide coverage of I/O systems and buses and of system architecture in general. [Bryant and OHallaron (2010)] provide a thorough overview of a computer system from the perspective of a computer programmer. Details of the Intel 64 instruction set and privilege modes can be found in [Intel (2011)].\\n',\n",
       " '   The history of open sourcing and its benefits and challenges appears in [Raymond (1999)]. The Free Software Foundation has published its philosophy in  http://www.gnu.org/philosophy/free-software-for-freedom.html.  The  open source of Mac OS X are available from http://www.apple.com/opensource/.\\n',\n",
       " 'Bibliography\\t53\\n',\n",
       " '   Wikipedia has an informative entry about the contributions of Richard Stallman at http://en.wikipedia.org/wiki/Richard Stallman.\\n',\n",
       " 'The source code of Multics is available at http://web.mit.edu/multics-history\\n',\n",
       " '/source/Multics Internet Server/Multics sources.html.\\n',\n",
       " 'Bibliography\\n',\n",
       " '[Brookshear (2012)]\\tJ. G. Brookshear, Computer Science: An Overview, Eleventh Edition, Addison-Wesley (2012).\\n',\n",
       " '[Bryant and OHallaron (2010)]\\tR. Bryant and D. OHallaron, Computer Systems: A Programmers Perspective, Second Edition, Addison-Wesley (2010).\\n',\n",
       " '[Cormen et al. (2009)]\\tT. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein,\\n',\n",
       " 'Introduction to Algorithms, Third Edition, MIT Press (2009).\\n',\n",
       " '[Deitel et al. (2004)]\\tH. Deitel, P. Deitel, and D. Choffnes, Operating Systems,\\n',\n",
       " 'Third Edition, Prentice Hall (2004).\\n',\n",
       " '[Hennessy and Patterson (2012)]\\tJ. Hennessy and D. Patterson, Computer Archi- tecture: A Quantitative Approach, Fifth Edition, Morgan Kaufmann (2012).\\n',\n",
       " '[Intel (2011)]\\tIntel 64 and IA-32 Architectures Software Developers Manual, Com- bined Volumes: 1, 2A, 2B, 3A and 3B. Intel Corporation (2011).\\n',\n",
       " '[Kurose and Ross (2013)]\\tJ. Kurose and K. Ross, Computer Networking A Top Down Approach, Sixth Edition, Addison-Wesley (2013).\\n',\n",
       " '[Love (2010)]\\tR. Love, Linux Kernel Development, Third Edition, Developers Library (2010).\\n',\n",
       " '[McDougall and Mauro (2007)]\\tR. McDougall and J. Mauro, Solaris Internals,\\n',\n",
       " 'Second Edition, Prentice Hall (2007).\\n',\n",
       " '[Raymond (1999)]\\tE. S. Raymond, The Cathedral and the Bazaar, OReilly & Associates (1999).\\n',\n",
       " '[Russinovich and Solomon (2009)] M. E. Russinovich and D. A. Solomon, Win- dows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition, Microsoft Press (2009).\\n',\n",
       " '[Singh (2007)] A. Singh, Mac OS X Internals: A Systems Approach, Addison- Wesley (2007).\\n',\n",
       " '[Stallings (2011)] W. Stallings, Operating Systems, Seventh Edition, Prentice Hall (2011).\\n',\n",
       " '[Tanenbaum (2007)] A. S. Tanenbaum, Modern Operating Systems, Third Edition, Prentice Hall (2007).\\n',\n",
       " '[Tarkoma and Lagerspetz (2011)] S. Tarkoma and E. Lagerspetz, Arching over the Mobile Computing Chasm: Platforms and Runtimes, IEEE Computer, Volume 44, (2011), pages 2228.\\n',\n",
       " 'Operating - System Structures\\n',\n",
       " 'An operating system provides the environment within which programs are executed. Internally, operating systems vary greatly in their makeup, since they are organized along many different lines. The design of a new operating system is a major task. It is important that the goals of the system be well defined before the design begins. These goals form the basis for choices among various algorithms and strategies.\\n',\n",
       " '   We can view an operating system from several vantage points. One view focuses on the services that the system provides; another, on the interface that it makes available to users and programmers; a third, on its components and their interconnections. In this chapter, we explore all three aspects of operating systems, showing the viewpoints of users, programmers, and operating system designers. We consider what services an operating system provides, how they are provided, how they are debugged, and what the various methodologies are for designing such systems. Finally, we describe how operating systems are created and how a computer starts its operating system.\\n',\n",
       " '2.1 Operating-System Services\\n',\n",
       " 'An operating system provides an environment for the execution of programs. It provides certain services to programs and to the users of those programs. The specific services provided, of course, differ from one operating system to another, but we can identify common classes. These operating system services are provided for the convenience of the programmer, to make the programming\\n',\n",
       " '55\\n',\n",
       " 'user and other system programs\\n',\n",
       " 'system calls\\n',\n",
       " ' program\\t\\tI/O\\t\\tfile\\tcommunication\\tresource\\taccounting execution\\toperations\\tsystems\\t\\tallocation\\n',\n",
       " 'error\\tprotection\\n',\n",
       " 'detection\\tand\\n',\n",
       " 'services\\tsecurity\\n',\n",
       " 'operating system\\n',\n",
       " 'hardware\\n',\n",
       " 'Figure 2.1 A view of operating system services.\\n',\n",
       " 'task easier. Figure 2.1 shows one view of the various operating-system services and how they interrelate.\\n',\n",
       " '   One set of operating system services provides functions that are helpful to the user.\\n',\n",
       " ' User interface. Almost all operating systems have a user interface (UI). This interface can take several forms. One is a command-line interface (CLI), which uses text commands and a method for entering them (say, a keyboard for typing in commands in a specific format with specific options). Another is a batch interface, in which commands and directives to control those commands are entered into files, and those files are executed. Most commonly, a graphical user interface (GUI) is used. Here, the interface is a window system with a pointing device to direct I/O, choose from menus, and make selections and a keyboard to enter text. Some systems provide two or all three of these variations.\\n',\n",
       " ' Program execution. The system must be able to load a program into memory and to run that program. The program must be able to end its execution, either normally or abnormally (indicating error).\\n',\n",
       " ' I/O operations.A running program may require I/O, which may involve a file or an I/O device. For specific devices, special functions may be desired (such as recording to a CD or DVD drive or blanking a display screen). For efficiency and protection, users usually cannot control I/O devices directly. Therefore, the operating system must provide a means to do I/O.\\n',\n",
       " ' File-system manipulation. The file system is of particular interest. Obvi- ously, programs need to read and write files and directories. They also need to create and delete them by name, search for a given file, and list file information. Finally, some operating systems include permissions management to allow or deny access to files or directories based on file ownership. Many operating systems provide a variety of file systems, sometimes to allow personal choice and sometimes to provide specific features or performance characteristics.\\n',\n",
       " '2.1 Operating-System Services\\t57\\n',\n",
       " ' Communications. There are many circumstances in which one process needs to exchange information with another process. Such communication may occur between processes that are executing on the same computer or between processes that are executing on different computer systems tied together by a computer network. Communications may be implemented via shared memory, in which two or more processes read and write to a shared section of memory, or message passing, in which packets of information in predefined formats are moved between processes by the operating system.\\n',\n",
       " ' Error detection. The operating system needs to be detecting and correcting errors constantly. Errors may occur in the CPU and memory hardware (such as a memory error or a power failure), in I/O devices (such as a parity error on disk, a connection failure on a network, or lack of paper in the printer), and in the user program (such as an arithmetic overflow, an attempt to access an illegal memory location, or a too-great use of CPU time). For each type of error, the operating system should take the appropriate action to ensure correct and consistent computing. Sometimes, it has no choice but to halt the system. At other times, it might terminate an error-causing process or return an error code to a process for the process to detect and possibly correct.\\n',\n",
       " '   Another set of operating system functions exists not for helping the user but rather for ensuring the efficient operation of the system itself. Systems with multiple users can gain efficiency by sharing the computer resources among the users.\\n',\n",
       " ' Resource allocation. When there are multiple users or multiple jobs running at the same time, resources must be allocated to each of them. The operating system manages many different types of resources. Some (such as CPU cycles, main memory, and file storage) may have special allocation code, whereas others (such as I/O devices) may have much more general request and release code. For instance, in determining how best to use the CPU, operating systems have CPU-scheduling routines that take into account the speed of the CPU, the jobs that must be executed, the number of registers available, and other factors. There may also be routines to allocate printers, USB storage drives, and other peripheral devices.\\n',\n",
       " ' Accounting. We want to keep track of which users use how much and what kinds of computer resources. This record keeping may be used for accounting (so that users can be billed) or simply for accumulating usage statistics. Usage statistics may be a valuable tool for researchers who wish to reconfigure the system to improve computing services.\\n',\n",
       " ' Protection and security. The owners of information stored in a multiuser or networked computer system may want to control use of that information. When several separate processes execute concurrently, it should not be possible for one process to interfere with the others or with the operating system itself. Protection involves ensuring that all access to system resources is controlled. Security of the system from outsiders is also important. Such security starts with requiring each user to authenticate\\n',\n",
       " 'himself or herself to the system, usually by means of a password, to gain access to system resources. It extends to defending external I/O devices, including network adapters, from invalid access attempts and to recording all such connections for detection of break-ins. If a system is to be protected and secure, precautions must be instituted throughout it. A chain is only as strong as its weakest link.\\n',\n",
       " '2.2 User and Operating-System Interface\\n',\n",
       " 'We mentioned earlier that there are several ways for users to interface with the operating system. Here, we discuss two fundamental approaches. One provides a command-line interface, or command interpreter, that allows users to directly enter commands to be performed by the operating system. The other allows users to interface with the operating system via a graphical user interface, or GUI.\\n',\n",
       " '2.2.1 Command Interpreters\\n',\n",
       " 'Some operating systems include the command interpreter in the kernel. Others, such as Windows and UNIX, treat the command interpreter as a special program that is running when a job is initiated or when a user first logs on (on interactive systems). On systems with multiple command interpreters to choose from, the interpreters are known as shells. For example, on UNIX and Linux systems, a user may choose among several different shells, including the Bourne shell, C shell, Bourne-Again shell, Korn shell, and others. Third-party shells and free user-written shells are also available. Most shells provide similar functionality, and a users choice of which shell to use is generally based on personal preference. Figure 2.2 shows the Bourne shell command interpreter being used on Solaris 10.\\n',\n",
       " '    The main function of the command interpreter is to get and execute the next user-specified command. Many of the commands given at this level manipulate files: create, delete, list, print, copy, execute, and so on. The MS-DOS and UNIX shells operate in this way. These commands can be implemented in two general ways.\\n',\n",
       " '   In one approach, the command interpreter itself contains the code to execute the command. For example, a command to delete a file may cause the command interpreter to jump to a section of its code that sets up the parameters and makes the appropriate system call. In this case, the number of commands that can be given determines the size of the command interpreter, since each command requires its own implementing code.\\n',\n",
       " 'An alternative approach used by UNIX, among other operating systems\\n',\n",
       " 'implements most commands through system programs. In this case, the command interpreter does not understand the command in any way; it merely uses the command to identify a file to be loaded into memory and executed.\\n',\n",
       " 'Thus, the UNIX command to delete a file\\n',\n",
       " 'rm  file.txt\\n',\n",
       " 'would search for a file called rm, load the file into memory, and execute it with the parameter file.txt. The function associated with the rm command would\\n',\n",
       " 'Figure 2.2 The Bourne shell command interpreter in Solrais 10.\\n',\n",
       " 'be defined completely by the code in the file rm. In this way, programmers can add new commands to the system easily by creating new files with the proper names. The command-interpreter program, which can be small, does not have to be changed for new commands to be added.\\n',\n",
       " '2.2.2 Graphical User Interfaces\\n',\n",
       " 'A second strategy for interfacing with the operating system is through a user- friendly graphical user interface, or GUI. Here, rather than entering commands directly via a command-line interface, users employ a mouse-based window- and-menu system characterized by a desktop metaphor. The user moves the mouse to position its pointer on images, or icons, on the screen (the desktop) that represent programs, files, directories, and system functions. Depending on the mouse pointers location, clicking a button on the mouse can invoke a program, select a file or directory known as a folder  or pull down a menu that contains commands.\\n',\n",
       " '    Graphical user interfaces first appeared due in part to research taking place in the early 1970s at Xerox PARC research facility. The first GUI appeared on the Xerox Alto computer in 1973. However, graphical interfaces became more widespread with the advent of Apple Macintosh computers in the 1980s. The user interface for the Macintosh operating system (Mac OS) has undergone various changes over the years, the most significant being the adoption of the Aqua interface that appeared with Mac OS X. Microsofts first version of Windows Version 1.0  was based on the addition of a GUI interface to the MS-DOS operating system. Later versions of Windows have made cosmetic\\n',\n",
       " 'changes in the appearance of the GUI along with several enhancements in its functionality.\\n',\n",
       " '   Because a mouse is impractical for most mobile systems, smartphones and handheld tablet computers typically use a touchscreen interface. Here, users interact by making gestures on the touchscreen for example, pressing and swiping fingers across the screen. Figure 2.3 illustrates the touchscreen of the Apple iPad. Whereas earlier smartphones included a physical keyboard, most smartphones now simulate a keyboard on the touchscreen.\\n',\n",
       " '   Traditionally, UNIX systems have been dominated by command-line inter- faces. Various GUI interfaces are available, however. These include the Common Desktop Environment (CDE) and X-Windows systems, which are common on commercial versions of UNIX, such as Solaris and IBMs AIX system. In addition, there has been significant development in GUI designs from various open-source projects, such as K Desktop Environment (or KDE) and the GNOME desktop by the GNU project. Both the KDE and GNOME desktops run on Linux and various UNIX systems and are available under open-source licenses, which means their source code is readily available for reading and for modification under specific license terms.\\n',\n",
       " 'Figure 2.3 The iPad touchscreen.\\n',\n",
       " '2.2.3 Choice of Interface\\n',\n",
       " 'The choice of whether to use a command-line or GUI interface is mostly one of personal preference. System administrators who manage computers and power users who have deep knowledge of a system frequently use the command-line interface. For them, it is more efficient, giving them faster access to the activities they need to perform. Indeed, on some systems, only a subset of system functions is available via the GUI, leaving the less common tasks to those who are command-line knowledgeable. Further, command- line interfaces usually make repetitive tasks easier, in part because they have their own programmability. For example, if a frequent task requires a set of command-line steps, those steps can be recorded into a file, and that file can be run just like a program. The program is not compiled into executable code but rather is interpreted by the command-line interface. These shell scripts are very common on systems that are command-line oriented, such as UNIX and Linux.\\n',\n",
       " '   In contrast, most Windows users are happy to use the Windows GUI environment and almost never use the MS-DOS shell interface. The various changes undergone by the Macintosh operating systems provide a nice study in contrast. Historically, Mac OS has not provided a command-line interface, always requiring its users to interface with the operating system using its GUI. However, with the release of Mac OS X (which is in part implemented using a UNIX kernel), the operating system now provides both a Aqua interface and a command-line interface. Figure 2.4 is a screenshot of the Mac OS X GUI.\\n',\n",
       " 'Figure 2.4 The Mac OS X GUI.\\n',\n",
       " '   The user interface can vary from system to system and even from user to user within a system. It typically is substantially removed from the actual system structure. The design of a useful and friendly user interface is therefore not a direct function of the operating system. In this book, we concentrate on the fundamental problems of providing adequate service to user programs. From the point of view of the operating system, we do not distinguish between user programs and system programs.\\n',\n",
       " '2.3 System Calls\\n',\n",
       " 'System calls provide an interface to the services made available by an operating system. These calls are generally available as routines written in C and C++, although certain low-level tasks (for example, tasks where hardware must be accessed directly) may have to be written using assembly-language instructions.\\n',\n",
       " '   Before we discuss how an operating system makes system calls available, lets first use an example to illustrate how system calls are used: writing a simple program to read data from one file and copy them to another file. The first input that the program will need is the names of the two files: the input file and the output file. These names can be specified in many ways, depending on the operating-system design. One approach is for the program to ask the user for the names. In an interactive system, this approach will require a sequence of system calls, first to write a prompting message on the screen and then to read from the keyboard the characters that define the two files. On mouse-based and icon-based systems, a menu of file names is usually displayed in a window. The user can then use the mouse to select the source name, and a window can be opened for the destination name to be specified. This sequence requires many I/O system calls.\\n',\n",
       " '   Once the two file names have been obtained, the program must open the input file and create the output file. Each of these operations requires another system call. Possible error conditions for each operation can require additional system calls. When the program tries to open the input file, for example, it may find that there is no file of that name or that the file is protected against access. In these cases, the program should print a message on the console (another sequence of system calls) and then terminate abnormally (another system call). If the input file exists, then we must create a new output file. We may find that there is already an output file with the same name. This situation may cause the program to abort (a system call), or we may delete the existing file (another system call) and create a new one (yet another system call). Another option, in an interactive system, is to ask the user (via a sequence of system calls to output the prompting message and to read the response from the terminal) whether to replace the existing file or to abort the program.\\n',\n",
       " '   When both files are set up, we enter a loop that reads from the input file (a system call) and writes to the output file (another system call). Each read and write must return status information regarding various possible error conditions. On input, the program may find that the end of the file has been reached or that there was a hardware failure in the read (such as a parity error). The write operation may encounter various errors, depending on the output device (for example, no more disk space).\\n',\n",
       " '   Finally, after the entire file is copied, the program may close both files (another system call), write a message to the console or window (more system calls), and finally terminate normally (the final system call). This system-call sequence is shown in Figure 2.5.\\n',\n",
       " '   As you can see, even simple programs may make heavy use of the operating system. Frequently, systems execute thousands of system calls per second. Most programmers never see this level of detail, however. Typically, application developers design programs according to an application programming interface (API). The API specifies a set of functions that are available to an application programmer, including the parameters that are passed to each function and the return values the programmer can expect. Three of the most common APIs available to application programmers are the Windows API for Windows systems, the POSIX API for POSIX-based systems (which include virtually all versions of UNIX, Linux, and Mac OS X), and the Java API for programs that run on the Java virtual machine. A programmer accesses an API via a library of code provided by the operating system. In the case of UNIX and Linux for programs written in the C language, the library is called libc. Note that unless specified the system-call names used throughout this text are generic examples. Each operating system has its own name for each system call.\\n',\n",
       " '   Behind the scenes, the functions that make up an API typically invoke the actual system calls on behalf of the application programmer. For example, the Windows function CreateProcess() (which unsurprisingly is used to create a new process) actually invokes the NTCreateProcess() system call in the Windows kernel.\\n',\n",
       " '   Why would an application programmer prefer programming according to an API rather than invoking actual system calls? There are several reasons for doing so. One benefit concerns program portability. An application program-\\n',\n",
       " 'Figure 2.5 Example of how system calls are used.\\n',\n",
       " 'mer designing a program using an API can expect her program to compile and run on any system that supports the same API (although, in reality, architectural differences often make this more difficult than it may appear). Furthermore, actual system calls can often be more detailed and difficult to work with than the API available to an application programmer. Nevertheless, there often exists a strong correlation between a function in the API and its associated system call within the kernel. In fact, many of the POSIX and Windows APIs are similar to the native system calls provided by the UNIX, Linux, and Windows operating systems.\\n',\n",
       " '   For most programming languages, the run-time support system (a set of functions built into libraries included with a compiler) provides a system- call interface that serves as the link to system calls made available by the operating system. The system-call interface intercepts function calls in the API and invokes the necessary system calls within the operating system. Typically, a number is associated with each system call, and the system-call interface maintains a table indexed according to these numbers. The system call interface\\n',\n",
       " 'open ( )\\n',\n",
       " 'user mode\\n',\n",
       " 'kernel mode\\n',\n",
       " 'open ( )\\n',\n",
       " 'Implementation of open ( ) system call\\n',\n",
       " 'return\\n',\n",
       " 'Figure 2.6 The handling of a user application invoking the open() system call.\\n',\n",
       " 'then invokes the intended system call in the operating-system kernel and returns the status of the system call and any return values.\\n',\n",
       " '   The caller need know nothing about how the system call is implemented or what it does during execution. Rather, the caller need only obey the API and understand what the operating system will do as a result of the execution of that system call. Thus, most of the details of the operating-system interface are hidden from the programmer by the API and are managed by the run-time support library. The relationship between an API, the system-call interface, and the operating system is shown in Figure 2.6, which illustrates how the operating system handles a user application invoking the open() system call. System calls occur in different ways, depending on the computer in use.\\n',\n",
       " 'Often, more information is required than simply the identity of the desired system call. The exact type and amount of information vary according to the particular operating system and call. For example, to get input, we may need to specify the file or device to use as the source, as well as the address and length of the memory buffer into which the input should be read. Of course, the device or file and length may be implicit in the call.\\n',\n",
       " '    Three general methods are used to pass parameters to the operating system. The simplest approach is to pass the parameters in registers. In some cases, however, there may be more parameters than registers. In these cases, the parameters are generally stored in a block, or table, in memory, and the address of the block is passed as a parameter in a register (Figure 2.7). This is the approach taken by Linux and Solaris. Parameters also can be placed, or pushed, onto the stack by the program and popped off the stack by the operating system. Some operating systems prefer the block or stack method because those approaches do not limit the number or length of parameters being passed.\\n',\n",
       " 'code for system call 13\\n',\n",
       " 'operating system\\n',\n",
       " 'Figure 2.7 Passing of parameters as a table.\\n',\n",
       " '2.4 Types of System Calls\\n',\n",
       " 'System calls can be grouped roughly into six major categories: process control, file manipulation, device manipulation, information maintenance, communications, and protection. In Sections 2.4.1 through 2.4.6, we briefly discuss the types of system calls that may be provided by an operating system. Most of these system calls support, or are supported by, concepts and functions that are discussed in later chapters. Figure 2.8 summarizes the types of system calls normally provided by an operating system. As mentioned, in this text, we normally refer to the system calls by generic names. Throughout the text, however, we provide examples of the actual counterparts to the system calls for Windows, UNIX, and Linux systems.\\n',\n",
       " '2.4.1 Process Control\\n',\n",
       " 'A running program needs to be able to halt its execution either normally (end()) or abnormally (abort()). If a system call is made to terminate the currently running program abnormally, or if the program runs into a problem and causes an error trap, a dump of memory is sometimes taken and an error message generated. The dump is written to disk and may be examined by a debugger a system program designed to aid the programmer in finding and correcting errors, or bugs to determine the cause of the problem. Under either normal or abnormal circumstances, the operating system must transfer control to the invoking command interpreter. The command interpreter then reads the next command. In an interactive system, the command interpreter simply continues with the next command; it is assumed that the user will issue an appropriate command to respond to any error. In a GUI system, a pop-up window might alert the user to the error and ask for guidance. In a batch system, the command interpreter usually terminates the entire job and continues with the next job. Some systems may allow for special recovery actions in case an error occurs. If the program discovers an error in its input and wants to terminate abnormally, it may also want to define an error level. More severe errors can be indicated by a higher-level error parameter. It is then\\n',\n",
       " ' Process control\\n',\n",
       " '? end, abort\\n',\n",
       " '? load, execute\\n',\n",
       " '? create process, terminate process\\n',\n",
       " '? get process attributes, set process attributes\\n',\n",
       " '? wait for time\\n',\n",
       " '? wait event, signal event\\n',\n",
       " '? allocate and free memory\\n',\n",
       " ' File management\\n',\n",
       " '? create file, delete file\\n',\n",
       " '? open, close\\n',\n",
       " '? read, write, reposition\\n',\n",
       " '? get file attributes, set file attributes\\n',\n",
       " ' Device management\\n',\n",
       " '? request device, release device\\n',\n",
       " '? read, write, reposition\\n',\n",
       " '? get device attributes, set device attributes\\n',\n",
       " '? logically attach or detach devices\\n',\n",
       " ' Information maintenance\\n',\n",
       " '? get time or date, set time or date\\n',\n",
       " '? get system data, set system data\\n',\n",
       " '? get process, file, or device attributes\\n',\n",
       " '? set process, file, or device attributes\\n',\n",
       " ' Communications\\n',\n",
       " '? create, delete communication connection\\n',\n",
       " '? send, receive messages\\n',\n",
       " '? transfer status information\\n',\n",
       " '? attach or detach remote devices\\n',\n",
       " 'Figure 2.8 Types of system calls.\\n',\n",
       " 'possible to combine normal and abnormal termination by defining a normal termination as an error at level 0. The command interpreter or a following program can use this error level to determine the next action automatically.\\n',\n",
       " '   A process or job executing one program may want to load() and execute() another program. This feature allows the command interpreter to execute a program as directed by, for example, a user command, the click of a\\n',\n",
       " 'Windows\\n',\n",
       " 'Unix\\n',\n",
       " 'Process\\n',\n",
       " 'CreateProcess()\\n',\n",
       " 'fork()\\n',\n",
       " 'Control\\n',\n",
       " 'ExitProcess()\\n',\n",
       " 'exit()\\n',\n",
       " 'WaitForSingleObject()\\n',\n",
       " 'wait()\\n',\n",
       " 'File\\n',\n",
       " 'CreateFile()\\n',\n",
       " 'open()\\n',\n",
       " 'Manipulation\\n',\n",
       " 'ReadFile()\\n',\n",
       " 'read()\\n',\n",
       " 'WriteFile()\\n',\n",
       " 'write()\\n',\n",
       " 'CloseHandle()\\n',\n",
       " 'close()\\n',\n",
       " 'Device\\n',\n",
       " 'SetConsoleMode()\\n',\n",
       " 'ioctl()\\n',\n",
       " 'Manipulation\\n',\n",
       " 'ReadConsole() WriteConsole()\\n',\n",
       " 'read()\\n',\n",
       " 'write()\\n',\n",
       " 'Information\\n',\n",
       " 'GetCurrentProcessID()\\n',\n",
       " 'getpid()\\n',\n",
       " 'Maintenance\\n',\n",
       " 'SetTimer()\\n',\n",
       " 'alarm()\\n',\n",
       " 'Sleep()\\n',\n",
       " 'sleep()\\n',\n",
       " 'Communication\\n',\n",
       " 'CreatePipe()\\n',\n",
       " 'pipe()\\n',\n",
       " 'CreateFileMapping()\\n',\n",
       " 'shm open()\\n',\n",
       " 'MapViewOfFile()\\n',\n",
       " 'mmap()\\n',\n",
       " 'Protection\\n',\n",
       " 'SetFileSecurity()\\n',\n",
       " 'chmod()\\n',\n",
       " 'InitlializeSecurityDescriptor()\\n',\n",
       " 'umask()\\n',\n",
       " 'SetSecurityDescriptorGroup()\\n',\n",
       " 'chown()\\n',\n",
       " 'mouse, or a batch command. An interesting question is where to return control when the loaded program terminates. This question is related to whether the existing program is lost, saved, or allowed to continue execution concurrently with the new program.\\n',\n",
       " '   If control returns to the existing program when the new program termi- nates, we must save the memory image of the existing program; thus, we have effectively created a mechanism for one program to call another program. If both programs continue concurrently, we have created a new job or process to be multiprogrammed. Often, there is a system call specifically for this purpose (create process() or submit job()).\\n',\n",
       " '   If we create a new job or process, or perhaps even a set of jobs or processes, we should be able to control its execution. This control requires the ability to determine and reset the attributes of a job or process, includ- ing the jobs priority, its maximum allowable execution time, and so on (get process attributes() and set process attributes()). We may also want to terminate a job or process that we created (terminate process()) if we find that it is incorrect or is no longer needed.\\n',\n",
       " '#include <stdio.h>\\n',\n",
       " 'int main ( )\\n',\n",
       " '{\\n',\n",
       " 'printf (\"Greetings\");\\n',\n",
       " '}\\n',\n",
       " 'return 0;\\n',\n",
       " '   Having created new jobs or processes, we may need to wait for them to finish their execution. We may want to wait for a certain amount of time to pass (wait time()). More probably, we will want to wait for a specific event to occur (wait event()). The jobs or processes should then signal when that event has occurred (signal event()).\\n',\n",
       " '   Quite often, two or more processes may share data. To ensure the integrity of the data being shared, operating systems often provide system calls allowing a process to lock shared data. Then, no other process can access the data until the lock is released. Typically, such system calls include acquire lock() and release lock(). System calls of these types, dealing with the coordination of concurrent processes, are discussed in great detail in Chapter 5.\\n',\n",
       " '   There are so many facets of and variations in process and job control that we next use two examples one involving a single-tasking system and the other a multitasking system to clarify these concepts. The MS-DOS operating system is an example of a single-tasking system. It has a command interpreter that is invoked when the computer is started (Figure 2.9(a)). Because MS-DOS is single-tasking, it uses a simple method to run a program and does not create a new process. It loads the program into memory, writing over most of itself to\\n',\n",
       " '\\t\\n',\n",
       " '(a)\\t(b)\\n',\n",
       " 'Figure 2.9 MS-DOS execution. (a) At system startup. (b) Running a program.\\n',\n",
       " 'give the program as much memory as possible (Figure 2.9(b)). Next, it sets the instruction pointer to the first instruction of the program. The program then runs, and either an error causes a trap, or the program executes a system call to terminate. In either case, the error code is saved in the system memory for later use. Following this action, the small portion of the command interpreter that was not overwritten resumes execution. Its first task is to reload the rest of the command interpreter from disk. Then the command interpreter makes the previous error code available to the user or to the next program.\\n',\n",
       " '    FreeBSD (derived from Berkeley UNIX) is an example of a multitasking system. When a user logs on to the system, the shell of the users choice is run. This shell is similar to the MS-DOS shell in that it accepts commands and executes programs that the user requests. However, since FreeBSD is a multitasking system, the command interpreter may continue running while another program is executed (Figure 2.10). To start a new process, the shell\\n',\n",
       " 'process D\\n',\n",
       " 'free memory\\n',\n",
       " 'process C\\n',\n",
       " 'interpreter\\n',\n",
       " 'process B\\n',\n",
       " 'kernel\\n',\n",
       " 'Figure 2.10 FreeBSD running multiple programs.\\n',\n",
       " 'executes a fork() system call. Then, the selected program is loaded into memory via an exec() system call, and the program is executed. Depending on the way the command was issued, the shell then either waits for the process to finish or runs the process in the background. In the latter case, the shell immediately requests another command. When a process is running in the background, it cannot receive input directly from the keyboard, because the shell is using this resource. I/O is therefore done through files or through a GUI interface. Meanwhile, the user is free to ask the shell to run other programs, to monitor the progress of the running process, to change that programs priority, and so on. When the process is done, it executes an exit() system call to terminate, returning to the invoking process a status code of 0 or a nonzero error code. This status or error code is then available to the shell or other programs. Processes are discussed in Chapter 3 with a program example using the fork() and exec() system calls.\\n',\n",
       " '2.4.2 File Management\\n',\n",
       " 'The file system is discussed in more detail in Chapters 11 and 12. We can, however, identify several common system calls dealing with files.\\n',\n",
       " '   We first need to be able to create() and delete() files. Either system call requires the name of the file and perhaps some of the files attributes. Once the file is created, we need to open() it and to use it. We may also read(), write(), or reposition() (rewind or skip to the end of the file, for example). Finally, we need to close() the file, indicating that we are no longer using it. We may need these same sets of operations for directories if we have a directory structure for organizing files in the file system. In addition, for either files or directories, we need to be able to determine the values of various attributes and perhaps to reset them if necessary. File attributes include the file name, file type, protection codes, accounting information, and so on. At least two system calls, get file attributes() and set file attributes(), are required for this function. Some operating systems provide many more calls, such as calls for file move() and copy(). Others might provide an API that performs those operations using code and other system calls, and others might provide system programs to perform those tasks. If the system programs are callable by other programs, then each can be considered an API by other system\\n',\n",
       " 'programs.\\n',\n",
       " '2.4.3 Device Management\\n',\n",
       " 'A process may need several resources to execute main memory, disk drives, access to files, and so on. If the resources are available, they can be granted, and control can be returned to the user process. Otherwise, the process will have to wait until sufficient resources are available.\\n',\n",
       " '   The various resources controlled by the operating system can be thought of as devices. Some of these devices are physical devices (for example, disk drives), while others can be thought of as abstract or virtual devices (for example, files). A system with multiple users may require us to first request() a device, to ensure exclusive use of it. After we are finished with the device, we release() it. These functions are similar to the open() and close() system calls for files. Other operating systems allow unmanaged access to devices.\\n',\n",
       " 'The hazard then is the potential for device contention and perhaps deadlock, which are described in Chapter 7.\\n',\n",
       " '   Once the device has been requested (and allocated to us), we can read(), write(), and (possibly) reposition() the device, just as we can with files. In fact, the similarity between I/O devices and files is so great that many operating systems, including UNIX, merge the two into a combined file device structure. In this case, a set of system calls is used on both files and devices. Sometimes, I/O devices are identified by special file names, directory placement, or file attributes.\\n',\n",
       " '    The user interface can also make files and devices appear to be similar, even though the underlying system calls are dissimilar. This is another example of the many design decisions that go into building an operating system and user interface.\\n',\n",
       " '2.4.4 Information Maintenance\\n',\n",
       " 'Many system calls exist simply for the purpose of transferring information between the user program and the operating system. For example, most systems have a system call to return the current time() and date(). Other system calls may return information about the system, such as the number of current users, the version number of the operating system, the amount of free memory or disk space, and so on.\\n',\n",
       " '   Another set of system calls is helpful in debugging a program. Many systems provide system calls to dump() memory. This provision is useful for debugging. A program trace lists each system call as it is executed. Even microprocessors provide a CPU mode known as single step, in which a trap is executed by the CPU after every instruction. The trap is usually caught by a debugger.\\n',\n",
       " '   Many operating systems provide a time profile of a program to indicate the amount of time that the program executes at a particular location or set of locations. A time profile requires either a tracing facility or regular timer interrupts. At every occurrence of the timer interrupt, the value of the program counter is recorded. With sufficiently frequent timer interrupts, a statistical picture of the time spent on various parts of the program can be obtained.\\n',\n",
       " '   In addition, the operating system keeps information about all its processes, and system calls are used to access this information. Generally, calls are also used to reset the process information (get process attributes() and set process attributes()). In Section 3.1.3, we discuss what information is normally kept.\\n',\n",
       " '2.4.5 Communication\\n',\n",
       " 'There are two common models of interprocess communication: the message- passing model and the shared-memory model. In the message-passing model, the communicating processes exchange messages with one another to transfer information. Messages can be exchanged between the processes either directly or indirectly through a common mailbox. Before communication can take place, a connection must be opened. The name of the other communicator must be known, be it another process on the same system or a process on another computer connected by a communications network. Each computer in a network has a host name by which it is commonly known. A host also has a\\n',\n",
       " 'network identifier, such as an IP address. Similarly, each process has a process name, and this name is translated into an identifier by which the operating system can refer to the process. The get hostid() and get processid() system calls do this translation. The identifiers are then passed to the general- purpose open() and close() calls provided by the file system or to specific open connection() and close connection() system calls, depending on the systems model of communication. The recipient process usually must give its permission for communication to take place with an accept connection() call. Most processes that will be receiving connections are special-purpose daemons, which are system programs provided for that purpose. They execute a wait for connection() call and are awakened when a connection is made. The source of the communication, known as the client, and the receiving daemon, known as a server, then exchange messages by using read message() and write message() system calls. The close connection() call terminates the communication.\\n',\n",
       " '   In the shared-memory model, processes use shared memory create() and shared memory attach() system calls to create and gain access to regions of memory owned by other processes. Recall that, normally, the operating system tries to prevent one process from accessing another processs memory. Shared memory requires that two or more processes agree to remove this restriction. They can then exchange information by reading and writing data in the shared areas. The form of the data is determined by the processes and is not under the operating systems control. The processes are also responsible for ensuring that they are not writing to the same location simultaneously. Such mechanisms are discussed in Chapter 5. In Chapter 4, we look at a variation of the process scheme threads in which memory is shared by default.\\n',\n",
       " '   Both of the models just discussed are common in operating systems, and most systems implement both. Message passing is useful for exchanging smaller amounts of data, because no conflicts need be avoided. It is also easier to implement than is shared memory for intercomputer communication. Shared memory allows maximum speed and convenience of communication, since it can be done at memory transfer speeds when it takes place within a computer. Problems exist, however, in the areas of protection and synchronization between the processes sharing memory.\\n',\n",
       " '2.4.6 Protection\\n',\n",
       " 'Protection provides a mechanism for controlling access to the resources provided by a computer system. Historically, protection was a concern only on multiprogrammed computer systems with several users. However, with the advent of networking and the Internet, all computer systems, from servers to mobile handheld devices, must be concerned with protection.\\n',\n",
       " '   Typically, system calls providing protection include set permission() and get permission(), which manipulate the permission settings of resources such as files and disks. The allow user() and deny user() system calls specify whether particular users can or cannot be allowed access to certain resources.\\n',\n",
       " '    We cover protection in Chapter 14 and the much larger issue of security in Chapter 15.\\n',\n",
       " '2.5 System Programs\\n',\n",
       " 'Another aspect of a modern system is its collection of system programs. Recall Figure 1.1, which depicted the logical computer hierarchy. At the lowest level is hardware. Next is the operating system, then the system programs, and finally the application programs. System programs, also known as system utilities, provide a convenient environment for program development and execution. Some of them are simply user interfaces to system calls. Others are considerably more complex. They can be divided into these categories:\\n',\n",
       " ' File management. These programs create, delete, copy, rename, print, dump, list, and generally manipulate files and directories.\\n',\n",
       " ' Status information. Some programs simply ask the system for the date, time, amount of available memory or disk space, number of users, or similar status information. Others are more complex, providing detailed performance, logging, and debugging information. Typically, these pro- grams format and print the output to the terminal or other output devices or files or display it in a window of the GUI. Some systems also support a registry, which is used to store and retrieve configuration information.\\n',\n",
       " ' File modification. Several text editors may be available to create and modify the content of files stored on disk or other storage devices. There may also be special commands to search contents of files or perform transformations of the text.\\n',\n",
       " ' Programming-language support. Compilers, assemblers, debuggers, and interpreters for common programming languages (such as C, C++, Java, and PERL) are often provided with the operating system or available as a separate download.\\n',\n",
       " ' Program loading and execution. Once a program is assembled or com- piled, it must be loaded into memory to be executed. The system may provide absolute loaders, relocatable loaders, linkage editors, and overlay loaders. Debugging systems for either higher-level languages or machine language are needed as well.\\n',\n",
       " ' Communications. These programs provide the mechanism for creating virtual connections among processes, users, and computer systems. They allow users to send messages to one anothers screens, to browse Web pages, to send e-mail messages, to log in remotely, or to transfer files from one machine to another.\\n',\n",
       " ' Background services. All general-purpose systems have methods for launching certain system-program processes at boot time. Some of these processes terminate after completing their tasks, while others continue to run until the system is halted. Constantly running system-program processes are known as services, subsystems, or daemons. One example is the network daemon discussed in Section 2.4.5. In that example, a system needed a service to listen for network connections in order to connect those requests to the correct processes. Other examples include process schedulers that start processes according to a specified schedule, system error monitoring services, and print servers. Typical systems have dozens\\n',\n",
       " 'of daemons. In addition, operating systems that run important activities in user context rather than in kernel context may use daemons to run these activities.\\n',\n",
       " '   Along with system programs, most operating systems are supplied with programs that are useful in solving common problems or performing common operations. Such application programs include Web browsers, word proces- sors and text formatters, spreadsheets, database systems, compilers, plotting and statistical-analysis packages, and games.\\n',\n",
       " '   The view of the operating system seen by most users is defined by the application and system programs, rather than by the actual system calls. Consider a users PC. When a users computer is running the Mac OS X operating system, the user might see the GUI, featuring a mouse-and-windows interface. Alternatively, or even in one of the windows, the user might have a command-line UNIX shell. Both use the same set of system calls, but the system calls look different and act in different ways. Further confusing the user view, consider the user dual-booting from Mac OS X into Windows. Now the same user on the same hardware has two entirely different interfaces and two sets of applications using the same physical resources. On the same hardware, then, a user can be exposed to multiple user interfaces sequentially or concurrently.\\n',\n",
       " '2.6 Operating-System Design and Implementation\\n',\n",
       " 'In this section, we discuss problems we face in designing and implementing an operating system. There are, of course, no complete solutions to such problems, but there are approaches that have proved successful.\\n',\n",
       " '2.6.1 Design Goals\\n',\n",
       " 'The first problem in designing a system is to define goals and specifications. At the highest level, the design of the system will be affected by the choice of hardware and the type of system: batch, time sharing, single user, multiuser, distributed, real time, or general purpose.\\n',\n",
       " '   Beyond this highest design level, the requirements may be much harder to specify. The requirements can, however, be divided into two basic groups: user goals and system goals.\\n',\n",
       " '   Users want certain obvious properties in a system. The system should be convenient to use, easy to learn and to use, reliable, safe, and fast. Of course, these specifications are not particularly useful in the system design, since there is no general agreement on how to achieve them.\\n',\n",
       " '   A similar set of requirements can be defined by those people who must design, create, maintain, and operate the system. The system should be easy to design, implement, and maintain; and it should be flexible, reliable, error free, and efficient. Again, these requirements are vague and may be interpreted in various ways.\\n',\n",
       " '   There is, in short, no unique solution to the problem of defining the requirements for an operating system. The wide range of systems in existence shows that different requirements can result in a large variety of solutions for different environments. For example, the requirements for VxWorks, a real-\\n',\n",
       " 'time operating system for embedded systems, must have been substantially different from those for MVS, a large multiuser, multiaccess operating system for IBM mainframes.\\n',\n",
       " '   Specifying and designing an operating system is a highly creative task. Although no textbook can tell you how to do it, general principles have been developed in the field of software engineering, and we turn now to a discussion of some of these principles.\\n',\n",
       " '2.6.2 Mechanisms and Policies\\n',\n",
       " 'One important principle is the separation of policy from mechanism. Mecha- nisms determine how to do something; policies determine what will be done. For example, the timer construct (see Section 1.5.2) is a mechanism for ensuring CPU protection, but deciding how long the timer is to be set for a particular user is a policy decision.\\n',\n",
       " '    The separation of policy and mechanism is important for flexibility. Policies are likely to change across places or over time. In the worst case, each change in policy would require a change in the underlying mechanism. A general mechanism insensitive to changes in policy would be more desirable. A change in policy would then require redefinition of only certain parameters of the system. For instance, consider a mechanism for giving priority to certain types of programs over others. If the mechanism is properly separated from policy, it can be used either to support a policy decision that I/O-intensive programs should have priority over CPU-intensive ones or to support the opposite policy. Microkernel-based operating systems (Section 2.7.3) take the separation of mechanism and policy to one extreme by implementing a basic set of primitive building blocks. These blocks are almost policy free, allowing more advanced mechanisms and policies to be added via user-created kernel modules or user programs themselves. As an example, consider the history of UNIX. At first, it had a time-sharing scheduler. In the latest version of Solaris, scheduling is controlled by loadable tables. Depending on the table currently loaded, the system can be time sharing, batch processing, real time, fair share, or any combination. Making the scheduling mechanism general purpose allows vast policy changes to be made with a single load-new-table command. At the other extreme is a system such as Windows, in which both mechanism and policy are encoded in the system to enforce a global look and feel. All applications have similar interfaces, because the interface itself is built into the kernel and system libraries. The Mac OS X operating system has similar\\n',\n",
       " 'functionality.\\n',\n",
       " '   Policy decisions are important for all resource allocation. Whenever it is necessary to decide whether or not to allocate a resource, a policy decision must be made. Whenever the question is how rather than what, it is a mechanism that must be determined.\\n',\n",
       " '2.6.3 Implementation\\n',\n",
       " 'Once an operating system is designed, it must be implemented. Because operating systems are collections of many programs, written by many people over a long period of time, it is difficult to make general statements about how they are implemented.\\n',\n",
       " '    Early operating systems were written in assembly language. Now, although some operating systems are still written in assembly language, most are written in a higher-level language such as C or an even higher-level language such as C++. Actually, an operating system can be written in more than one language. The lowest levels of the kernel might be assembly language. Higher-level routines might be in C, and system programs might be in C or C++, in interpreted scripting languages like PERL or Python, or in shell scripts. In fact, a given Linux distribution probably includes programs written in all of those languages.\\n',\n",
       " '   The first system that was not written in assembly language was probably the Master Control Program (MCP) for Burroughs computers. MCP was written in a variant of ALGOL. MULTICS, developed at MIT, was written mainly in the system programming language PL/1. The Linux and Windows operating system kernels are written mostly in C, although there are some small sections of assembly code for device drivers and for saving and restoring the state of registers.\\n',\n",
       " '   The advantages of using a higher-level language, or at least a systems- implementation language, for implementing operating systems are the same as those gained when the language is used for application programs: the code can be written faster, is more compact, and is easier to understand and debug. In addition, improvements in compiler technology will improve the generated code for the entire operating system by simple recompilation. Finally, an operating system is far easier to port to move to some other hardware if it is written in a higher-level language. For example, MS-DOS was written in Intel 8088 assembly language. Consequently, it runs natively only on the Intel X86 family of CPUs. (Note that although MS-DOS runs natively only on Intel X86, emulators of the X86 instruction set allow the operating system to run on other CPUs but more slowly, and with higher resource use. As we mentioned in Chapter 1, emulators are programs that duplicate the functionality of one system on another system.) The Linux operating system, in contrast, is written mostly in C and is available natively on a number of different CPUs, including Intel X86, Oracle SPARC, and IBMPowerPC.\\n',\n",
       " '    The only possible disadvantages of implementing an operating system in a higher-level language are reduced speed and increased storage requirements. This, however, is no longer a major issue in todays systems. Although an expert assembly-language programmer can produce efficient small routines, for large programs a modern compiler can perform complex analysis and apply sophisticated optimizations that produce excellent code. Modern processors have deep pipelining and multiple functional units that can handle the details of complex dependencies much more easily than can the human mind.\\n',\n",
       " '   As is true in other systems, major performance improvements in oper- ating systems are more likely to be the result of better data structures and algorithms than of excellent assembly-language code. In addition, although operating systems are large, only a small amount of the code is critical to high performance; the interrupt handler, I/O manager, memory manager, and CPU scheduler are probably the most critical routines. After the system is written and is working correctly, bottleneck routines can be identified and can be replaced with assembly-language equivalents.\\n',\n",
       " '2.7 Operating-System Structure\\n',\n",
       " 'A system as large and complex as a modern operating system must be engineered carefully if it is to function properly and be modified easily. A common approach is to partition the task into small components, or modules, rather than have one monolithic system. Each of these modules should be a well-defined portion of the system, with carefully defined inputs, outputs, and functions. We have already discussed briefly in Chapter 1 the common components of operating systems. In this section, we discuss how these components are interconnected and melded into a kernel.\\n',\n",
       " '2.7.1 Simple Structure\\n',\n",
       " 'Many operating systems do not have well-defined structures. Frequently, such systems started as small, simple, and limited systems and then grew beyond their original scope. MS-DOS is an example of such a system. It was originally designed and implemented by a few people who had no idea that it would become so popular. It was written to provide the most functionality in the least space, so it was not carefully divided into modules. Figure 2.11 shows its structure.\\n',\n",
       " '   In MS-DOS, the interfaces and levels of functionality are not well separated. For instance, application programs are able to access the basic I/O routines to write directly to the display and disk drives. Such freedom leaves MS-DOS vulnerable to errant (or malicious) programs, causing entire system crashes when user programs fail. Of course, MS-DOS was also limited by the hardware of its era. Because the Intel 8088 for which it was written provides no dual mode and no hardware protection, the designers of MS-DOS had no choice but to leave the base hardware accessible.\\n',\n",
       " '   Another example of limited structuring is the original UNIX operating system. Like MS-DOS, UNIX initially was limited by hardware functionality. It consists of two separable parts: the kernel and the system programs. The kernel\\n',\n",
       " 'Figure 2.11 MS-DOS layer structure.\\n',\n",
       " ' \\n',\n",
       " 'Figure 2.12 Traditional UNIX system structure.\\n',\n",
       " 'is further separated into a series of interfaces and device drivers, which have been added and expanded over the years as UNIX has evolved. We can view the traditional UNIX operating system as being layered to some extent, as shown in Figure 2.12. Everything below the system-call interface and above the physical hardware is the kernel. The kernel provides the file system, CPU scheduling, memory management, and other operating-system functions through system calls. Taken in sum, that is an enormous amount of functionality to be combined into one level. This monolithic structure was difficult to implement and maintain. It had a distinct performance advantage, however: there is very little overhead in the system call interface or in communication within the kernel. We still see evidence of this simple, monolithic structure in the UNIX, Linux, and Windows operating systems.\\n',\n",
       " '2.7.2 Layered Approach\\n',\n",
       " 'With proper hardware support, operating systems can be broken into pieces that are smaller and more appropriate than those allowed by the original MS-DOS and UNIX systems. The operating system can then retain much greater control over the computer and over the applications that make use of that computer. Implementers have more freedom in changing the inner workings of the system and in creating modular operating systems. Under a top- down approach, the overall functionality and features are determined and are separated into components. Information hiding is also important, because it leaves programmers free to implement the low-level routines as they see fit, provided that the external interface of the routine stays unchanged and that the routine itself performs the advertised task.\\n',\n",
       " '   A system can be made modular in many ways. One method is the layered approach, in which the operating system is broken into a number of layers (levels). The bottom layer (layer 0) is the hardware; the highest (layer N) is the user interface. This layering structure is depicted in Figure 2.13.\\n',\n",
       " 'Figure 2.13 A layered operating system.\\n',\n",
       " '    An operating-system layer is an implementation of an abstract object made up of data and the operations that can manipulate those data. A typical operating-system layer say, layer M  consists of data structures and a set of routines that can be invoked by higher-level layers. Layer M, in turn, can invoke operations on lower-level layers.\\n',\n",
       " '   The main advantage of the layered approach is simplicity of construction and debugging. The layers are selected so that each uses functions (operations) and services of only lower-level layers. This approach simplifies debugging and system verification. The first layer can be debugged without any concern for the rest of the system, because, by definition, it uses only the basic hardware (which is assumed correct) to implement its functions. Once the first layer is debugged, its correct functioning can be assumed while the second layer is debugged, and so on. If an error is found during the debugging of a particular layer, the error must be on that layer, because the layers below it are already debugged. Thus, the design and implementation of the system are simplified. Each layer is implemented only with operations provided by lower-level layers. A layer does not need to know how these operations are implemented; it needs to know only what these operations do. Hence, each layer hides the existence of certain data structures, operations, and hardware from higher-level\\n',\n",
       " 'layers.\\n',\n",
       " '   The major difficulty with the layered approach involves appropriately defining the various layers. Because a layer can use only lower-level layers, careful planning is necessary. For example, the device driver for the backing store (disk space used by virtual-memory algorithms) must be at a lower level than the memory-management routines, because memory management requires the ability to use the backing store.\\n',\n",
       " '   Other requirements may not be so obvious. The backing-store driver would normally be above the CPU scheduler, because the driver may need to wait for I/O and the CPU can be rescheduled during this time. However, on a large\\n',\n",
       " 'system, the CPU scheduler may have more information about all the active processes than can fit in memory. Therefore, this information may need to be swapped in and out of memory, requiring the backing-store driver routine to be below the CPU scheduler.\\n',\n",
       " '   A final problem with layered implementations is that they tend to be less efficient than other types. For instance, when a user program executes an I/O operation, it executes a system call that is trapped to the I/O layer, which calls the memory-management layer, which in turn calls the CPU-scheduling layer, which is then passed to the hardware. At each layer, the parameters may be modified, data may need to be passed, and so on. Each layer adds overhead to the system call. The net result is a system call that takes longer than does one on a nonlayered system.\\n',\n",
       " '   These limitations have caused a small backlash against layering in recent years. Fewer layers with more functionality are being designed, providing most of the advantages of modularized code while avoiding the problems of layer definition and interaction.\\n',\n",
       " '2.7.3 Microkernels\\n',\n",
       " 'We have already seen that as UNIX expanded, the kernel became large and difficult to manage. In the mid-1980s, researchers at Carnegie Mellon University developed an operating system called Mach that modularized the kernel using the microkernel approach. This method structures the operating system by removing all nonessential components from the kernel and implementing them as system and user-level programs. The result is a smaller kernel. There is little consensus regarding which services should remain in the kernel and which should be implemented in user space. Typically, however, microkernels provide minimal process and memory management, in addition to a communication facility. Figure 2.14 illustrates the architecture of a typical microkernel.\\n',\n",
       " '    The main function of the microkernel is to provide communication between the client program and the various services that are also running in user space. Communication is provided through message passing, which was described in Section 2.4.5. For example, if the client program wishes to access a file, it\\n',\n",
       " 'user mode\\n',\n",
       " 'kernel mode\\n',\n",
       " 'Figure 2.14 Architecture of a typical microkernel.\\n',\n",
       " 'must interact with the file server. The client program and service never interact directly. Rather, they communicate indirectly by exchanging messages with the microkernel.\\n',\n",
       " '   One benefit of the microkernel approach is that it makes extending the operating system easier. All new services are added to user space and consequently do not require modification of the kernel. When the kernel does have to be modified, the changes tend to be fewer, because the microkernel is a smaller kernel. The resulting operating system is easier to port from one hardware design to another. The microkernel also provides more security and reliability, since most services are running as user rather than kernel processes. If a service fails, the rest of the operating system remains untouched. Some contemporary operating systems have used the microkernel approach. Tru64 UNIX (formerly Digital UNIX) provides a UNIX interface to the user, but it is implemented with a Mach kernel. The Mach kernel maps UNIX system calls into messages to the appropriate user-level services. The Mac OS X kernel (also known as Darwin) is also partly based on the Mach microkernel. Another example is QNX, a real-time operating system for embedded systems. The QNX Neutrino microkernel provides services for message passing and process scheduling. It also handles low-level network communication and hardware interrupts. All other services in QNX are provided by standard\\n',\n",
       " 'processes that run outside the kernel in user mode.\\n',\n",
       " '   Unfortunately, the performance of microkernels can suffer due to increased system-function overhead. Consider the history of Windows NT. The first release had a layered microkernel organization. This versions performance was low compared with that of Windows 95. Windows NT 4.0 partially corrected the performance problem by moving layers from user space to kernel space and integrating them more closely. By the time Windows XP was designed, Windows architecture had become more monolithic than microkernel.\\n',\n",
       " '2.7.4 Modules\\n',\n",
       " 'Perhaps the best current methodology for operating-system design involves using loadable kernel modules. Here, the kernel has a set of core components and links in additional services via modules, either at boot time or during run time. This type of design is common in modern implementations of UNIX, such as Solaris, Linux, and Mac OS X, as well as Windows.\\n',\n",
       " '   The idea of the design is for the kernel to provide core services while other services are implemented dynamically, as the kernel is running. Linking services dynamically is preferable to adding new features directly to the kernel, which would require recompiling the kernel every time a change was made. Thus, for example, we might build CPU scheduling and memory management algorithms directly into the kernel and then add support for different file systems by way of loadable modules.\\n',\n",
       " '   The overall result resembles a layered system in that each kernel section has defined, protected interfaces; but it is more flexible than a layered system, because any module can call any other module. The approach is also similar to the microkernel approach in that the primary module has only core functions and knowledge of how to load and communicate with other modules; but it\\n',\n",
       " 'Figure 2.15 Solaris loadable modules.\\n',\n",
       " 'is more efficient, because modules do not need to invoke message passing in order to communicate.\\n',\n",
       " '   The Solaris operating system structure, shown in Figure 2.15, is organized around a core kernel with seven types of loadable kernel modules:\\n',\n",
       " '1. Scheduling classes\\n',\n",
       " '2. File systems\\n',\n",
       " '3. Loadable system calls\\n',\n",
       " '4. Executable formats\\n',\n",
       " '5. STREAMS modules\\n',\n",
       " '6. Miscellaneous\\n',\n",
       " '7. Device and bus drivers\\n',\n",
       " '   Linux also uses loadable kernel modules, primarily for supporting device drivers and file systems. We cover creating loadable kernel modules in Linux as a programming exercise at the end of this chapter.\\n',\n",
       " '2.7.5 Hybrid Systems\\n',\n",
       " 'In practice, very few operating systems adopt a single, strictly defined structure. Instead, they combine different structures, resulting in hybrid systems that address performance, security, and usability issues. For example, both Linux and Solaris are monolithic, because having the operating system in a single address space provides very efficient performance. However, they are also modular, so that new functionality can be dynamically added to the kernel. Windows is largely monolithic as well (again primarily for performance reasons), but it retains some behavior typical of microkernel systems, including providing support for separate subsystems (known as operating-system personalities) that run as user-mode processes. Windows systems also provide support for dynamically loadable kernel modules. We provide case studies of Linux and Windows 7 in in Chapters 18 and 19, respectively. In the remainder of this section, we explore the structure of\\n',\n",
       " 'three hybrid systems: the Apple Mac OS X operating system and the two most prominent mobile operating systems iOS and Android.\\n',\n",
       " '2.7.5.1 Mac OS X\\n',\n",
       " 'The Apple Mac OS X operating system uses a hybrid structure. As shown in Figure 2.16, it is a layered system. The top layers include the Aqua user interface (Figure 2.4) and a set of application environments and services. Notably, the Cocoa environment specifies an API for the Objective-C programming language, which is used for writing Mac OS X applications. Below these layers is the kernel environment, which consists primarily of the Mach microkernel and the BSD UNIX kernel. Mach provides memory management; support for remote procedure calls (RPCs) and interprocess communication (IPC) facilities, including message passing; and thread scheduling. The BSD component provides a BSD command-line interface, support for networking and file systems, and an implementation of POSIX APIs, including Pthreads. In addition to Mach and BSD, the kernel environment provides an I/O kit for development of device drivers and dynamically loadable modules (which Mac OS X refers to as kernel extensions). As shown in Figure 2.16, the BSD application environment can make use of BSD facilities directly.\\n',\n",
       " '2.7.5.2 iOS\\n',\n",
       " 'iOS is a mobile operating system designed by Apple to run its smartphone, the iPhone, as well as its tablet computer, the iPad. iOS is structured on the Mac OS X operating system, with added functionality pertinent to mobile devices, but does not directly run Mac OS X applications. The structure of iOS appears in Figure 2.17.\\n',\n",
       " '    Cocoa Touch is an API for Objective-C that provides several frameworks for developing applications that run on iOS devices. The fundamental difference between Cocoa, mentioned earlier, and Cocoa Touch is that the latter provides support for hardware features unique to mobile devices, such as touch screens. The media services layer provides services for graphics, audio, and video.\\n',\n",
       " 'Figure 2.16 The Mac OS X structure.\\n',\n",
       " 'Figure 2.17 Architecture of Apples iOS.\\n',\n",
       " 'The core services layer provides a variety of features, including support for cloud computing and databases. The bottom layer represents the core operating system, which is based on the kernel environment shown in Figure 2.16.\\n',\n",
       " '2.7.5.3 Android\\n',\n",
       " 'The Android operating system was designed by the Open Handset Alliance (led primarily by Google) and was developed for Android smartphones and tablet computers. Whereas iOS is designed to run on Apple mobile devices and is close-sourced, Android runs on a variety of mobile platforms and is open-sourced, partly explaining its rapid rise in popularity. The structure of Android appears in Figure 2.18.\\n',\n",
       " '   Android is similar to iOS in that it is a layered stack of software that provides a rich set of frameworks for developing mobile applications. At the bottom of this software stack is the Linux kernel, although it has been modified by Google and is currently outside the normal distribution of Linux releases.\\n',\n",
       " 'Figure 2.18 Architecture of Googles Android.\\n',\n",
       " 'Linux is used primarily for process, memory, and device-driver support for hardware and has been expanded to include power management. The Android runtime environment includes a core set of libraries as well as the Dalvik virtual machine. Software designers for Android devices develop applications in the Java language. However, rather than using the standard Java API, Google has designed a separate Android API for Java development. The Java class files are first compiled to Java bytecode and then translated into an executable file that runs on the Dalvik virtual machine. The Dalvik virtual machine was designed for Android and is optimized for mobile devices with limited memory and CPU processing capabilities.\\n',\n",
       " '    The set of libraries available for Android applications includes frameworks for developing web browsers (webkit), database support (SQLite), and multi- media. The libc library is similar to the standard C library but is much smaller and has been designed for the slower CPUs that characterize mobile devices.\\n',\n",
       " '2.8 Operating-System Debugging\\n',\n",
       " 'We have mentioned debugging frequently in this chapter. Here, we take a closer look. Broadly, debugging is the activity of finding and fixing errors in a system, both in hardware and in software. Performance problems are considered bugs, so debugging can also include performance tuning, which seeks to improve performance by removing processing bottlenecks. In this section, we explore debugging process and kernel errors and performance problems. Hardware debugging is outside the scope of this text.\\n',\n",
       " '2.8.1 Failure Analysis\\n',\n",
       " 'If a process fails, most operating systems write the error information to a log file to alert system operators or users that the problem occurred. The operating system can also take a core dump a capture of the memory of the process and store it in a file for later analysis. (Memory was referred to as the core in the early days of computing.) Running programs and core dumps can be probed by a debugger, which allows a programmer to explore the code and memory of a process.\\n',\n",
       " '    Debugging user-level process code is a challenge. Operating-system kernel debugging is even more complex because of the size and complexity of the kernel, its control of the hardware, and the lack of user-level debugging tools. A failure in the kernel is called a crash. Whena crash occurs, error information is saved to a log file, and the memory state is saved to a crash dump.\\n',\n",
       " '   Operating-system debugging and process debugging frequently use dif- ferent tools and techniques due to the very different nature of these two tasks. Consider that a kernel failure in the file-system code would make it risky for the kernel to try to save its state to a file on the file system before rebooting. A common technique is to save the kernels memory state to a section of disk set aside for this purpose that contains no file system. If the kernel detects an unrecoverable error, it writes the entire contents of memory, or at least the kernel-owned parts of the system memory, to the disk area. When the system reboots, a process runs to gather the data from that area and write it to a crash\\n',\n",
       " 'dump file within a file system for analysis. Obviously, such strategies would be unnecessary for debugging ordinary user-level processes.\\n',\n",
       " '2.8.2 Performance Tuning\\n',\n",
       " 'We mentioned earlier that performance tuning seeks to improve performance by removing processing bottlenecks. To identify bottlenecks, we must be able to monitor system performance. Thus, the operating system must have some means of computing and displaying measures of system behavior. In a number of systems, the operating system does this by producing trace listings of system behavior. All interesting events are logged with their time and important parameters and are written to a file. Later, an analysis program can process the log file to determine system performance and to identify bottlenecks and inefficiencies. These same traces can be run as input for a simulation of a suggested improved system. Traces also can help people to find errors in operating-system behavior.\\n',\n",
       " '   Another approach to performance tuning uses single-purpose, interactive tools that allow users and administrators to question the state of various system components to look for bottlenecks. One such tool employs the UNIX command top to display the resources used on the system, as well as a sorted list of the top resource-using processes. Other tools display the state of disk I/O, memory allocation, and network traffic.\\n',\n",
       " '   The Windows Task Manager is a similar tool for Windows systems. The task manager includes information for current applications as well as processes, CPU and memory usage, and networking statistics. A screen shot of the task manager appears in Figure 2.19.\\n',\n",
       " '   Making operating systems easier to understand, debug, and tune as they run is an active area of research and implementation. A new generation of kernel-enabled performance analysis tools has made significant improvements in how this goal can be achieved. Next, we discuss a leading example of such a tool: the Solaris 10 DTrace dynamic tracing facility.\\n',\n",
       " '2.8.3 DTrace\\n',\n",
       " 'DTrace is a facility that dynamically adds probes to a running system, both in user processes and in the kernel. These probes can be queried via the D programming language to determine an astonishing amount about the kernel, the system state, and process activities. For example, Figure 2.20 follows an application as it executes a system call (ioctl()) and shows the functional calls within the kernel as they execute to perform the system call. Lines ending with U are executed in user mode, and lines ending in K in kernel mode.\\n',\n",
       " 'Figure 2.19 The Windows task manager.\\n',\n",
       " '   Debugging the interactions between user-level and kernel code is nearly impossible without a toolset that understands both sets of code and can instrument the interactions. For that toolset to be truly useful, it must be able to debug any area of a system, including areas that were not written with debugging in mind, and do so without affecting system reliability. This tool must also have a minimum performance impact ideally it should have no impact when not in use and a proportional impact during use. The DTrace tool meets these requirements and provides a dynamic, safe, low-impact debugging environment.\\n',\n",
       " '   Until the DTrace framework and tools became available with Solaris 10, kernel debugging was usually shrouded in mystery and accomplished via happenstance and archaic code and tools. For example, CPUs havea breakpoint feature that will halt execution and allow a debugger to examine the state of the system. Then execution can continue until the next breakpoint or termination. This method cannot be used in a multiuser operating-system kernel without negatively affecting all of the users on the system. Profiling, which periodically samples the instruction pointer to determine which code is being executed, can show statistical trends but not individual activities. Code can be included in the kernel to emit specific data under specific circumstances, but that code slows down the kernel and tends not to be included in the part of the kernel where the specific problem being debugged is occurring.\\n',\n",
       " '# ./all.d pgrep xclock XEventsQueued dtrace: script ./all.d matched 52377 probes\\n',\n",
       " 'CPU\\n',\n",
       " 'FUNCTION\\n',\n",
       " '0\\n',\n",
       " '> XEventsQueued\\n',\n",
       " 'U\\n',\n",
       " '0\\n',\n",
       " '> _XEventsQueued\\n',\n",
       " 'U\\n',\n",
       " '0\\n',\n",
       " '> _X11TransBytesReadable\\n',\n",
       " 'U\\n',\n",
       " '0\\n',\n",
       " '< _X11TransBytesReadable\\n',\n",
       " 'U\\n',\n",
       " '0\\n',\n",
       " '> _X11TransSocketBytesReadable\\n',\n",
       " 'U\\n',\n",
       " '0\\n',\n",
       " '< _X11TransSocketBytesreadable\\n',\n",
       " 'U\\n',\n",
       " '0\\n',\n",
       " '> ioctl\\n',\n",
       " 'U\\n',\n",
       " '0\\n',\n",
       " '> ioctl\\n',\n",
       " 'K\\n',\n",
       " '0\\n',\n",
       " '> getf\\n',\n",
       " 'K\\n',\n",
       " '0\\n',\n",
       " '> set_active_fd\\n',\n",
       " 'K\\n',\n",
       " '0\\n',\n",
       " '< set_active_fd\\n',\n",
       " 'K\\n',\n",
       " '0\\n',\n",
       " '< getf\\n',\n",
       " 'K\\n',\n",
       " '0\\n',\n",
       " '> get_udatamodel\\n',\n",
       " 'K\\n',\n",
       " '0\\n',\n",
       " '< get_udatamodel\\n',\n",
       " 'K\\n',\n",
       " '...\\n',\n",
       " '0\\n',\n",
       " '> releasef\\n',\n",
       " 'K\\n',\n",
       " '0\\n',\n",
       " '> clear_active_fd\\n',\n",
       " 'K\\n',\n",
       " '0\\n',\n",
       " '< clear_active_fd\\n',\n",
       " 'K\\n',\n",
       " '0\\n',\n",
       " '> cv_broadcast\\n',\n",
       " 'K\\n',\n",
       " '0\\n',\n",
       " '< cv_broadcast\\n',\n",
       " 'K\\n',\n",
       " '0\\n',\n",
       " '< releasef\\n',\n",
       " 'K\\n',\n",
       " '0\\n',\n",
       " '< ioctl\\n',\n",
       " 'K\\n',\n",
       " '0\\n',\n",
       " '< ioctl\\n',\n",
       " 'U\\n',\n",
       " '0\\n',\n",
       " '< _XEventsQueued\\n',\n",
       " 'U\\n',\n",
       " '0\\n',\n",
       " '< XEventsQueued\\n',\n",
       " 'U\\n',\n",
       " 'Figure 2.20 Solaris 10 dtrace follows a system call within the kernel.\\n',\n",
       " '   In contrast, DTrace runs on production systems  systems that are running important or critical applications  and causes no harm to the system. It slows activities while enabled, but after execution it resets the system to its pre-debugging state. It is also a broad and deep tool. It can broadly debug everything happening in the system (both at the user and kernel levels and between the user and kernel layers). It can also delve deep into code, showing individual CPU instructions or kernel subroutine activities.\\n',\n",
       " '   DTrace is composed of a compiler, a framework, providers of probes written within that framework, and consumers of those probes. DTrace providers create probes. Kernel structures exist to keep track of all probes that the providers have created. The probes are stored in a hash-table data structure that is hashed by name and indexed according to unique probe identifiers. When a probe is enabled, a bit of code in the area to be probed is rewritten to call dtrace probe(probe identifier) and then continue with the codes original operation. Different providers create different kinds of probes. For example, a kernel system-call probe works differently from a user-process probe, and that is different from an I/O probe.\\n',\n",
       " '    DTrace features a compiler that generates a byte code that is run in the kernel. This code is assured to be safe by the compiler. For example, no loops are allowed, and only specific kernel state modifications are allowed when specifically requested. Only users with DTrace privileges (or root users)\\n',\n",
       " 'are allowed to use DTrace, as it can retrieve private kernel data (and modify data if requested). The generated code runs in the kernel and enables probes. It also enables consumers in user mode and enables communications between the two.\\n',\n",
       " '   A DTrace consumer is code that is interested in a probe and its results. A consumer requests that the provider create one or more probes. When a probe fires, it emits data that are managed by the kernel. Within the kernel, actions called enabling control blocks, or ECBs, are performed when probes fire. One probe can cause multiple ECBs to execute if more than one consumer is interested in that probe. Each ECB contains a predicate (if statement) that can filter out that ECB. Otherwise, the list of actions in the ECB is executed. The most common action is to capture some bit of data, such as a variables value at that point of the probe execution. By gathering such data, a complete picture of a user or kernel action can be built. Further, probes firing from both user space and the kernel can show how a user-level action caused kernel-level reactions. Such data are invaluable for performance monitoring and code optimization. Once the probe consumer terminates, its ECBs are removed. If there are no ECBs consuming a probe, the probe is removed. That involves rewriting the code to remove the dtrace probe() call and put back the original code. Thus, before a probe is created and after it is destroyed, the system is exactly the\\n',\n",
       " 'same, as if no probing occurred.\\n',\n",
       " '    DTrace takes care to assure that probes do not use too much memory or CPU capacity, which could harm the running system. The buffers used to hold the probe results are monitored for exceeding default and maximum limits. CPU time for probe execution is monitored as well. If limits are exceeded, the consumer is terminated, along with the offending probes. Buffers are allocated per CPU to avoid contention and data loss.\\n',\n",
       " '    An example of D code and its output shows some of its utility. The following program shows the DTrace code to enable scheduler probes and record the amount of CPU time of each process running with user ID 101 while those probes are enabled (that is, while the program runs):\\n',\n",
       " 'sched:::on-cpu uid == 101\\n',\n",
       " '{  self->ts = timestamp;\\n',\n",
       " '}\\n',\n",
       " 'sched:::off-cpu self->ts\\n',\n",
       " '{ @time[execname] = sum(timestamp - self->ts); self->ts = 0;\\n',\n",
       " '}\\n',\n",
       " 'The output of the program, showing the processes and how much time (in nanoseconds) they spend running on the CPUs, is shown in Figure 2.21.\\n',\n",
       " '    Because DTrace is part of the open-source OpenSolaris version of the Solaris 10 operating system, it has been added to other operating systems when those\\n',\n",
       " '2.9 Operating-System Generation\\t91\\n',\n",
       " '# dtrace -s sched.d\\n',\n",
       " 'dtrace: script sched.d matched 6 probes C\\n',\n",
       " 'gnome-settings-d\\t142354\\n',\n",
       " 'gnome-vfs-daemon\\n',\n",
       " '158243\\n',\n",
       " 'dsdm\\n',\n",
       " '189804\\n',\n",
       " 'wnck-applet\\n',\n",
       " '200030\\n',\n",
       " 'gnome-panel\\n',\n",
       " '277864\\n',\n",
       " 'clock-applet\\n',\n",
       " '374916\\n',\n",
       " 'mapping-daemon\\n',\n",
       " '385475\\n',\n",
       " 'xscreensaver\\n',\n",
       " '514177\\n',\n",
       " 'metacity\\n',\n",
       " '539281\\n',\n",
       " 'Xorg\\n',\n",
       " '2579646\\n',\n",
       " 'gnome-terminal\\n',\n",
       " '5007269\\n',\n",
       " 'mixer applet2\\n',\n",
       " '7388447\\n',\n",
       " 'java\\n',\n",
       " '10769137\\n',\n",
       " 'Figure 2.21 Output of the D code.\\n',\n",
       " 'systems do not have conflicting license agreements. For example, DTrace has been added to Mac OS X and FreeBSD and will likely spread further due to its unique capabilities. Other operating systems, especially the Linux derivatives, are adding kernel-tracing functionality as well. Still other operating systems are beginning to include performance and tracing tools fostered by research at various institutions, including the Paradyn project.\\n',\n",
       " '2.9 Operating-System Generation\\n',\n",
       " 'It is possible to design, code, and implement an operating system specifically for one machine at one site. More commonly, however, operating systems are designed to run on any of a class of machines at a variety of sites with a variety of peripheral configurations. The system must then be configured or generated for each specific computer site, a process sometimes known as system generation SYSGEN.\\n',\n",
       " '   The operating system is normally distributed on disk, on CD-ROM or DVD-ROM, or as an ISO image, which is a file in the format of a CD-ROM or DVD-ROM. To generate a system, we use a special program. This SYSGEN program reads from a  given file, or asks the operator of the  system for information concerning the specific configuration of the hardware system, or probes the hardware directly to determine what components are there. The following kinds of information must be determined.\\n',\n",
       " ' What CPU is to be used? What options (extended instruction sets, floating- point arithmetic, and so on) are installed? For multiple CPU systems, each CPU may be described.\\n',\n",
       " ' How will the boot disk be formatted? How many sections, or partitions, will it be separated into, and what will go into each partition?\\n',\n",
       " ' How much memory is available? Some systems will determine this value themselves by referencing memory location after memory location until an illegal address fault is generated. This procedure defines the final legal address and hence the amount of available memory.\\n',\n",
       " ' What devices are available? The system will need to know how to address each device (the device number), the device interrupt number, the devices type and model, and any special device characteristics.\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDict={'context':s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(dataDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Internally, operating systems vary greatly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Because an operating system is large and co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Introduction\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An operating system is a program that manages ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Before we can explore the details of compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12040</th>\n",
       "      <td>X\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12041</th>\n",
       "      <td>x86-64 architecture, 387\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12042</th>\n",
       "      <td>Xen, 714\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12043</th>\n",
       "      <td>Z\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12044</th>\n",
       "      <td>zones, 728\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12045 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 context\n",
       "0         Internally, operating systems vary greatly ...\n",
       "1         Because an operating system is large and co...\n",
       "2                                         Introduction\\n\n",
       "3      An operating system is a program that manages ...\n",
       "4          Before we can explore the details of compu...\n",
       "...                                                  ...\n",
       "12040                                                X\\n\n",
       "12041                         x86-64 architecture, 387\\n\n",
       "12042                                         Xen, 714\\n\n",
       "12043                                                Z\\n\n",
       "12044                                       zones, 728\\n\n",
       "\n",
       "[12045 rows x 1 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,val=train_test_split(data,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=BlenderbotForCausalLM.from_pretrained('facebook/blenderbot-400M-distill')\n",
    "tokenizer=AutoTokenizer.from_pretrained('facebook/blenderbot-400M-distill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(examples):\n",
    "    return tokenizer(examples[\"context\"],max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata=Dataset.from_pandas(train)\n",
    "valdata=Dataset.from_pandas(val)\n",
    "data_dict=DatasetDict({\n",
    "    'train':traindata,\n",
    "    'validation':valdata\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', '__index_level_0__'],\n",
       "        num_rows: 9033\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['context', '__index_level_0__'],\n",
       "        num_rows: 3012\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 9033/9033 [00:02<00:00, 3524.37 examples/s]\n",
      "Map: 100%|| 3012/3012 [00:00<00:00, 3857.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "token_dataset=data_dict.map(\n",
    "    prep,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    remove_columns=data_dict[\"train\"].column_names\n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 9033\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 3012\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlenderbotConfig {\n",
       "  \"_name_or_path\": \"facebook/blenderbot-400M-distill\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": true,\n",
       "  \"architectures\": [\n",
       "    \"BlenderbotForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classif_dropout\": 0.0,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 1280,\n",
       "  \"decoder_attention_heads\": 32,\n",
       "  \"decoder_ffn_dim\": 5120,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 12,\n",
       "  \"decoder_start_token_id\": 1,\n",
       "  \"do_blenderbot_90_layernorm\": true,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 32,\n",
       "  \"encoder_ffn_dim\": 5120,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 2,\n",
       "  \"encoder_no_repeat_ngram_size\": 3,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"extra_layer_norm\": false,\n",
       "  \"extra_pos_embeddings\": 0,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_decoder\": true,\n",
       "  \"is_encoder_decoder\": false,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"layernorm_variant\": \"prelayernorm\",\n",
       "  \"length_penalty\": 0.65,\n",
       "  \"max_length\": 60,\n",
       "  \"max_position_embeddings\": 128,\n",
       "  \"min_length\": 20,\n",
       "  \"model_type\": \"blenderbot\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": true,\n",
       "  \"normalize_embedding\": false,\n",
       "  \"num_beams\": 10,\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"scale_embedding\": true,\n",
       "  \"static_position_embeddings\": false,\n",
       "  \"transformers_version\": \"4.42.3\",\n",
       "  \"unk_token_id\": 3,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 8008\n",
       "}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=64,                                # Adapter size or other fine-tuning parameter\n",
    "    lora_alpha=32,                       # Adapter initialization strength\n",
    "    target_modules=[\n",
    "    \"model.shared\",                       # Shared modules across encoder and decoder\n",
    "    \"model.encoder.layers\",               # Encoder layers for processing input sequences\n",
    "    \"model.encoder.self_attn.q_proj\",     # Attention projection for encoder self-attention query\n",
    "    \"model.encoder.self_attn.v_proj\",     # Attention projection for encoder self-attention value\n",
    "    \"model.decoder.layers.0.self_attn.q_proj\", # Example of accessing a specific Linear layer\n",
    "    \"model.decoder.layers.0.self_attn.v_proj\", # Example of accessing another specific Linear layer\n",
    "],\n",
    "    lora_dropout=0.05,                   # Dropout rate for LORA\n",
    "    bias=\"none\",                         # Bias setting (e.g., none, fixed, adapt)\n",
    "    task_type=TaskType.CAUSAL_LM         # Task type for causal language modeling (adjust if necessary)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_peft_model(model,lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 327,680 || all params: 325,608,960 || trainable%: 0.1006\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
