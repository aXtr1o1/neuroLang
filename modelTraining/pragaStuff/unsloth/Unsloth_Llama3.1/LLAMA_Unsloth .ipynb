{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkSnE5D15CX8",
        "outputId": "b81e1495-e485-45b8-99a0-9df623d779ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG8i5-pkAJej"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoIEftC534Ri",
        "outputId": "77e362d6-4afd-471e-a4e4-60aabd4067ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8w0o7Ap4aO1"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "b1b559db59ff49e5aa56dbe988c06cfa",
            "48488603ffcd4f0c92da86c778c912ff",
            "98357043412f4debb4335e4c9d3da29e",
            "e83b0541df6f484eb38d6ca8a27ea92c",
            "a52f2a33ed664b3fadeadd06ed5a35c8",
            "02d2d36b5e134cc588256e50ec6d49f7",
            "284e8f6f30c840d7a4e9e91667a3614e",
            "30b7b85e597e4686929f75ccbd2534f4",
            "a96fa91b973949238968f4009bf37f8a",
            "25b64988b1494ea9a67eb8ddbe0df2bd",
            "dc9aef1ea9454f7997fcfc5e156537a5",
            "cd29f4c9913d4417a5abe66b46f2d52f",
            "4d455f15f9314fd4b5a38a188907797e",
            "5ba2524f281f4ed8aa51417767a7ebb0",
            "f6978ffbdd774fa79c258ce6da563445",
            "a4152df4f38e4dbfba0a06f9fec3b5ff",
            "82bf35dea0c64f34bce08c79613022cc",
            "d7dd1acc86f843eb886f2ecd960741b4",
            "cdfe1982940a4f4e862bc3deb6a2cf4b",
            "3e88208bde114c1ea71ef489d7f83d66",
            "b486faea4afa435aae8c01b3f0171c87",
            "7bd2a4f7c93e464c83e182e89a848a3e"
          ]
        },
        "id": "O7X3rw5Z391_",
        "outputId": "ad248df4-e5dd-4e60-b1ba-e0b98b1f663b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.4.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1b559db59ff49e5aa56dbe988c06cfa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd29f4c9913d4417a5abe66b46f2d52f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model,tokenizer=FastLanguageModel.from_pretrained(\"/lora_model\",load_in_4bit=True,dtype=dtype,max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7NTp5Sy-164"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is question,answer and followup question. Provide a followup question for the answer given.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Answer:\n",
        "{}\n",
        "\n",
        "### Followup question:\n",
        "{}\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MK4R0uA4Hzf",
        "outputId": "401ea4c6-22dc-4c1d-f8bd-8b2177a170ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the Question:How can you implement unit testing in Angular applications? Briefly explain the purpose of a testing framework like Jasmine.\n",
            "Enter the Answer:Unit testing in Angular involves testing individual components and services in isolation. This helps ensure they function as expected and improves overall code quality.  Jasmine is a popular testing framework that provides tools for writing clear and concise unit tests. It allows you to define test cases that simulate\n",
            "<|begin_of_text|>Below is question,answer and followup question. Provide a followup question for the answer given.\n",
            "\n",
            "### Question:\n",
            "How can you implement unit testing in Angular applications? Briefly explain the purpose of a testing framework like Jasmine.\n",
            "\n",
            "### Answer:\n",
            "Unit testing in Angular involves testing individual components and services in isolation. This helps ensure they function as expected and improves overall code quality.  Jasmine is a popular testing framework that provides tools for writing clear and concise unit tests. It allows you to define test cases that simulate\n",
            "\n",
            "### Followup question:\n",
            " \n",
            "\n",
            "### Followup question:\n",
            "What is the importance of unit testing in Angular development?<|end_of_text|>\n",
            "Time Taken is 1.8271303176879883 seconds\n"
          ]
        }
      ],
      "source": [
        "Question=input(\"Enter the Question:\")\n",
        "Answer=input(\"Enter the Answer:\")\n",
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "       alpaca_prompt.format(Question,Answer,\" \")\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "start_time=time.time()\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "end_time=time.time()\n",
        "ans=tokenizer.batch_decode(outputs)[0]\n",
        "ans=ans.split('\\n')\n",
        "for i in ans:\n",
        "  print(i)\n",
        "print(\"Time Taken is {} seconds\".format(end_time-start_time))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
