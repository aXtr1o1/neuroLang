{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma 7B Model \n",
    "## 4 Bit Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/sayhan/gemma-7b-it-GGUF-quantized/revision/main HTTP/1.1\" 200 1655\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b892a7a6d7b741d3839fe72d763e735b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/sayhan/gemma-7b-it-GGUF-quantized/revision/main HTTP/1.1\" 200 1655\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b3dd9b53344f15bf1a442f0799daf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to load model: Failed to create LLM 'gguf' from 'C:\\Users\\Rauf\\.cache\\huggingface\\hub\\models--sayhan--gemma-7b-it-GGUF-quantized\\snapshots\\a1fa0c9a98f42d2b27fd293cb2d59e2f5ffe4ccd\\gemma-7b-it.Q4_K_M-v2.gguf'.\n"
     ]
    }
   ],
   "source": [
    "llm4 = AutoModelForCausalLM.from_pretrained('sayhan/gemma-7b-it-GGUF-quantized', model_file=\"C:/Users/Rauf/.cache/huggingface/hub/models--sayhan--gemma-7b-it-GGUF-quantized/snapshots/a1fa0c9a98f42d2b27fd293cb2d59e2f5ffe4ccd/gemma-7b-it.Q4_K_M-v2.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gemma 7B Model\")\n",
    "print(\"4 Bit Quantized Model\")\n",
    "print(\"CPU: Intel i5 11th Gen\")\n",
    "print(\"\\n\\n\",\"Prompt: 'What is life?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_time = time.time()\n",
    "prompt = \"What is life?\"\n",
    "\n",
    "for word in llm4(prompt, stream=True):\n",
    "    print(word, end='')\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Total Time Taken to respond: \",end_time-st_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma 7B Model\n",
    "## 5 Bit Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/sayhan/gemma-7b-it-GGUF-quantized/revision/main HTTP/1.1\" 200 1655\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0715467ea30c45ea98a3696ce60ab13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'variant' is an invalid keyword argument for from_pretrained()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm5 \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msayhan/gemma-7b-it-GGUF-quantized\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgemma-7b-it.Q5_K_M-v2.gguf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Rauf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ctransformers\\hub.py:151\u001b[0m, in \u001b[0;36mAutoModelForCausalLM.from_pretrained\u001b[1;34m(cls, model_path_or_repo_id, model_type, model_file, config, lib, local_files_only, revision, hf, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gptq\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gptq\u001b[38;5;241m.\u001b[39mAutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    145\u001b[0m         model_path_or_repo_id,\n\u001b[0;32m    146\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    147\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    149\u001b[0m     )\n\u001b[1;32m--> 151\u001b[0m config \u001b[38;5;241m=\u001b[39m config \u001b[38;5;129;01mor\u001b[39;00m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    152\u001b[0m     model_path_or_repo_id,\n\u001b[0;32m    153\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    154\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    156\u001b[0m )\n\u001b[0;32m    157\u001b[0m model_type \u001b[38;5;241m=\u001b[39m model_type \u001b[38;5;129;01mor\u001b[39;00m config\u001b[38;5;241m.\u001b[39mmodel_type\n\u001b[0;32m    159\u001b[0m path_type \u001b[38;5;241m=\u001b[39m get_path_type(model_path_or_repo_id)\n",
      "File \u001b[1;32mc:\\Users\\Rauf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ctransformers\\hub.py:57\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, model_path_or_repo_id, local_files_only, revision, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, k):\n\u001b[1;32m---> 57\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     58\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is an invalid keyword argument for from_pretrained()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m         )\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(config, k, v)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m auto_config\n",
      "\u001b[1;31mTypeError\u001b[0m: 'variant' is an invalid keyword argument for from_pretrained()"
     ]
    }
   ],
   "source": [
    "llm5 = AutoModelForCausalLM.from_pretrained('sayhan/gemma-7b-it-GGUF-quantized', variant='gemma-7b-it.Q5_K_M-v2.gguf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gemma 7B Model\")\n",
    "print(\"4 Bit Quantized Model\")\n",
    "print(\"CPU: Intel i5 11th Gen\")\n",
    "print(\"\\n\\n\",\"Prompt: 'What is life?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_time = time.time()\n",
    "prompt = \"What is life?\"\n",
    "\n",
    "for word in llm5(prompt, stream=True):\n",
    "    print(word, end='')\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Total Time Taken to respond: \",end_time-st_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
